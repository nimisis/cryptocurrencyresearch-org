---
title: "Cryptocurrency Research - Reproducible Example"
output: 
  revealjs::revealjs_presentation:
    theme: sky
    self_contained: true
bibliography:
- packages.bib
---

```{r cache_all, include=FALSE}
# Once I'm done making edits get rid of the cache by commenting out the line below
knitr::opts_chunk$set(cache = TRUE)
# automatically format all the code chunks
# knitr::opts_chunk$set(tidy = F)
# THIS IS SIMPLER + PRESENTATION VERSION

# REMEMBER THIS IF NEED TO RESIZE OR COLOR TEXT:
# <span style="color: red; font-size: 10pt">**[Hourly]**</span
```

# Introduction

Welcome to this tutorial on supervised machine learning in the R programming language. This is the high-level version of a <a href="https://cryptocurrencyresearch.org/" target="_blank">more detailed tutorial</a>.

<!-- In this tutorial we will go through the process of predicting prices for a cryptocurrency using supervised machine learning. These results never change, and this is the high-level version of a <a href="https://cryptocurrencyresearch.org/" target="_blank">more detailed tutorial</a>. -->

<!-- Welcome to the more high-level version of the <a href="https://cryptocurrencyresearch.org/" target="_blank">predictive analytics tutorial on using supervised machine learning to make predictions on the cryptocurrency markets</a>. -->

**You can navigate between topics with the right and left arrow keys, and press on the down key to learn more about each topic.**

You can also press on the **`"o"`** or **`"esc"`** keys on your keyboard for an overview of the slides that can be help with navigation. Press **`"f"`** to switch to full-screen.

**Press on the down arrow key on your keyboard for an overview of what you will learn by following along with the tutorial.**

## Overview

-   Whenever an **R package** is referenced, the text will be [colored orange]{style="color: #ae7b11;"}. We will discuss ***R packages*** and the rest of the terms below later on in this presentation.

-   Whenever a **function** is referenced, it will be [colored green]{style="color: green;"}.

-   Whenever an **R object** is referenced, it will be [colored blue]{style="color: blue;"}. We will also refer to the **parameters** of functions in [blue]{style="color: blue;"}.

-   When a **term** is particularly common in machine learning or data science, we will call it out with [purple text]{style="color: purple;"}, but only the first time it appears.

## Overview - Continued

[You do not need pre-existing R knowledge to follow along, but you should familiar with working with data (even if only in Excel). **Here is what we will cover:**]{style="font-size: 70%;"}

-   You will gain a better understanding of how to install and load packages in R.

-   You will get an introduction to the [tidyverse]{style="color: #ae7b11;"} by learning about data manipulation and visualizations.

-   You will learn to make many different types of [supervised machine learning models]{style="color: purple;"} using a standardized approach using the [caret]{style="color: #ae7b11;"} package.

-   Finally, you will learn how to use [tidy tools for time series analysis](https://tidyverts.org/) ([tsibble]{style="color: #ae7b11;"} and [fable]{style="color: #ae7b11;"}.) <!-- , which are powerful and straightforward. -->

## This Version

-   This version of the tutorial is meant to be fully [reproducible]{style="color: purple;"}, meaning the results will never change and by running the code as outlined you will always get the exact same results outlined here. <!-- The <a href="https://cryptocurrencyresearch.org/" target="_blank">full version</a> on the other hand, refreshes every 12 hours adding the new 12 hours of data to the analysis each time. -->

-   The second main difference, is in this version we only deal with the historical prices for one cryptocurrency, while in the full version we make independent predictive models for each one on fresh data every 12 hours.

-   This problem itself has also been simplified, here we don't deal with all nuances relating to a cryptocurrency's price.

<!-- **This version does not take a viable approach to predicting cryptocurrency prices. There are many considerations that we ignore in this version and only deal with in the [full version](https://cryptocurrencyresearch.org/) of the tutorial. This tutorial is meant to provide helpful boiler-plate code that can be used for many different problems, rather than the specific problem of predicting cryptocurrency prices, like the [full version](https://cryptocurrencyresearch.org/) attempts to do.**  -->

<!-- Some key steps that are not taken in this higher-level version, include not taking a specific price at a specific exchange -->

<!-- Press the right arrow key to move on to the next topic.  -->

<!-- You can navigate between topics with the right and left arrow keys, and press on the down key to learn more about each topic. -->

<!-- ## Who is this tutorial for? -->

<!-- - This tutorial is meant for people who regularly deal with data, but think doing predictive modeling is too difficult or complex.  -->

<!-- - You do not necessarily need to be proficient in R to follow along, although that is certainly helpful. -->

## What is Machine Learning?

<!-- ^ could make title purple too -->

Making forecasts about the future using data from the past using a process called [**Supervised Machine Learning**]{style="color: purple;"} has become increasingly easier over the past decade, and has become a clearly defined step-by-step process that any professional who regularly works with data can follow along with.

Machine Learning is the process of [***training***]{style="color: purple;"} a computer to find an answer within clearly defined rules. We provide a machine learning algorithm data about the past and outline the "rules" to work within, and the model looks for statistical structure (depending on the specific model used) that can be used to make [**forecasts**]{style="color: purple;"} about the future.

## Supervised Machine Learning

The *Supervised* part of supervised machine learning refers to the fact that the data was observed in the past, and provides us information regarding both the inputs (independent variables) and the outcome (dependent variable, also known as the [target variable]{style="color: purple;"}). [Click here for a more in-depth explanation of the different branches of machine learning](https://machinelearningmastery.com/types-of-learning-in-machine-learning/).

Go to the next slide (**right** arrow key `r emo::ji('right arrow')`) to get started!

# R Interface

This section will provide a very basic overview of what it actually means to use the R programming language. We will start by covering what an ***R session*** is and how to get started. **If you have already used R and RStudio in the past, feel free to skip the section below.**

## R and RStudio {#r-and-rstudio}

The R programming language is a free computer application that [can be downloaded from the internet](https://www.r-project.org/). Anyone using R should be aware of **RStudio**, which is a different computer application that [can also be downloaded for free online](https://rstudio.com/products/rstudio/download/#download). RStudio makes programming in R a much better experience and is a tool anyone using R should be familiar with. After installing these two programs, anyone is ready to start programming in R and follow along with this tutorial.

## R Sessions

[Whenever you open the **RStudio** application for the first time, you will be brought to a ***new R session*** that should look like this:]{style="font-size: 95%;"}

![](C:/Users/ries9/Documents/Research-Paper-Example/images/rstudio_interface.PNG){width="800"}

## R Sessions - Continued {#r-sessions-continued}

Throughout this tutorial, we will show pieces of R code that execute as part of this presentation (which was created in a tool called [R Markdown]{style="color: purple;"} within RStudio that you do not need to worry about):

```{r hello_from_R}
print("Hello from R!")
```

The output above shows the result of the code we ran, where the number **[1]** indicates this is the first value of the output.

Keep going below ⬇️ to learn more.

## R Sessions - Continued

![](C:/Users/ries9/Documents/Research-Paper-Example/images/rstudio_interface_annotated.PNG)

[**1.** When we run code in this presentation as shown in the [previous slide ⬆️](#r-sessions-continued), it is equivalent to running the code in your **console** in RStudio, which is where the ***\>*** symbol is inside the red highlighted area.]{style="font-size: 55%;"}

[**2.** If you wanted to save your code to use in a new R session once you close the program, you would need to create a new file in the top left corner of RStudio.]{style="font-size: 55%;"}

## R Objects

[When we start a new R session, for example when we open RStudio, the environment should be empty (as displayed in the top right pane of the previous screenshot). We can create new objects by naming them whatever we would like, and using an arrow to assign the object some kind of value:]{style="font-size: 65%;"}

![](C:/Users/ries9/Documents/Research-Paper-Example/images/rstudio_assign_variable_2.PNG){width="705"}

## R Objects - Continued

[Whenever we run a command in R, we have the option of saving the result in the R environment. For example, we can create a new object called words]{style="font-size: 75%;"}:

```{r make_words_obj}
words <- "this object stores text"
```

[We could have shown the results directly and not saved the results as a variable. Savings results as R objects is helpful for approaching a problem step-by-step. We can show the results that we saved by running a command with the name of the object:]{style="font-size: 75%;"}

```{r show_words_obj}
words
```

</span>

<!-- ## R Objects - Continued -->

<!-- We can save new objects from new objects -->

## R Functions

When we want to perform some kind of operation on an R object, we will use a [function]{style="color: purple;"}. Earlier, when we ran the command **print("Hello from R!")**, we asked R to [**print()**]{style="color: green;"} the text "Hello from R!".

A new R session has many functions available to it, but the default functionality pales in comparison to the number of functions that have been created by R users over the years. We can load functions created by other users and researchers by installing a **package** on our computer and loading the functionality in the R session, which we will review in detail in a [later section](#what-are-packages).

## R Functions - continued

Different functions have different [**parameters**]{style="color: purple;"}, which are the inputs for a function. Some functions require more inputs than others to work. The [**print()**]{style="color: green;"} function for example only requires us to input one parameter for what we want to ***print*** in the output. Many other functions however, require more inputs and will not work with a single input provided. In many cases, the function will have [default values]{style="color: purple;"} for a parameter, that way if no selection is made by the user the default behavior will be used, and if the user makes a selection then the selection overrides the default behavior.

## R - Getting Help

[But what if you don't know how to use a function? R has some excellent resources available, and in most cases you can figure out the answers to most usage questions directly inside of R by running the **help()** function on any other function. When packages are installed, they will come with documentation that can be opened this way. The last section tends to be the most useful and includes useful examples of using the function.]{style="font-size: 65%;"}

![](C:/Users/ries9/Documents/Research-Paper-Example/images/rstudio_help.PNG){width="529"}

## Move on

There is a lot more to cover in terms of "how do I program in R?", but this is all the knowledge you need to be able to follow along with this tutorial.

Move on to the next section by pressing the right arrow key on your keyboard `r emo::ji('right arrow')`

# Disclaimer

**This tutorial is made available for learning and educational purposes only and the information to follow does not constitute trading advice in any way shape or form.** We avoid giving any advice when it comes to trading strategies, as this is a very complex ecosystem that is out of the scope of this tutorial; we make no attempt in this regard, and if this, rather than data science, is your interest, your time would be better spent following a different tutorial that aims to answer those questions.

# Follow Along With the code

As code comes up and is explained, we recommend that you run it yourself on your computer as well. You can run the code by following the installation instructions as outlined in the steps below.

**Alternatively, you can** <a href="https://mybinder.org/v2/gh/ries9112/high-level-reprex-jupyter.git/master" target="_blank">**click here to run the code in the cloud without having to worry about installing anything** </a>.

<!-- If you are familiar with the [RStudio](https://rstudio.com/) interface, you can [**click here to run the code in the cloud and in RStudio**](https://mybinder.org/v2/gh/ries9112/high-level-reprex-jupyter.git/master?urlpath=rstudio/master) -->

## Setup R Environment

In order to follow along with this tutorial, you will need to install additional R packages. At this point you will need to have R and RStudio installed [as previously outlined](#r-and-rstudio). This tutorial is for beginners, and each step is clearly explained. If you do not, [follow these instructions](https://www.youtube.com/watch?v=cX532N_XLIs) first.

**If you do not want to install and run R on your computer, you can instead [click here to run the code in a cloud environment](https://mybinder.org/v2/gh/ries9112/high-level-reprex-jupyter/f74b9ab4b500c6b2cc0ebb0e48985c4be03a97ba)**.

**Press on the down arrow key ⬇️ on your keyboard for instructions on how to setup your machine to follow along with this R programming example.**

```{r initialSetup_needs_adjusting, message=FALSE, warning=FALSE, include=F}
library(bookdown)
library(knitr)
library(tictoc)
library(DT)
#library(devtools)
# library(dplyr)
# install_github("ries9112/PredictCrypto")
#library(PredictCrypto) #import with eval=F later
options(scipen=999) # disable scientific notation

# change wd to show images
setwd("~/Research-Paper-Example")
# start timer
tic('whole process time')
```

<!-- If you want to follow along from your own computer, please follow the steps that follow on your own machine. After following these setup instructions you will be able to follow along with the rest of the steps in this analysis. -->

<!-- ## Installing Packages  (Sure about removing this?) -->

<!-- One of the main tools that we will be using in this tutorial is the `tidyverse`. -->

<!-- We can install the `tidyverse` package by running the command `install.packages('tidyverse')`: -->

<!-- ```{r tidy_install, eval=F} -->

<!-- install.packages('tidyverse') -->

<!-- ``` -->

<!-- As of writing this, the ***core tidyverse*** is comprised of [8 different packages](https://www.tidyverse.org/packages/) and includes most if not all of the tools you would need for your daily data analysis work. By running the command `install.packages('tidyverse')` you installed each one. From this collection we will mostly leverage [`dplyr`](https://dplyr.tidyverse.org/) for data manipulation and [`ggplot2`](https://ggplot2.tidyverse.org/) for visualizations in this tutorial. -->

<!-- ## The tidyverse -->

<!-- ## Import the tidyverse -->

<!-- The previous command `install.packages('tidyverse')` installed all packages that are part of the tidyverse, but before we can use the contents of those packages we also need to run the line `library(tidyverse)` to import them into the current R session. We could also import the packages individually, for example by running `library(dplyr)`, but let's import all of them at once into the R session: -->

<!-- ```{r import_tidyverse, warning=FALSE} -->

<!-- library(tidyverse) -->

<!-- ``` -->

## What are packages? {#what-are-packages}

Packages are collections of functions and data that other users have made shareable. We can install these packages into our own library of R tools and load them into our R session, which can enable us to write powerful code with minimal effort compared to writing the same code without the additional packages. Many packages are simply time savers for things we could do with the default/base functionality of R, but sometimes if we want to do something like make a static chart interactive when hovering over points on the chart, we are better off using a package someone already came up with rather than re-invent the wheel.

Keep going below ⬇️ to learn how to install R packages.

## Installing Packages

We can install packages to extend the functionality of what we can do in R. As a first step, we will need to run the command [**`install.packages()`**:]{style="color: green;"}

```{r, eval=F}
install.packages("package_name")
```

We only need to install any given package once on any given computer, kind of like installing an application (like RStudio or Google Chrome) once.

## Using Packages With library()

After installation, we can use [**library(package)**]{style="color: green;"} to import those packages into the current R session. Each time we open a new session in R, we need to import the packages again using [**library()**]{style="color: green;"}. R has some functionality that is just *there* at startup, which is typically referred to as **base R**. When we want to extend R past what base R has to offer, we need to import the package in the current session by using the function [library()]{style="color: green;"}; if the package/functionality has never been installed on top of your current version of base R, you will first need to first install it with [install.packages()]{style="color: green;"}. 

A package only needs to be installed once, but needs to be imported into each new R session using [library()]{style="color: green;"}.

## Installing a Package Example

<!-- Because you might already have some of the packages we will be using installed on your computer, we can start by installing a package that will help us  -->

<!-- the `pacman` package, which will allow us to install the remaining packages in a "smarter" way, where if you already have a package in your library, it will not be installed again. -->

We can install the package called [**pacman**]{style="color: #ae7b11;"} using `install.packages()`:

```{r pacman_install, eval=F}
install.packages("pacman")
```

[**pacman**]{style="color: #ae7b11;"} does not refer to the videogame, and stands for ***package manager***. After we import this package, we will be able to use new functions that come with it. We can use one of those functions to install the remaining packages we will need for the rest of the tutorial. The advantage to using the new function, is the installation will happen in a "smarter" way, where if you already have a package in your library, it will not be installed again.

## Use "pacman"

We can now import the [**pacman**]{style="color: #ae7b11;"} package:

```{r import_pacman, message=FALSE, warning=FALSE}
library(pacman)
```

Now we have access to the function [**p\_load()**]{style="color: green;"}:

```{r p_load_packages, message=FALSE, warning=FALSE}
p_load("pins","dplyr","ggplot2","ggthemes","gganimate","caret","xgboost","kernlab", "tsibble","fabletools","fable","feasts","urca","plotly")
```

[Running **p\_load()** is equivalent to running [**install.packages()**]{style="color: green;"} on each of the packages listed **(but only when they are not already installed on your computer)**, and then running [library()]{style="color: green;"} for each package in quotes separated by commas to import the functionality from the package into the current R session. **Both commands are wrapped inside the single function call to** [**p\_load()**]{style="color: green;"}.]{style="font-size: 70%;"}

## All Set

If you followed along with the steps as outlined, the code in the upcoming slides should work on your computer when run in the correct order.

Move on to the next slide (right arrow key `r emo::ji('right arrow')`) to get started!

# Get the Data

[When doing an analysis, you would typically start with some kind of tabular data, like an Excel file. An Excel file can be saved with the **csv** extension, which can then easily be read by R using the [**read.csv()**]{style="color: green;"} function. When running the function we can save the results in an R object and start doing the analysis.]{style="font-size: 82%;"} 

[The approach described requires you to download the data to your computer. Rather than having you download the data and place it in the correct folder for your R session to reference, we will take a more unorthodox approach that will let us download the data directly from a specific URL. Do not worry too much about the code in the next two slides outside of the fact that we are retrieving the data from an online resource (instead of the more common local computer storage methodology) and saving the data to an R object.]{style="font-size: 82%;"}

## Pins Package

<span style="font-size: 90%;">
The first package we will use is the [**pins**]{style="color: #ae7b11;"} package [[@R-pins]](#pins-citation) to retrieve the data that we will be working with.
</span>

<span style="font-size: 90%;">
First, we will need to connect to a public [GitHub repository]{style="color: purple;"} (anyone can post their code to the GitHub website and make a "repository" with code for their project) and ***register*** the ***board*** that the data is ***pinned*** to by using the [**board\_register()**]{style="color: green;"} function:
</span>

```{r board_register}
board_register(name = "pins_board", url = "https://raw.githubusercontent.com/predictcrypto/pins/master/", board = "datatxt")
```

<span style="font-size: 90%;">
By running the [board\_register()]{style="color: green;"} command on the URL where the data is located, we will be able to ***"ask"*** for any dataset available on the board (in the next slide).
</span>

## Pull Data

<span style="font-size: 80%;">
The previous step created a connection to pull the dataset we will use, don't worry too much about the code on the previous slide, but [you can learn more by clicking here](https://pins.rstudio.com/). Now that we are connected to the **board**, we can use the [**pin\_get()**]{style="color: green;"} function to retrieve the data that we will use for the rest of this tutorial:
</span>

```{r pull_cryptodata_data_reprex}
cryptodata <- pin_get(name = "ETH_Binance")
```

<span style="font-size: 80%;">
The dataset named ***ETH\_Binance*** is data relating to the cryptocurrency **Ethereum (ETH)** on the **Binance** exchange and was downloaded from the website [**https://cryptodatadownload.com/**](https://cryptodatadownload.com/){.uri}.
</span>

<span style="font-size: 75%;">
We will be working with hourly data for the **date range from 2020-01-01 to 2020-07-31**.
</span>

## Data Source

The data is made available by the website ***cryptodatadownload.com***:

```{r show_cryptodatadownload, echo=F}
knitr::include_url("https://www.cryptodatadownload.com/about",
  height = "500px")
```

## Original Data Source

The data was originally sourced from the link for the **ETH/USD** pair and the [**[Hourly]**]{style="color: red;"} option found below:

```{r download_datasets_embed, echo=F}
knitr::include_url("https://www.cryptodatadownload.com/data/binance/",
  height = "500px")
```

## Data Preview

```{r show_cryptodata1, echo=F, message=FALSE, warning=FALSE}
library(DT)
datatable(select(head(arrange(cryptodata, Date), 100), DateTime, Date:Target24HourClose),  style = "default", 
          options = list(scrollX = TRUE, pageLength=2, lengthMenu = c(1,2,3,4)))
```

<small>*Only 100 rows of data are made available in the table above as an example*</small>

## Data Alternative - BTC

We also made a similar dataset available for the Bitcoin (BTC) cryptocurrency. If you wanted to run the same analysis on Bitcoin instead of Ethereum, you could run this command instead:

```{r pull_cryptodata_data_reprex_BTC_option, eval=F}
cryptodata <- pin_get(name = "cryptodatadownload_BTC_Binance", board = "pins_board")
```

Keep in mind that this tutorial will **only be reproducible if you enter** ***ETH\_Binance*** **for the `name` parameter**. Otherwise your code would run, but from the perspective of the Bitcoin cryptocurrency rather than the analysis we are performing on the Ethereum cryptocurrency in this tutorial.

<!-- ## More Data Alternatives -->

<!-- There are options for the `name` parameter:  -->

<!-- - cryptodatadownload_LTC_Binance -->

<!-- - cryptodatadownload_NEO_Binance (Still need to add this one in) -->

<!-- - ... Add one more? -->

<!-- Again, keep in mind that this tutorial will only be reproducible if you enter the name **`ETH_Binance`** for the **`name** parameter, but feel free to get as much practice as you would like looking at a different cryptocurrency. -->

<!-- WON"T FIT <small>In the [full version of the tutorial](https://cryptocurrencyresearch.org/), we model many cryptocurrencies independently at the same time, in this tutorial we only handle one cryptocurrency (Ethereum) </small>  -->

<!-- ## Reproducible Example{#reprex} -->

<!-- This is a fully reproducible example that does not change over time. Meaning, if you were to come back to this static document in 6 months, it would look completely identical. The [full version is the only one that refreshes every 12 hours](https://cryptocurrencyresearch.org/). -->

<!-- The dataset is comprised of `r as.numeric(max(cryptodata$date) - min(cryptodata$date))` days worth of hourly data starting from `r min(cryptodata$date)` and ending `r max(cryptodata$date)`. -->

<!-- Go to the next slide to learn more about the data we will be using in this example. -->

# Documentation

**The data we will be using was downloaded from [cryptodatadownload.com](https://cryptodatadownload.com/).** This is [***tidy data***](https://tidyr.tidyverse.org/articles/tidy-data.html), meaning:

1.  Every column is a variable.

2.  Every row is an observation.

3.  Every cell is a single value.

<span style="font-size: 93%;">
The cryptocurrency's price is **recorded in one hour intervals** and includes `r max(cryptodata$Date)-min(cryptodata$Date)` days worth of data. Each row summarizes pricing information relating to the cryptocurrency over the specified 1 hour period (the very last field shows the date and the time associated with the row).
</span>

<!-- SHOULD I MAKE THE DATETIME FIELD THE SECOND FIELD? -->

## Data Dictionary {#data-dictionary}

<!-- The data is made up of the following columns: -->

-   [**DateTime**]{style="color: blue;"}: The date and time that the data was collected in the **UTC timezone**.
-   [**Date**]{style="color: blue;"}: The date on which the data was collected in the **UTC timezone**.
-   [**Symbol**]{style="color: blue;"}: The cryptocurrency associated with the row of data. "ETH" stands for the "Ethereum" cryptocurrency.
-   [**Open**]{style="color: blue;"}: The price of the cryptocurrency in US Dollars (\$) **at the start of the hour**.
-   [**High**]{style="color: blue;"}: The highest price during the course of the hour associated with the row of data in US Dollars.
-   [**Low**]{style="color: blue;"}: The lowest price during the course of the hour associated with the row of data in US Dollars.


## Data Dictionary - Continued  

-   [**Close**]{style="color: blue;"}: The price of the cryptocurrency in US Dollars (\$) **at the end of the hour.**
-   [**VolumeUSDT**]{style="color: blue;"}: The total trading volume associated with the one hour period associated with the row of data.
-   [**CloseLag1Hour**]{style="color: blue;"}: The value of the field Close, offset by 1 hour into the past, meaing the Close price ofthe cryptocurrency 1 hour earlier in US Dollars (\$).
-   [**CloseLag12Hour**]{style="color: blue;"}: The Close price of the cryptocurrency 12 hours earlier in US Dolars (\$)
-   [**CloseLag24Hour**]{style="color: blue;"}: The Close price of the cryptocurrency 24 hours earlier in US Dolars (\$)
-   [**CloseLag3Day**]{style="color: blue;"}: The Close price of the cryptocurrency 3 days earlier in US Dollars (\$)

## Data Dictionary - Continued

-   [**CloseLag7Day**]{style="color: blue;"}: The Close price of the cryptocurrency 7 days earlier in US Dollars (\$).
-   [**CloseLag14Day**]{style="color: blue;"}: The Close price of the cryptocurrency 14 days earlier in US Dollars (\$)
-   [**CloseLag30Day**]{style="color: blue;"}: The Close price of the cryptocurrency 30 days earlier in US Dollars (\$)
-   [**CloseLag90Day**]{style="color: blue;"}: The Close price of the cryptocurrency 90 days earlier in US Dollars (\$)
-   [**CloseLag120Day**]{style="color: blue;"}: The Close price of the cryptocurrency 120 days earlier in US Dollars (\$).
-   [**Target24HourClose**]{style="color: blue;"}: The value of the field Close, offset by **24 hours into the future**. **This is the column we are interested in predicting values for**.

# Data Preview

```{r show_cryptodata2, echo=F, message=FALSE, warning=FALSE}
library(DT)
datatable(select(head(arrange(cryptodata, Date), 100), DateTime, Date:Target24HourClose),  style = "default", 
          options = list(scrollX = TRUE, pageLength=2, lengthMenu = c(1,2,3,4)))
```

<small>Only 100 rows shown. If you are unclear on what a particular column might be, you can refer to the [previous section, which contains a data dictionary describing each column.](#data-dictionary) </small>

## What is the goal?

<span style="font-size: 85%;">
Before we start even cleaning the data, it's important to establish ***what*** we want to do, as the steps we take may very well depend on those intentions. Our goal, is to predict the future price of the cryptocurrency called **Ethereum**. For any given hour, we are given the **Open**price at the start of the hour, the [**Close**]{style="color: blue;"} price at the end of the hour, the lowest and highest prices for the given hour, the volume associated with the hour of trading, as well as the Close price from the previous hour, previous 12 hours, and 7 other ***lagged*** variables. We want to be able to draw relationships between these variables in order to predict the value of the field called [**Target24HourClose**]{style="color: blue;"}, which is the [**Close**]{style="color: blue;"} price 24 hours into the future relative to the rest of the columns in the data.
</span>

# Data Prep {#data-prep}

Data preparation is an essential step for effective predictive modeling. Not all datasets are created equal, and the quality of the models we are able to make will only ever be as good as the quality of the data itself. [***Cleaning***]{style="color: purple;"} a dataset can mean many different things depending on the problem at hand, and unfortunately we are unable to cover them all here; this tutorial focuses on the predictive modeling side of things, so **the data provided has already been cleaned beforehand**, but we still introduce some useful tools in the section below.

## Cleaning Data - Overview

<span style="font-size: 75%;">
What does "cleaning data" look like more specifically? Data tends to have issues, and before we start deriving insights from it, we usually need to fix those issues, which is what we mean by ***cleaning the data***. Some examples:
</span>
<small>

-   **Are the columns of the dataset the correct data types (i.e. numeric, string, etc..)?**

-   **Are there any missing ([null]{style="color: purple;"}****) values?** What proportion of all rows is missing for each column? What is the context behind them? Is this problematic in terms of what you end goal is? Are there any duplicate

-   **Are there any inconsistencies in the data?** Do the outliers make sense or is there a data capture issue of some kind? Are there typos if the data is manually entered?

There are tools that can help answer all of these questions, in the [full version](https://cryptocurrencyresearch.org/) of this tutorial, we cover some functionality found in the [[**skimr**]{style="color: #ae7b11;"} package](https://github.com/ropensci/skimr). After we have made sure we are working with a clean dataset, we can do some [***data engineering***]{style="color: purple;"} to try and improve the accuracy of the models we will be making by adding new information that might be relevant to the problem at hand, which is what we will be doing in the next step when we add the new [**Volatility**]{style="color: blue;"} column to the data.

</small>

## Exploratory Data Analysis

It is worth mentioning that [**E**xploratory **D**ata **A**nalysis (EDA)]{style="color: purple;"} is the important process by which you figure out ***what*** about the data needs to be ***cleaned*** and adjusted, but we are not covering it in this tutorial, and these steps have already been taken care of. You can find an excellent guide to EDA here: <https://r4ds.had.co.nz/exploratory-data-analysis.html>

In the [full version of this tutorial](https://cryptocurrencyresearch.org/) we do more EDA because new data is added every 12 hours and we need to make sure there are no issues across all \~90 columns, while here we are providing a dataset that will never change and we have already cleaned ahead of time.

## Dplyr Package

There are resources and tools out there that are very effective for cleaning and manipulating data. We recommend starting with the [**dplyr**]{style="color: #ae7b11;"} package if working in R, which is what we used to clean the data. Because this is such a useful tool in R, we included a brief example using the [**mutate()**]{style="color: green;"} function to add a new column to the data that calculates the **volatility** for each row.

<!-- We used the `mutate()` function to calculate the **`CloseLag...`** and **`Target24HourClose`** fields, but have omitted those steps from this tutorial to keep things more focused on the predictive analytics process instead of providing data preparation steps that are not relevant to many problems. Please refer to the **[data dictionary](#data-dictionary)** to understand what these columns represent and how they were calculated.-->

</small>

## Mutate Function - dplyr

<small>The [**dplyr**]{style="color: #ae7b11;"} package from the [tidyverse](https://www.tidyverse.org/) is very useful for cleaning and manipulating data. Using the [**mutate()**]{style="color: green;"} function from [**dplyr**]{style="color: #ae7b11;"}, we can add a new column to the data according to a formula that gets applied to each row. For example, we could create add a brand new column (C) to our data by taking the sum of two columns (A+B) like illustrated below:</small>

<!-- ```{r, eval=F} -->

<!-- mutate(cryptodata, sum_example = Open + High) -->

<!-- ``` -->

[![Illustration by Allison Horst](C:/Users/ries9/Documents/Research-Paper-Example/images/dplyr_mutate.png){width="48%"}](https://dplyr.tidyverse.org/reference/mutate.html)

<small>*Illustration by [Allison Horst](https://github.com/allisonhorst/stats-illustrations)*</small>

<!-- ## Add Lagged Prices -->

<!-- ```{r lagged_prices} -->

<!-- cryptodata <- mutate(cryptodata, CloseLag1Hour = lag(Close, n = 1)) -->

<!-- ``` -->

<!-- <small>In the code above, we used the function `mutate()` on the object `cryptodata` to create a new column called `Lag1Hour`, which contains the *Close* price from 1 hour earlier. The `lag()` function is also from `dplyr` and allows us to offset values by one row, which in our case is equivalent to one hour. See the table below to use the DateTime field to compare the original `Close` price to the `Lag1Hour` Close price:</small> -->

<!-- ```{r lagged_prices_show, echo=F} -->

<!-- datatable(select(head(arrange(cryptodata, desc(Date)), 2000), DateTime, Close, CloseLag1Hour),  style = "default",  -->

<!--           options = list(scrollX = TRUE, pageLength=2, lengthMenu = c(1,2,3)), rownames = F) -->

<!-- ``` -->

<!-- ## More lagged prices -->

<!-- <small>Let's also add lagged prices for a 12 hour, 24 hour, 3 day, 7 day, 14 day, 30 day, 90 day and 120 day period as new columns:</small> -->

<!-- ```{r lagged_prices_all} -->

<!-- cryptodata <- mutate(cryptodata, CloseLag12Hour = lag(Close, n = 12), -->

<!--                                  CloseLag24Hour = lag(Close, n = 24), -->

<!--                                  CloseLag3Day = lag(Close, n = 72), -->

<!--                                  CloseLag7Day = lag(Close, n = 168), -->

<!--                                  CloseLag14Day = lag(Close, n = 336), -->

<!--                                  CloseLag30Day = lag(Close, n = 720), -->

<!--                                  CloseLag90Day = lag(Close, n = 2160), -->

<!--                                  CloseLag120Day = lag(Close, n = 2880)) -->

<!-- ``` -->

<!-- ```{r lagged_prices_show_all, echo=F} -->

<!-- datatable(select(head(arrange(cryptodata, desc(Date)), 2000), DateTime, Close, CloseLag1Hour, CloseLag12Hour, CloseLag24Hour, CloseLag3Day, CloseLag7Day, CloseLag14Day, CloseLag30Day, CloseLag90Day, CloseLag120Day),  style = "default",  -->

<!--           options = list(scrollX = TRUE, pageLength=2, lengthMenu = c(1,2,3)), rownames = F) -->

<!-- ``` -->

## Add Volatility

Using [**mutate()**]{style="color: green;"} we can add a new column called [**Volatility**]{style="color: blue;"}, which tracks the percent difference between the [Low]{style="color: blue;"} price and the [High]{style="color: blue;"} price at each hourly datapoint as an absolute value:

```{r add_volatility}
cryptodata <- mutate(cryptodata, Volatility = abs(((High - Low)/Low)*100))
```

<!-- , and overwrote `cryptodata` with the result:  -->

```{css, echo=FALSE}
pre[class] {
  max-height: 150px;
}
```

```{css, echo=FALSE}
.scroll-150 {
  max-height: 150px;
  overflow-y: auto;
  background-color: inherit;
}
```

```{r, class.output="scroll-150"}
select(cryptodata, DateTime, Low, High, Volatility) #Show results (only columns of interest)
```

<!-- ## Add Target Variable -->

<!-- <small>We will also need to worry about what we are actually trying to predict. For this tutorial, we will be making price predictions for 24 hours into the future, so we will need to calculate a new column containing those outcomes, as well. We can do that by taking the same code as before and changing the `lag()` function with `lead()`:</small> -->

<!-- ```{r add_target} -->

<!-- cryptodata <- mutate(cryptodata, Target24HourClose = lead(Close, n = 24)) -->

<!-- ``` -->

<!-- ```{r lagged_prices_show_all_w_target, echo=F} -->

<!-- datatable(select(tail(head(arrange(cryptodata, Date), 3000),2000), DateTime, Close, CloseLag24Hour,  Target24HourClose),  style = "default",  -->

<!--           options = list(scrollX = TRUE, pageLength=2, lengthMenu = c(1,2,3)), rownames = F) -->

<!-- ``` -->

<!-- ## Remove NA's -->

<!-- <!-- HERE NEED TO REMOVE ROWS WITH NA VALUES FOR TARGET! CARET WON'T WORK OTHERWISE. -->

<!-- The [**`dplyr`**](https://dplyr.tidyverse.org/) package has ***a lot*** of functionality that we will not cover here, but let's use the `drop_na()` function to get rid of rows with null values to run into less issues when making the predictive models. -->

<!-- ```{r} -->

<!-- cryptodata <- drop_na(cryptodata) #, Target24HourClose) -->

<!-- ``` -->

<!-- We now have everything we need to produce the predictive models. Before making the predictive models however, let's spend some time learning how to visualize data. -->

## Remove Symbol Column

In the previous slide we checked we calculated the Volatility correctly by only returning the relevant columns using the [**select()**]{style="color: green;"} function. The column named [Symbol]{style="color: blue;"} in the dataset always has a consistent value of **ETH** and adds no value when making predictions, so we can remove the column very easily using the [**select()**]{style="color: green;"} function again, but this time using it to remove a column from the data:

```{r remove_symbol}
cryptodata <- select(cryptodata, -Symbol)
```

## Train/Test Split

When we make predictive models, we would not want to make important real life decisions based on those models without having a good idea of how effective the methodology actually is, and ensuring no mistakes were made. We will likely never get a 100% accurate representation of what the model will actually perform like in the real world without actually tracking those results over time, but there are ways for us to get a sense of whether something works or not ahead of time. Here, we are taking the simplest of those approaches by training our models on the earlier 80% of the data, and testing the accuracy of those models on the remaining 20% new unseen data to start evaluating how good the models are. <!-- Later on we will discuss a slightly better way called ***cross validation***. -->

## Train/Test Split - Continued

<span style="font-size: 80%;">
As a last step, let's split the data into a [**train**]{style="color: purple;"} and [**test**]{style="color: purple;"} dataset. We will build the models on the earlier 80% of the data, and assess how well the models perform on the last 20% of the data:
</span>

```{r}
cryptodata_train <- head(cryptodata, as.integer(nrow(cryptodata)*.8))
```

And the test data:

```{r}
cryptodata_test <- tail(cryptodata, as.integer(nrow(cryptodata)*.2))
```

<span style="font-size: 65%;">
The first piece of code is taking the first 80% of the rows, and the second line for **cryptodata\_test** creates the object taking the last 20% of the rows of [**cryptodata**]{style="color: blue;"} (which was already sorted ahead of time from the earliest to the latest datapoint).
</span>

<!-- ## A Note on overfitting -->

<!-- (below isn't very accurate to what overfitting actually is) -->

<!-- There are some situations that tend to be problematic. One such scenario, is one where the model ***seems*** to perform extremely well, but doesn't work as expected when used in a practical context. One reason for this kind of <span style="color: purple;"> overfitting</span>  -->

## Helpful Resources

As previously mentioned, we omitted data cleaning/preparation steps as they might not be relevant to all predictive analytics problems. If you are looking for a good resource for these types of problems, look no further than "STAT 545": [**https://stat545.com/**](https://stat545.com/){.uri} which takes the opposite approach of what we are doing here, and "is about everything that comes up during data analysis except for statistical modelling and inference."

Also remember that if you do not know what a function does, you can put it inside of the function [**help()**]{style="color: green;"} to open the documentation associated with the function. You can learn more about [help()]{style="color: green;"} here: <https://www.r-project.org/help.html>

# Visualizing Data

<small> The [**ggplot2**]{style="color: #ae7b11;"} package [[@R-ggplot2]](#ggplot-citation) is the standard when it comes to making plots in R. The ***gg*** in [**ggplot2**]{style="color: #ae7b11;"} stands for the ***Grammar of Graphics***, which is essentially the idea that many different types of charts share the same underlying building blocks, and that they can be put together in different ways to make charts that look very different from each other. [In Hadley Wickham's (the creator of ggplot2) own words,](https://qz.com/1007328/all-hail-ggplot2-the-code-powering-all-those-excellent-charts-is-10-years-old/) "a pie chart is just a bar chart drawn in polar coordinates", "They look very different, but in terms of the grammar they have a lot of underlying similarities."</small>

[![Illustration by Allison Horst](C:/Users/ries9/Documents/Research-Paper-Example/images/ggplot2_masterpiece.png){width="44%"}](https://ggplot2.tidyverse.org/)

<small>*Illustration by [Allison Horst](https://github.com/allisonhorst/stats-illustrations)*</small>

## Basics - ggplot2

<span style="font-size: 70%;">
So how does **ggplot2** actually work? According to the [official documentation](https://ggplot2.tidyverse.org/), ***"...start with*** [***ggplot()***]{style="color: green;"}***, supply a dataset and aesthetic mapping (with*** [***aes()***]{style="color: green;"}***)...***
</span>

```{r ggplot_blank, fig.width=3, fig.height=2}
price_chart <- ggplot(data = cryptodata, aes(x = DateTime, y = Close))
# Show chart:
price_chart
```

<small> Here we specified which data to use and which variables we are interested in plotting, find out about specifying a [***geom***]{style="color: purple;"} to visually represent the points in the slide below ⬇️</small>

## ggplot - Add geom

<small>In the previous slide, we got a blank chart, but that was not due to an error. Up to this point, we have specified the data as being [**cryptodata**]{style="color: blue;"}, and we assigned the x-axis to the variable [**DateTime**]{style="color: blue;"} and the y-axis to the variable [**Close**]{style="color: blue;"}, but we have not yet specified ***how*** to plot the information. Should the points be shown as dots? Lines? Bars? We can specify the shape by adding a ***geom***, in this case a line using [**geom\_line()**]{style="color: green;"}. You can find many more ***geom*** types here: <https://ggplot2.tidyverse.org/reference/> </small></small>

<!-- adjust font size above? -->

```{r ggplot_point, fig.width=4.5, fig.height=2.7}
price_chart <- price_chart + geom_line()
# Show chart
price_chart
```

## ggplot add title

<small> The main takeaway, is that we don't need to re-build every aspect of a chart when we want to produce something that looks different. We can keep building a chart one piece at a time, and change those pieces when we have to. Here is another example adding a title:</small>

```{r ggplot_title, fig.width=4, fig.height=2.2}
price_chart <- price_chart + ggtitle('Price ($) Over Time - Ethereum')
# Show chart
price_chart
```

<small> We could of course do the same to rename the x and y axis. The package [**ggplot2**]{style="color: #ae7b11;"} is much more than that however, go to the slide below to learn about how the functionality of the [**ggplot2**]{style="color: #ae7b11;"} package can be extended further.</small>

## Extending ggplot2

<small> The [**ggplot2**]{style="color: #ae7b11;"} package has official frameworks on how it can be extended, and the R community has developed several very useful extensions, which allow for things like and interactive charts. You can find a pretty comprehensive list of useful extensions here: <https://exts.ggplot2.tidyverse.org/> </small>

```{r show_ggplot_extensions, echo=F}
knitr::include_url("https://exts.ggplot2.tidyverse.org/gallery/",
  height = "480px")
```

<!-- ## Interactive Chart Example -->

<!-- One neat way to extend the functionality of a chart we've made using **`ggplot`**, is by using the **`ggplotly()`** function from the **`plotly()`** package: -->

<!-- ```{r plotly} -->

<!-- ggplotly(price_chart) -->

<!-- ``` -->

## Extension - ggthemes

For example, we could use the [**ggthemes**]{style="color: #ae7b11;"} package (installed and loaded previously) to change the look of the chart:

```{r ggplot_theme, fig.width=4.5, fig.height=3.5}
price_chart + theme_economist()
```

<!-- ## Extensions - plotly -->

<!-- ```{r ggplotly, fig.width=4.5, fig.height=3.5} -->

<!-- library(plotly) -->

<!-- ggplotly(price_chart + theme_economist()) -->

<!-- ``` -->

## Extension - gganimate

We cannot cover all these extensions in detail here, but as one last example here we use the [**gganimate**]{style="color: #ae7b11;"} package (also installed and loaded in a previous step) to create an animation that iterates through each date and adjusts the axis relative to the current data:

```{r gganimate_show}
animated_chart <- price_chart + transition_states(Date) + view_follow() + geom_point()
```

View the results in the slide below ⬇️

## gganimate results

<!-- The object **`animated_chart`** is now an animated GIF showing many charts: -->

```{r animate_chart}
animate(animated_chart, fps=2, height = 350, width =500)
```

<small> We wrapped [**animated\_chart**]{style="color: blue;"} in the function [**animate()**]{style="color: green;"} to slow down and resize the GIF shown. We will stop here, but we encourage you to keep exploring the possibilities of [**ggplot2**]{style="color: #ae7b11;"}. Move on to the next slide `r emo::ji('right arrow')` to start making predictive models! </small>

<!-- ```{r gganimate_run, echo=F, fig.width=2.8, fig.height=2} -->

<!-- animate(price_chart + transition_states(Date) + view_follow() + geom_point(),fps=1) -->

<!-- ``` -->


# Predictive Models

<span style="font-size: 65%;">
Now we can start thinking about how we will make predictions into the future. We will take information/data observed in the past, and use it to make forecasts about the future. Each row in the data already comes with the <span style="color: purple;">**target variable**</span>, meaning what we want to predict. The column <span style="color: blue;">**Target24HourClose**</span> is the only information in the dataset that refers to a point in the future. It is **really important** to make sure that all the data we are using to make predictions would actually be available at the time we want to perform a new prediction. If the model was trained on information only available once it was too late to act on it, we might run into <span style="color: purple;">***target leakage***</span>, which may give us the impression the models are much better than they actually are. [Click here to read more about this idea](https://www.datarobot.com/wiki/target-leakage/#:~:text=It%20happens%20when%20you%20train,you%20collect%20in%20the%20future.&text=To%20avoid%20target%20leakage%2C%20omit,time%20of%20the%20target%20outcome.), and how this is similar to having the answer sheet to an exam and how it would not help on the next different exam; we want the model to have a proper understanding of the problem rather than one answer sheet. **Beware results that are too good to be true** and always double check your work!
</span>


## Overview

Therefore, we will:

<span style="font-size: 65%;">
1. Take data from the past, where all columns are associated with a specific point in time, with the exception of the <span style="color: blue;">**Target24HourClose**</span> column, which represents the price of the cryptocurrency 24 hours in the future relative to the rest of the columns. </span>

<span style="font-size: 65%;">
2. Choose a model, which will determine the approach the computer will take to draw statistical relationships between the columns. </span>

<span style="font-size: 65%;">
3. Consider ways of reducing the risk of real-world results not matching the accuracy of the original model's results. </span>

<span style="font-size: 65%;">
4. Make forecasts about the future. </span>


We will start with a simple <span style="color: purple;">***linear regression***</span> model in the slide below ⬇️

## Simple Linear Regression Model

Because we are looking to predict numeric values (rather than a 0/1 outcome), we could make a very straightforward model using the **Linear Regression** algorithm. We will not dive into the mathematical formula or specifics around linear regression here, you can take a deeper dive into linear regression here if you would like: https://uc-r.github.io/linear_regression

```{r make_lm_execute, class.output="scroll-150"}
lm(data = cryptodata_train, formula = "Target24HourClose ~ .")
```

## "lm" model - Continued

<span style="font-size: 70%;">
In the previous slide, we were able to make a linear regression model by using the function <span style="color: green;">**lm()**</span> and specifying the data to be used as <span style="color: blue;">**cryptodata_train**</span>, and the formula as predicting the <span style="color: blue;">**Target24HourClose** </span> and using (**~**) all other variables (represented by the **period "."**). Here is the code again for reference:</span>

```{r make_lm_ref, eval=F}
lm(data = cryptodata_train, formula = "Target24HourClose ~ .")
```

<span style="font-size: 55%;">
If we wanted to implement a different model, there may be different structural requirements to the data used by the model, and we may have more parameters to adjust, so in some cases it may not be very straightforward to implement a different model, even though we intend to use the same dataset and the same prediction formula predicting the <span style="color: blue;">**Target24HourClose** </span> column using all other columns. Instead of figuring out the implementation of different models, we can use a framework that has standardized the usage of many models using the <span style="color: #ae7b11;">**caret**</span> package.</span>

<!-- The elements we specified for this model wouldn't be different for a more complex predictive model, but the implementation would usu -->

<!-- ## Output  -->

<!-- The output that was cutoff in the previous slide, describes the  -->

<!-- ```{r show_lm_equation, results='asis', echo=F} -->
<!-- library(equatiomatic) -->
<!-- extract_eq(lm(data = cryptodata_train, formula = "Target24HourClose ~ .") -->
<!-- ,use_coefs  = T, wrap=T, terms_per_line = 2) -->
<!-- ``` -->


## Reproducibility - Set seed

<span style="font-size: 60%;">
When making predictive models, some functions we will use next will involve some degree of randomness. Computers are not ***truly*** random, we will not dive into this topic here, but essentially we can control the randomness by specifying a number on which the randomness depends on, and get the same "random" results by doing so. Without this step, the results on your computer might be slightly different than the ones shown here. We can make sure the results are the same (and reproducible) by running the function from base R <span style="color: green;">**set.seed()**</span>:
</span>

```{r set_seed}
set.seed(123) # arbitrarily set to 123, can be any number
```

<span style="font-size: 60%;">
This ensures the "randomness" begins from the same starting point on your computer as it does in the example code being shown. [Click here to find out more about this concept](https://stats.stackexchange.com/questions/86285/random-number-set-seedn-in-r). You will see this before training models because there is a degreee of "randomness" involved, and in order to get the same exact results you will need to set the same seed.
</span>

<!-- ## set.seed() - continued -->

<!-- Normally we would set the seed before running a specific piece of code, and we would need to run <span style="color: green;">**set.seed(123)**</span> each time before running the code with the random aspect to it. To make the code more minimal and clear however, let's set the seed for the entire session. All you need to understand about the code below, is that when you run it you will set the seed to be the number 123 for the entire R session instead of once: -->

<!-- ```{r set_seed_whole_session, results=F} -->
<!-- addTaskCallback(function(...) {set.seed(123);TRUE}) -->
<!-- ``` -->

<!-- ***Code sourced from: https://stackoverflow.com/questions/20624698/fixing-set-seed-for-an-entire-session*** -->


## Caret Package

<!-- (CLEAN UP A BIT?) -->

If we [think back to the approach we took with <span style="color: #ae7b11;">ggplot</span>](#visualizing-data), we identified which parts of the code would need to change; we specified the data and axis first, and we were then free to add points to the chart as lines or dots or any other "geom". We will take a similar approach here for predictive modeling. As already pointed out earlier, if we made a different predictive model **we would still be predicting the value of the** <span style="color: blue;">**Target24HourClose**</span> **column**, and we would still use the rest of the independent variables as predictors. What usually changes between predictive models, is the implementation and use of them, as they may have different structural requirements and parameters for their specific implementation.

## Caret Package - Continued

The caret package is one of [several solutions](https://parsnip.tidymodels.org/) to resolve the problem described in the previous slide. It allows the user to specify the data, variables to use and everything else that would not change regardless of the model we want to use. As a separate parameter, we can specify which modeling framework to use (i.e. linear regression). Different models will have different parameters, and we will be able to adjust those parameters depending on the model, or leave those blank to use the sensible default values set by the [creator of the package](https://rstudio.com/speakers/max-kuhn/).

See this idea in action in the slide below ⬇️

```{r message=FALSE, warning=FALSE, echo=F}
# Hidden code to speed things up
library(doParallel)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
```

## Caret - Simple Linear Regression
```{r simple_lm}
set.seed(123)
lm_fit <- train(Target24HourClose ~ .,
                    data = cryptodata_train,
                    method = "lm")
# Show fit
lm_fit
```

## "Method" Options

You can find the entire list of models supported by caret, as well as the <span style="color: blue;">**method**</span> value to enter to produce the specified model at the page below:

```{r show_caret_models_list, echo=F}
knitr::include_url("https://topepo.github.io/caret/available-models.html",
  height = "600px")
```
*[https://topepo.github.io/caret/available-models.html](https://topepo.github.io/caret/available-models.html)*

## 

![](C:/Users/ries9/Documents/Research-Paper-Example/images/AvailableModels_InDepth.png)

The website from the previous slide is all you need in order to make any of the 238 (currently) available model choices. The columns circled and labeled as **1** and **2** are explained on the next slide below ⬇️

## "Method" Options continued 

1. <span style="color: blue;">***method***</span> **Value:** Two slides back/up we specified <span style="color: blue;">**method = "lm"**</span> for a linear regression model. You can make any of the listed models by changing the <span style="color: blue;">**method**</span> **Value** which would be ***adaboost*** for the model highlighted in the screenshot above.

<!-- ^Removed: This is the option as you would write it in your code for the **`train()`** function of the **`caret`** package.  -->

2. **Libraries:** This tells us what packages need to be installed for the model to be able to run. Before we would be able to run the model on the first row of the screenshot, we would need to install the required package by running <span style="color: green;">**install.packages("fastAdaboost")**</span> (for the highlighted model) or else we would run into an error when training the model.


## XGBoost

The <span style="color: purple;">**XGBoost**</span> predictive modeling framework has proven itself to be very powerful in recent years on a variety of different problems, so that's the next model we will make. XGBoost stands for E**x**treme **G**radient **B**oosting, which is a mathematical concept we won't tackle in this tutorial, but you can learn more about the high-level idea and how this model uses it in the context of <span style="color: purple;">***Decision Tree Ensembles***</span> here: https://xgboost.readthedocs.io/en/latest/tutorials/model.html

Move on to the next slide below where we keep all the same code, but change the <span style="color: blue;">**method**</span> to make an XGBoost model.

## XGBoost - Caret

We can use the same code we used when making the linear regression model, but specify the <span style="color: blue;">**method**</span> to be <span style="color: blue;">**xgbLinear**</span> instead of the **lm** value we used before:

```{r xgboost, message=FALSE, warning=FALSE, class.output="scroll-150"}
set.seed(123)
xgb_fit <- train(Target24HourClose ~ . ,
                    data = cryptodata_train,
                    method = "xgbLinear")
# Show results
xgb_fit
```

You may have noticed that both outputs said ***Resampling: Bootstrapped (25 reps)***. This is because of a default value on an additional option of <span style="color: green;">**train()**</span>, go to the next slide below to learn more about additional options/functionality.

## Caret Additional Options

Some models, like a **S**upport **V**ector **M**achine model, need the data to be <span style="color: purple;">***centered***</span> and <span style="color: purple;">***scaled***</span> for each column, [click here to learn why that is the case](https://stats.stackexchange.com/questions/65094/why-scaling-is-important-for-the-linear-svm-classification). We could make this change adding the parameter <span style="color: blue;">**preProc = c("center", "scale")**</span> to the <span style="color: green;">**train()**</span> call:

```{r svm, class.output="scroll-150"}
set.seed(123)
train(Target24HourClose ~ . ,
          data = cryptodata_train,
          method = "svmPoly",
          preProc = c("center", "scale"))
```

The output that said ***Resampling: Bootstrapped (25 reps)*** was referring to a different option <span style="color: blue;">**trControl**</span> of the <span style="color: green;">**train()**</span> function.

## Cross Validation for timeseries

```{r cross_validate_timeseries}
train_control <- trainControl(method = "timeslice",
                              initialWindow = 100*24,
                              horizon = 30*24,
                              fixedWindow = TRUE,
                              number = 5)
```

[![](C:/Users/ries9/Documents/Research-Paper-Example/images/caret_cross_validate.png){width=62%}](https://topepo.github.io/caret/data-splitting.html#data-splitting-for-time-series)

<small>
*Source: https://topepo.github.io/caret/data-splitting.html#data-splitting-for-time-series*</small>

## Cross Validation for timeseries

In the previous slide we set the timeseries cross-validation to take an <span style="color: blue;">**initialWindow**</span> of 150 days (given by 24 hourly time points multiplied by 150 days), and a <span style="color: blue;">**fixedWindow**</span> of 30 days. The parameter <span style="color: blue;">**number = 5**</span> specifies the process should take place on 5 separate occasions. By the end of this process, we end up with 5 separate ***training*** and ***testing*** datasets. If a model works well across 5 separate periods of time, we can have greater confidence things will go well when we apply the model to the real world as well. Next, let's run the XGBoosst model with the specified cross-validation.


## XGBoost with Time-Aware Cross Validation
```{r xgboost_cv, message=F, warning=F, class.output="scroll-150"}
set.seed(123)
xgb_fit <- train(Target24HourClose ~ . ,
                    data = cryptodata_train,
                    method = "xgbLinear",
                    trControl = train_control)
# Show fit
xgb_fit
```


## Hyperparameter tuning

We will not discuss this at length here, but because we are able to assess the accuracy of the models we make by creating a test set with data that the model has never seen before, we could keep making slight changes to the parameters used by the models. What is the ideal number of trees for the XGBoost model to have? It might be the default value specified by the <span style="color: #ae7b11;">**caret**</span> package, but an alternative approach would be to specify a range we find to be sensible, and have the computer iterate over the range to find the most optimal solution. You can read more about hyperparameter tuning in <span style="color: #ae7b11;">**caret**</span> here: https://topepo.github.io/caret/model-training-and-tuning.html

## Hypterparameter tuning

Caret as a default behavior will perform hyperparameter tuning, we can view the results by accessing the column of the ***fitted model*** we made for XGBoost that is called <span style="color: blue;">***bestTune***</span>. To do this, we start with the <span style="color: blue;">**xgb_fit**</span> object followed by a dollar sign and the name <span style="color: blue;">**bestTune**</span> to indicate we want to return the table with the best found values for the different parameters:

```{r show_xgb_tuned_parameters}
xgb_fit$bestTune
```


## Make Predictions on New Data

We can use the <span style="color: green;">**predict()**</span> function to make predictions, supplying the model to use as <span style="color: blue;">**xgb_fit**</span> and making predictions on the dataset <span style="color: blue;">**cryptodata_test**</span>

```{r xgboost_predictions}
xgb_pred <- predict(xgb_fit, newdata = cryptodata_test)
```


## Evaluate Model Performance

<span style="font-size: 70%;">
Because we actually know what really happened for the target variable in the test data we used in the previous step, we can get a good idea of how good the model performed on a dataset it has never seen before. We do this to avoid <span style="color: purple;">**overfitting**</span>, which is the idea that the model may work really well on the training data we provided, but not on the new data that we want to predictions on. If the performance on the test set is good, that is a good sign. If the data is split into several subsets and each subset has consistent results for the training and test datasets, that is an even better sign the model may perform as expected. Read more about measuring performance here: https://topepo.github.io/caret/measuring-performance.html
</span>

```{r accuracy_measures}
postResample(pred = xgb_pred, obs = cryptodata_test$Close)
```

## Error Metrics

In the previous slide, by using the  <span style="color: green;">**postResample()**</span> function we returned three columns:

<small>
1.  <span style="color: purple;">**MAE**</span>: The **M**ean **A**bsolute **E**rror measures the average error between the predictions and what actually happened as an absolute value (MAE could never be less than 0).

2.  <span style="color: purple;">**RMSE**</span>: The **R**oot **M**ean **S**quared **E**rror takes the **square root of the average squared errors** and behaves slightly differently than MAE depending on the magnitude of the error, but would always be larger or equal to MAE. You can learn more about the differences between the two here: https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d

3.<span style="color: purple;">**Rsquared**</span>: The **R Squared** metric gives us a general indication of how informative the independent variables are in relation to the dependent variable (<span style="color: blue;">**Target24HourClose**</span>), where a perfect score of 1.0 would suggest the price movements of the dependent variable are completely explained by the independent variables and a score of 0 would suggest the opposite. This metric should be taken with a grain of salt, but taken in conjunction with the RMSE or MAE error metrics it can be informative. Learn more about R Squared here: https://www.investopedia.com/terms/r/r-squared.asp

</small>

<!-- ## What now? -->

<!-- Here discuss how to use model to make predictions for new data, etc..? -->

# Timeseries Data Prep

In the previous section, we performed the cross-validation relative to the date and time that the data was collected, because we would have risked overfitting our models otherwise. Dealing with data that is a sequence in time also has a positive side to it however. In this section, we will learn to extrapolate seasonal patterns from the data to make forecasts about the future, and the greater flexibility it will provide relative to the supervised machine learning models we made in the previous section.

<small>
***Much of the content for this section was created referencing this talk at the 2019 RStudio conference: https://rstudio.com/resources/rstudioconf-2019/melt-the-clock-tidy-time-series-analysis/***
</small>

<!-- ## IF NEED TO SCALE DATA -->
<!-- ```{r scale_temp_del} -->
<!-- cryptodata$Close <- scale(cryptodata$Close,center = T, scale = T) -->
<!-- ``` -->

## The "tsibble" data structure

<span style="font-size: 97%;">
Up to this point, when we referred to tabular data, we were working with a data structure called <span style="color: purple;">**tibble**</span>. **Hadley Wickham** is the creator of packages we have gone over like <span style="color: #ae7b11;">dplyr</span>, <span style="color: #ae7b11;">ggplot2</span> and many others, including the <span style="color: #ae7b11;">**tibble**</span> package. The default way for R to read tabular data is as a <span style="color: purple;">**dataframe**</span>, which can be converted to a ***tibble*** and viceversa. The main difference between the two, is a tibble will show the results in a much neater fashion when there are many columns and/or rows. Next, we will convert the data from being a **tibble** to a <span style="color: purple;">**t*****s*****ibble**</span> (notice the extra ***s***), which will be recognized as a time series dataset that was collected over an even time interval, and will have many useful properties for the time series analysis we will perform next.
</span>


## Make a "tsibble"

<span style="font-size: 80%;">
We can create a <span style="color: #ae7b11;">**tsibble**</span> object using the <span style="color: blue;">**as_tsibble()**</span> function, and using the <span style="color: blue;">**DateTime** field as the <span style="color: blue;">**index**</span> of the tsibble:</span>
```{r make_tsibble}
cryptodata_ts <- as_tsibble(cryptodata, index = DateTime)
```

<span style="font-size: 70%;">
The index represents the Date/Time field of when the data was collected. If we were dealing with multiple cryptocurrencies, we would also specify the <span style="color: blue;">**key**</span> parameter as being the name of the cryptocurrency, because a tsibble needs to have a unique combination of the <span style="color: blue;">**index**</span> and <span style="color: blue;">**key**</span>. Meaning, if the time series data was collected once hourly, we would only expect one row every hour, and we cannot have duplicates, which is why if we are tracking more than one series at a time we would have to specify the group the rows belong to using the <span style="color: blue;">**key**</span> parameter as well. Here we only have one series, so we don't need a <span style="color: blue;">**key**</span>.</span>

## tsibble preview

If we print the new object that we created in the previous slide <span style="color: blue;">**cryptodata_ts**</span>, the top of the output will now show the text **"A tsibble: 22,847 x 18 [1h] \<UTC\>"** which specifies the dimensions (rows X columns), 1 hour intervals between rows, and the UTC timezone:

```{r show_cryptodata_ts, class.output="scroll-150"}
cryptodata_ts
```

## Dealing with gaps

Almost all timeseries models will require a complete dataset with no missing gaps for the index we specified. We can check whether our data has any gaps using the function <span style="color: green;">**has_gaps()**</span>:

```{r has_gaps}
has_gaps(cryptodata_ts)
```

## Dealing with gaps - Continued

We can find the gaps using the <span style="color: green;">**scan_gaps()**</span> function:
```{r scan_gaps}
scan_gaps(cryptodata_ts)
```

And we can turn these missing values into **explicit** missing values that the timeseries can recognize as null with <span style="color: green;">**fill_gaps()**</span>:
```{r fill_gaps}
cryptodata_ts <- fill_gaps(cryptodata_ts)
```

## fill_gaps recap

When we created the **tsibble**, we specified an <span style="color: blue;">index</span>, which allowed the function to figure out our data is in 1 hour intervals. It was also able to figure out the range of the data. Based on that information, it is able to detect that we have 1 gap in our timeseries dataset. We were able to add this missing row of data using <span style="color: green;">**fill_gaps()**</span>, which explicitly defines the row as missing and avoids issues moving forward. Printing the new overwritten version of <span style="color: blue;">**cryptodata_ts**</span>, the dimensions have changed to have 1 additional row (22,848):
```{r show_cryptodata_ts_additional_row}
cryptodata_ts
```

## Train/Test Split

As a last step, let's again split the data into a **train** and **test** dataset. We will build the models on the earlier 80% of the data, and assess how well the models perform on the last 20% of the data:

```{r}
ts_train <- head(cryptodata_ts, as.integer(nrow(cryptodata)*.8))
```

And the test data:
```{r}
ts_test <- tail(cryptodata_ts, as.integer(nrow(cryptodata)*.2))
```


# Timeseries Predictive Modeling

Next, we will use the ***fable*** package to create timeseries models. Why is it called fable?

"

1. It makes **f**orecasting t**able**s

2. A fable is like a forecast: it's never true, but it tells you something important about reality

" 

-***Rob J Hyndman***

## Building a model

All the models made from this point are made using the <span style="color: #ae7b11;">**fable**</span> package. The simplest type of timeseries model we can make, is a <span style="color: purple;">***Naive***</span> model, which simply takes values from the past to forecast the future. Here we make one using the closing price from the previous hour to predict the future closing price:
```{r naive_model}
set.seed(123)
naive_model <- model(ts_train, naive = NAIVE(Target24HourClose))
# Show model
naive_model
```


## The "mable" data structure

In the previous slide you may have noticed the output produced a <span style="color: purple;">***mable***</span>, which stands for **m**odel **t**able. Just like we [saw earlier with the tibble and t**s**ibble data structures](the-tsibble-data-structure), a **mable** has properties which will help evaluate the efficacy of the models later on.


## Naive Model Rationale

A Naive model like this is usually not very useful for forecasting, but it can give us a good baseline to compare against. If we made a model that performed worse than a Naive model, chances are that is not very informative. We will also add a very similar model called a <span style="color: purple;">**seasonal naive**</span> model, which we will call ***snaive***. In a later step we will discuss the difference between looking at the trend of the previous values vs. the trend from the previous ***season***.

## Show Naive Model Predictions

We can use the <span style="color: green;">**augment()**</span> function to compare the results for each row looking at the real result, the prediction (<span style="color: blue;">***.fitted***</span> in output below) and the difference between those two values, corresponding to the error of the prediction (<span style="color: blue;">***.resid***</span>):
```{r augment_naive, class.output="scroll-150"}
augment(naive_model)
```

<!-- ## ETS and ARIMA -->

<!-- The Naive model will be our baseline, but against what? The two popular frameworks we will cover here are **ETS** and **ARIMA**. -->

<!-- At a **very** high level, if we are working with a **stationary** dataset we would probably prefer an ETS model, and if the data were non-stationary, we might want to use an ARIMA model instead. In the [full version](https://cryptocurrencyresearch.org/) we make both ETS and ARIMA models and compare the results, but to keep things more straightforward we will only make an ARIMA model here. -->

## A note on the target variable

Previously we defined the **target variable** as the closing price 24 hour in the future relative to the rest of the variables; this time we are using tools that "understand" time, and after making a model we will be able to make a forecast for however long in the future we would like. This time therefore, we will assume all columns reference the same exact point in time, we will pick the one we are interested in predicting (**the** ***Close*** **price**), and make a forecast that goes as far out as we need it to. Therefore, we will switch to the **Close** price as our target variable, and be very careful to **not** include the variable called ***Target24HourClose*** as a dependent variable, or we would run into <span style="color: purple;">**target leak**</span> and end up with a meaningless model.
<!-- [as explained in a previous slide](#a-note-on-overfitting). -->

<!-- Previously we outlined our formula as **Target24HourClose ~ .**, which means we specified the Target24HourClose variable as the dependent variable, and all other columns (denoted as a period ".") as the independent/predictor variables. When doing this, we were including the  -->


## Make multiple models

Let's create a new object like we did for the naive model earlier, but this time including a naive model, a snaive model and an <span style="color: purple;">**ARIMA**</span> model (which will be explained next) at the same time:

<!-- HERE SHOULD ADD SNAIVE(Close) AS WELL, WOULD BE MORE INTERESTING IN FINAL RESULTS! -->

```{r multiple_ts_models}
set.seed(123)
ts_model <- model(ts_train, naive = NAIVE(Close),
                            snaive = SNAIVE(Close),
                            arima = ARIMA(Close))
```

```{r show_ts_models}
ts_model$arima[[1]]
```


## ARIMA

<small>
<span style="color: purple;">**ARIMA**</span> stands for **A**uto**R**egressive **I**ntegrated **M**oving **A**verage. Looking at the code from before, you can see there were several terms that were added with the prefixes of <span style="color: blue;">**ar**</span> and <span style="color: blue;">**ma**</span>. The **terms** that start with the name <span style="color: blue;">**ar**</span> refer to the **A**uto **R**egressive components, while the **terms** that starts with the prefix of <span style="color: blue;">**ma**</span> refer to **M**oving **A**verages. The **I**ntegrated part of ARIMA is meant to help eliminate trends in the time series data to make it more stationary ([click here for an article explaining this idea in more detail](https://link.medium.com/zPs3yrt7s8 )). </small>

**From https://people.duke.edu/~rnau/411arim.htm:**

A nonseasonal ARIMA model is classified as an "ARIMA(p,d,q)" model, where:

- **p** is the number of **auto-regressive terms**,

- **d** is the number of **nonseasonal differences needed for stationarity**, and

- **q** is the number of **lagged forecast errors in the prediction equation**.


## ARIMA - Fable Documentation

![non-seasonal components](C:/Users/ries9/Documents/Research-Paper-Example/images/arima_pdq.png)

## ARIMA - Fable Documentation

![seasonal components](C:/Users/ries9/Documents/Research-Paper-Example/images/arima_pdq_caps.png)

## ARIMA Terms

The naive models doesn't have any *terms*, but ARIMA does and we can view them using the function <span style="color: green;">**tidy()**</span>:

```{r tidy_arima}
tidy(ts_model)
```

<span style="font-size: 88%;">
In the <span style="color: blue;">**term**</span> column, we can see there are three auto-regressive terms (<span style="color: blue;">***ar***</span>) and two seasonal auto-regressive terms (<span style="color: blue;">***sar***</span>) which correspond to the <span style="color: blue;">**p**</span> and <span style="color: blue;">**P**</span> terms from the documentation shown.</span>

## ARIMA Terms - Continued

In the previous slide, the output showed three auto-regressive terms, and two seasonal auto-regressive terms. We can see this by showing the <span style="color: blue;">**ts_models**</span> object:
```{r show_ts_models_arima}
ts_model
```

The ARIMA model has terms as described earlier for **(p, d, q)(P, D, Q)**, shown above as <span style="color: blue;">**(3,1,0)(2,0,0)**</span>. This summarizes the ARIMA model go to the next slide to get a better understanding of what this tells us.

## ARIMA Terms - Continued

![](C:/Users/ries9/Documents/Research-Paper-Example/images/arima_labelled.PNG)

<!-- As labeled in red above: -->

1. The <span style="color: green;">**ARIMA()**</span> function we used automatically picked a value for <span style="color: blue;">**p**</span> of **3**, meaning there are three non-seasonal auto-regressive terms (as seen two slides back).

2. The <span style="color: blue;">**d**</span> term was set to one, meaning the data was differenced once to try and achieve stationarity (a note on that in two slides).

3. The <span style="color: blue;">**P**</span> term was set to a value of two, indicating there are two seasonal auto-regressive terms.
<!-- (which is why we saw the terms ***sar1*** and ***sar2*** when we used the command **`tidy(ts_models)`**) -->


<!-- tells us it automatically selected 3 non-seasonal auto-regressive terms, differenced one time (to try and achieve stationarity, see note in next slide) and a value of 0 for q means the term is not used in the model; it then selected 2 seasonal auto-regressive terms, and did not use D or Q to construct the model. -->


## ARIMA Terms - Simplified

ARIMA will add new variables for the specified terms <span style="color: blue;">**p, P**</span> and <span style="color: blue;">**q, Q**</span>, which could have been manually set as well. When the parameter <span style="color: blue;">**p**</span> is equal to three, that means the model will calculate and leverage three new columns each being the previous value of the <span style="color: blue;">**Close**</span> column one, two, and three rows/hours in the past. The difference for the <span style="color: blue;">**P**</span> parameter, is the fact that the lagged values are added looking at the previous <span style="color: purple;">**seasonal**</span> value, where instead of taking the value from the previous hour, it would take the value from last quarter. The <span style="color: blue;">**q**</span> term on the other hand, would add new columns with <span style="color: purple;">**M**oving **A**verages</span> (the **MA** of ARIMA) averaging the value of the <span style="color: blue;">**Close**</span> column over a specified number of hours in the past (quarters when using <span style="color: blue;">**Q**</span> instead of <span style="color: blue;">**q**</span>).


## A note on stationarity

Please note that ARIMA works best on data with <span style="color: purple;">**stationary**</span> trends, meaning a time series with a mean and variance that is constant over time (**which this data is not**). The **I** in ARIMA refers to the <span style="color: blue;">**d, D**</span> terms, which determine how much of an adjustment to make (if any) to the data to make it stationary. If you had data that was clearly non-stationary, an <span style="color: purple;">ETS</span> model may be more appropriate, we will not cover it here, but you can learn more about ETS models here: https://fable.tidyverts.org/reference/ETS.html

There are ways of making both models and selecting the one with the better cross-validation results, or you can find an example of testing the stationarity of your data here: https://rpubs.com/richkt/269797

<!-- It is also important to note that in the previous slide we saw the **d**, **Q**, **D** terms were set to 0 meaning these terms were not used, which means we actually made an ARI model, since the MA refers to the **d** and **D** terms and neither is present. -->

<!-- ## Compare results -->

<!-- We can use the **`augment()`** function like we did before, and this time the results  -->
<!-- ```{r augment_ts_models} -->
<!-- augment(ts_models) -->
<!-- # could plot: ggplot(augment(ts_models), aes(x=DateTime, y=.resid, color = .model)) + geom_line() -->
<!-- ``` -->


## Forecast

We can use the <span style="color: green;">**forecast()**</span> function on the <span style="color: blue;">**ts_models**</span> object we created earlier to generate forecasts for a given timeframe for all models found in <span style="color: blue;">**ts_models**</span>:

```{r forecasts}
ts_forecasts <- forecast(ts_model, h = '30 days')
```

The new object contains hourly forecasts for a 30 day period for both models:

```{r forecasts_results, echo=F}
ts_forecasts
```

## Forecast - Continued

The new output <span style="color: blue;">**ts_forecasts**</span> gives us the normal distribution of a prediction. The predictions that are made this time are not simple numbers with error metrics, but ranges of a prediction. As more time goes on, we get less confident about our predictions about the future. Here, we can extract the 80% and 95% confidence intervals of the forecasts:

```{r forecasts_confidence_dist, eval=F}
hilo(ts_forecasts, level = c(80, 95))
```

See the output in the slide below. Learn more about confidence intervals: https://online.stat.psu.edu/stat200/lesson/4/4.2

## Forecast - Continued

```{r forecasts_confidence_dist_run, echo=F}
select(hilo(ts_forecasts, level = c(70, 95)), `70%`, `95%`)
```

<span style="font-size: 85%;">
You could read the results in the first row of the output as: *we are 70% confident the price on 2020-01-31 14:00:00 will be between about \$168 and \$188, and we are 95% confident it will be between \$159 and \$197*. As we look at predictions farther out into the future, we will have a larger range of values because it becomes harder for us to be very confident.</span>

## Plot Forecasts

We can plot the forecast and the confidence intervals:

```{r, fig.width=6.5, fig.height=4.5}
autoplot(ts_forecasts)
```


## Plot Forecasts - Continued

<span style="font-size: 65%;">
The results for the <span style="color: blue;">naive</span> approach covered the results for the <span style="color: blue;">arima</span> model. Because the confidence intervals are going to be hard to visually compare, we can instead use what we learned earlier in the [visualization section of this tutorial](#visualizing-data) to plot the average value of the given distribution of forecasts:</span>

```{r ts_plot_average_forecasts, fig.width=4.5, fig.height=2.5}
ggplot(ts_forecasts, aes(x=DateTime, y=.mean, color=.model)) + geom_line()
```

## View Forecasts
<span style="font-size: 70%;">
Anytime we make a chart in using <span style="color: green;">**gglot()**</span>, we can wrap the results in a function from the <span style="color: #ae7b11;">**plotly**</span> package called <span style="color: green;">**ggplotly()**</span>. We won't show the code to leave more room to interact with the chart (try hovering over it with your mouse) below:</span>

```{r ts_plot_average_forecasts_plotly, fig.width=6.5, fig.height=4.5, echo=F}
ggplotly(ggplot(ts_forecasts, aes(x=DateTime, y=.mean, color=.model)) +
           geom_line() + 
           xlim(c(as.POSIXct('2020-01-31 14:00:00', format = "%Y-%m-%d %H:%M:%S"),
     as.POSIXct('2020-02-05 00:00:00', format = "%Y-%m-%d %H:%M:%S"))))
```



## Forecast - Recap

<span style="font-size: 85%;">
1. **Now we can define how far out to make predictions for.** With the models we made with **`caret`** we defined the **target variable** as the price 24 hours into the future, and built a model to predict the Close price 24 hours into the future. If we were interested in any other forecast, we would need to need to adjust the target variable in the data and create a new model, making sure only one variable is related to future values.</span>

<span style="font-size: 85%;">
2. The resulting **forecast is a distribution of values**, we can take the mid-point of the distribution to return what we think the most likely scenario is, but we could also explore more unlikely scenarios by looking at the forecast for more extreme confidence intervals.
</span>


## Accuracy on Test set

```{r test_set_accuracy}
accuracy(ts_forecasts, ts_test)
```

<!-- ## Visually compare results -->


## Note

The **fable** package is specifically designed to work the exact same way no matter how many different models you would like to make or how many cryptocurrency you would like to model. Head over to the [full version of the tutorial](https://cryptocurrencyresearch.org/) to see these ideas applied to a more "complete" approach to predicting prices on the cryptocurrency markets.


# Next steps

- Full version of the tutorial: https://cryptocurrencyresearch.org/




# References

References on the slide below ⬇️ in the order in which they are referenced throughout this presentation.

## R Package - pacman {#pacman-citation}

Rinker, T. W. & Kurkiewicz, D. (2017). pacman: Package Management for R. version 0.5.0. Buffalo, New York. http://github.com/trinker/pacman

</br>

http://trinker.github.io/pacman/


## R Package - pins {#pins-citation}

<!-- ```{r pins_citation} -->
<!-- citation("pins") -->
<!-- ``` -->

</br>

Javier Luraschi (2020). pins: Pin, Discover and Share Resources. R package version 0.4.0. https://CRAN.R-project.org/package=pins
  
</br>

https://pins.rstudio.com/


## Illustrations by Allison Horst

</br>

https://github.com/allisonhorst/stats-illustrations


## R Package - dplyr {#dplyr-citation}

</br>

Hadley Wickham, Romain François, Lionel Henry and Kirill Müller (2020). dplyr: A Grammar of Data Manipulation. R package version 1.0.2. https://CRAN.R-project.org/package=dplyr
  
</br>
  
https://dplyr.tidyverse.org


## R Package - ggplot2 {#ggplot-citation}

</br>

H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.
  
</br>
  
https://ggplot2.tidyverse.org

## R Package - caret {caret-citation}

</br>

Max Kuhn (2020). caret: Classification and Regression Training. R package version 6.0-86. https://CRAN.R-project.org/package=caret

</br>

http://topepo.github.io/caret/


## R Package - tsibble {#tsibble-citation}

</br>

Wang, E, D Cook, and RJ Hyndman (2020). A new tidy data structure to support exploration and modeling of temporal data. Journal of Computational and Graphical Statistics. https://doi.org/10.1080/10618600.2019.1695624.

</br>

https://tsibble.tidyverts.org/


## Timeseries

- https://www.tandfonline.com/doi/full/10.1080/10618600.2019.1695624

- https://rstudio.com/resources/rstudioconf-2019/melt-the-clock-tidy-time-series-analysis/

## ARIMA 

- add resources used


## ADD HERE...


  


<!-- ```{r stop_cluster} -->
<!-- stopCluster(cluster) -->
<!-- ``` -->









