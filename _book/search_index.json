[["index.html", "Cryptocurrency Research Section - 1 Introduction 1.1 What will I learn? 1.2 Before Getting Started 1.3 Format Notes 1.4 Plan of Attack 1.5 GitHub Repository 1.6 Disclaimer", " Cryptocurrency Research Last Updated: 2020-11-05 17:07:33 Section - 1 Introduction (this book is a work in progress and is NOT ready) Welcome to this programming tutorial on machine learning! In this tutorial we will use the latest data from the cryptocurrency markets to provide a hands-on and complete example that anyone can follow along with. 1.1 What will I learn? In this tutorial you will primarily learn about tools for the R programming language developed by RStudio and more specifically the tidyverse. If you are not familiar with these open source products, don’t worry. We’ll introduce these throughout the tutorial as needed. The focus of the tutorial is on supervised machine learning, a process for building models that can predict future events, such as cryptocurrency prices. You will learn how to use the caret package to make many different predictive models, and tools to evaluate their performance. Before we can make the models themselves, we will need to “clean” the data. You will learn how to perform “group by” operations on your data using the dplyr package and to clean and modify grouped data. You will learn to visualize data using the ggplot2 package, as well as some powerful tools to extend the functionality of ggplot2. You will gain a better understanding of the steps involved in any supervised machine learning process, as well as considerations you might need to keep in mind based on your specific data and the way you plan on acting on the predictions you have made. Each problem comes with a unique set of challenges, but there are steps that are necessary in creating any supervised machine learning model, and questions you should ask yourself regardless of the specific problem at hand. You will also learn a little about cryptocurrencies themselves, but this is not a tutorial centered around trading or blockchain technology. 1.2 Before Getting Started You can toggle the sidebar on the left side of the page by clicking on the menu button in the top left, or by pressing on the s key on your keyboard. You can read this document as if it were a book, scrolling to the bottom of the page and going to the next “chapter”, or navigating between sections using the sidebar. This document is the tutorial itself, but in order to make the tutorial more accessible to people with less programming experience (or none) we created a high-level version of this tutorial, which simplifies both the problem at hand (what we want to predict) and the specific programming steps, but uses the same tools and methodology providing easier to digest examples that can be applied to many different datasets and problems. 1.2.1 High-Level Version If you are not very familiar with programming in either R or Python, or are not sure what cryptocurrencies are, you should definitely work your way through the high-level version first. If you are an intermediate programmer we still recommend the high-level version first. Below is an embedded version of the high-level version, but we recommend using any of the links above to view it in its own window. Once in its own window it can be full-screened by pressing on the f button on your keyboard. When following along with the high-level tutorial embedded above, the results will be completely reproducible and the document is static and does not update over time meaning your results will exactly match those shown. The document you are currently reading this text on on the other hand updates every 12 hours. You will be able to access the same data source, but it updates hourly by the 8th minute of every hour with the most recent data, so this document and your results won’t perfectly match. Whenever text is highlighted this way, that means it is a snippet of R code, which will usually be done to bring attention to specific parts of the code. 1.3 Format Notes Are you a Bob, Hosung, or Jessica below? This section provides an example of how different users may want to approach the tutorial differently based on their levels of experience. You can hide the sidebar on the left by pressing the s key on your keyboard. The cryptocurrency data was chosen to produce results that change over time and because these markets don’t have any downtime (unlike the stock market). Whenever an R package is referenced, the text will be colored orange. We will discuss R packages and the rest of the terms below later in this document, but if you are not familiar with these terms please look through the high-level version first. Whenever a function is referenced, it will be colored green. Whenever an R object is referenced, it will be colored blue. We will also refer to the parameters of functions in blue. When a term is particularly common in machine learning or data science, we will call it out with purple text, but only the first time it appears. Whenever text is highlighted this way, that means it is a snippet of R code, which will usually be done to bring attention to specific parts of the code. 1.4 Plan of Attack How will we generate predictive models to predict cryptocurrency prices? At a high level, here are the steps we will be taking: Setup guide. Installation guide on getting the tools used installed on your machine. Explore data. What is the data we are working with? How “good” is the “quality”? Prepare the data. Make adjustments to “clean” the data based on the findings from the previous section to avoid running into problems when making models to make predictions. Visualize the data. Visualizing the data can be an effective way to understand relationships, trends and outliers before creating predictive models, and is generally useful for many different purposes. Definitely the most fun section! Make predictive models. Now we are ready to take the data available to us and use it to make predictive models that can be used to make predictions about future price movements using the latest data. Evaluate the performance of the models. Before we can use the predictive models to make predictions on the latest data, we need to understand how well we expect them to perform, which is also essential in detecting issues. 1.5 GitHub Repository This tutorial has a public GitHub Repository associated with it, which is not only a website for us to easily distribute all the code behind this document for others to view and use, but also where it actually runs. Every 12 hours, a process gets kicked off on the page associated with our project on GitHub.com and refreshes these results. You can view the latest runs as they execute over time here: https://github.com/ries9112/cryptocurrencyresearch-org/actions In the next section we will talk more about the GitHub Repository for this tutorial, for now you can check on the latest run history for this document, which is expected to update every 12 hours every day: https://github.com/ries9112/cryptocurrencyresearch-org/actions If you are running into problems using the code or notice any problems, please let us know by creating an issue on the GitHub repository: https://github.com/ries9112/cryptocurrencyresearch-org/issues Go to the next section for instruction on getting setup to follow along with the code run in the tutorial. You can run every step either in the cloud on your web browser, or in your own R session. You can even make a copy of the GitHub Repository on your computer and run this “book” on the latest data (updated hourly), and make changes wherever you would like. All explained in the next section ➡️ 1.6 Disclaimer This tutorial is made available for learning and educational purposes only and the information to follow does not constitute trading advice in any way shape or form. We avoid giving any advice when it comes to trading strategies, as this is a very complex ecosystem that is out of the scope of this tutorial; we make no attempt in this regard, and if this, rather than data science, is your interest, your time would be better spent following a different tutorial that aims to answer those questions. "],["setup.html", "Section - 2 Setup and Installation 2.1 Option 1 - Run in the Cloud 2.2 Option 2 - Run Locally", " Section - 2 Setup and Installation Every part of this document can be run on any computer that has R installed You can also follow along with the tutorial without running the individual steps yourself. In that case, you can move on to the next page where the tutorial actually begins. 2.1 Option 1 - Run in the Cloud If you do not currently have R and RStudio installed on your computer, you can run all of the code from your web browser one step at a time here: this mobile friendly link. This can take up to 30 seconds to load, and once it has you should see a page that looks like this: From here, you can run the code one cell at a time: You can also use Shift + Enter to run an individual code cell Or run all code cells at once: If you feel lost and are not familiar with Jupyter Notebooks, you can do a quick interactive walkthrough under Help –&gt; User Interface Tour: 2.2 Option 2 - Run Locally If you want to follow along from your own computer directly (recommended option), please follow the installation instructions below. Afterwards, you will be able to run the code. You only need to follow these instructions once. If you have followed these steps once already, skip ahead to the next section. 2.2.1 Setup R If you do not already have R and R Studio installed on your computer, you will need to: Install R. Install RStudio. This step is optional, but it is very recommended that you use an integrated development environment (IDE) like RStudio as you follow along, rather than just using the R console as it was installed in step 1 above. Once RStudio is installed, run the application on your computer and you are ready to run the code as it is shown below and in the rest of this document! You can run your code directly through the Console (what you are prompted to write code into when RStudio boots up), or create a new document to save your code as you go along: You will then be able to save your document with the .R extension on your computer and re-run your code line by line. 2.2.2 Installing and Loading Packages [ADD HERE] describe how to install packages, what they are, etc… like high-level version. install.packages(&#39;pacman&#39;) Load pacman: library(pacman) Install other packages: p_load(&#39;pins&#39;, &#39;skimr&#39;, &#39;DT&#39;, # Data Exploration &#39;tidyverse&#39;, &#39;tsibble&#39;, &#39;anytime&#39;, &#39;httr&#39;, &#39;jsonlite&#39;, # Data Prep &#39;ggTimeSeries&#39;, &#39;gifski&#39;, &#39;av&#39;, &#39;magick&#39;, &#39;ggthemes&#39;, &#39;plotly&#39;, # Visualization &#39;ggpubr&#39;, &#39;ggforce&#39;, &#39;gganimate&#39;, &#39;transformr&#39;, # Visualization continued &#39;caret&#39;, &#39;doParallel&#39;, &#39;xgboost&#39;, &#39;gbm&#39;, &#39;deepnet&#39;, # Predictive Modeling &#39;hydroGOF&#39;) # Evaluate Model Performance It is normal for this step to take a long time, as it will install every package you will need to follow along with the rest of the tutorial. The next time you run this command it would be much faster because it would skip installing the already installed packages. Nice work! Now you have everything you need to follow along with this example. –&gt; "],["explore-data.html", "Section - 3 Explore the Data 3.1 Pull the Data 3.2 Data Preview 3.3 The definition of a “price” 3.4 Data Quality 3.5 Data Source Details", " Section - 3 Explore the Data 3.1 Pull the Data The first thing we will need to do is download the latest data. We will do this by using the pins package(Luraschi 2020), which has already been loaded into our session in the previous section. First, we will need to connect to a public GitHub repository (anyone can post their code to the GitHub website and make a “repository” with code for their project) and register the board that the data is pinned to by using the board_register() function: board_register(name = &quot;pins_board&quot;, url = &quot;https://raw.githubusercontent.com/predictcrypto/pins/master/&quot;, board = &quot;datatxt&quot;) By running the board_register() command on the URL where the data is located, we can now “ask” for the data we are interested in, which is called hitBTC_orderbook: cryptodata &lt;- pin_get(name = &quot;hitBTC_orderbook&quot;) The data has been saved to the cryptodata object. 3.2 Data Preview Below is a preview of the data: Only the first 2,000 rows of the data are shown in the table above. There are 223085 rows in the actual full dataset. The latest data is from 2020-11-05. This is tidy data, meaning: Every column is a variable. Every row is an observation. Every cell is a single value relating to a specific variable and observation. The data is collected once per hour. Each row is an observation of an individual cryptocurrency, and the same cryptocurrency is tracked on an hourly basis, each time presented as a new row in the dataset. 3.3 The definition of a “price” When we are talking about the price of a cryptocurrency, there are several different ways to define it. Most people check cryptocurrency prices on websites that aggregate data across thousands of exchanges, and have ways of computing a global average that represents the current price of the cryptocurrency. This is what happens on the very popular website coinmarketcap.com, but is this the correct approach for our use case? Before we even start programming, a crucial step of any successful predictive modeling process is defining the problem in a way that makes sense in relation to the actions we are looking to take. If we are looking to trade a specific cryptocurrency on a specific exchange, using the global average price is not going to be the best approach because we might create a model that believes we could have traded at certain prices when this was actually not the case. If this was the only data available we could certainly try and extrapolate trends across all exchanges and countries, but a better alternative available to us is to define the price as the price that we could have actually purchased the cryptocurrency at. If we are interested in purchasing a cryptocurrency, we should consider data for the price we could have actually purchased it at. The way these markets work, is you have a constantly evolving order book, where traders can post specific trades they want to make to either sell or buy a cryptocurrency specifying the price and quantity. When someone posts a trade to sell a cryptocurrency at a price that someone else is looking to purchase it at, a trade between the two parties will take place. You can find an example of a live order book for the exchange we will be using here: https://hitbtc.com/btc-to-usdt Scroll down to view the information relating to the orderbooks: Let’s focus on the Market Depth Chart for now: Here, the x-axis shows the price of the cryptocurrency, with the lower prices on the left and the higher ones on the right, while the y-axis shows the cumulative volume (here in terms of Bitcoins) of orders that could currently be triggered on each side. In the screenshot above, around the $13,000 price point the market essentially “runs out” of people willing to buy the cryptocurrency, and shows us how much people are willing to sell at the higher price points. The screenshot shows there are many orders that are waiting to be fulfilled, around the $12,500 price point shown for example the chart tells us if we wanted to buy the cryptocurrency BTC at that price, about 1,500 BTC would have to be sold for more expensive prices before the order was triggered. The market will aggregate trades people want to perform based on the given prices, and fulfill orders from the orderbook of pending trades. The price at which the two sides of the orderbook converge is the price we could currently trade the cryptocurrency on this exchange at. Because the price works this way, we couldn’t simply buy 1,500 BTC at the $13,000 price point because we would run out of people who are willing to sell their BTC at that price point, and to fulfill the entire order we would need to pay more than what we would consider to be the “price” depending on how much we were looking to purchase. This is one of the many reasons for why any positive results shown here wouldn’t necessarily produce an effective trading strategy if put into practice in the real world. There is a meaningful difference between predicting price movements for the cryptocurrency markets, and actually performing effective trades, so please just have fun and experiment with the data but hold back the urge to use this to perform real trades. 3.3.1 In Summary To summarize the implications of what was explained above, the data we are using gives us the 5 nearest prices on both sides of where the red and green lines connect on the Market Depth Chart, as well as the quantity available to purchase or sell at that given price point. In order to make predictive models to predict the price of a cryptocurrency, we will first need to define the price as the lowest available price that allows us to buy “enough” of it based on the current orderbook data as described above. 3.4 Data Quality Before jumping into actually cleaning your data, it’s worth spending time doing some Exploratory Data Analysis (EDA), which is the process of analyzing the data itself for issues before starting on any other process. Most data science and business problems will require you to have a deep understanding of your dataset and all of its caveats and issues, and without those fundamental problems understood and accounted for no model will make sense. In our case this understanding mostly comes from understanding how the price of a cryptocurrency is defined, which we reviewed above, and there isn’t too much else for us to worry about in terms of the quality of the raw data, but in other cases doing EDA will be a more involved process doing things like visualizing the different columns of the data. There are a lot of tools that can be used for this, but as an example we can use one of the packages we already imported into the R session in the setup section called skimr(Waring et al. 2020) to get a quick overview/summary of the “quality” of the different columns that make up the data. We can use the skim() function on the cryptodata dataframe to get a summary of the data to help locate any potential data quality issues: skim(cryptodata) Table 3.1: Data summary Name cryptodata Number of rows 223085 Number of columns 27 _______________________ Column type frequency: character 5 Date 1 numeric 20 POSIXct 1 ________________________ Group variables Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace pair 0 1 5 9 0 217 0 symbol 0 1 2 6 0 217 0 quote_currency 0 1 3 3 0 1 0 pkDummy 498 1 13 13 0 2096 0 pkey 498 1 15 19 0 222307 0 Variable type: Date skim_variable n_missing complete_rate min max median n_unique date 498 1 2020-08-10 2020-11-05 2020-09-24 88 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist ask_1_price 43 1 31643.53 14821930.32 0 0.01 0.06 0.60 7000000000.0 ▇▁▁▁▁ ask_1_quantity 43 1 209261.44 5335888.43 0 19.00 430.00 3760.00 455776000.0 ▇▁▁▁▁ ask_2_price 84 1 241.58 7183.57 0 0.01 0.06 0.60 999999.0 ▇▁▁▁▁ ask_2_quantity 84 1 228066.39 4826647.57 0 20.00 464.00 5300.00 459459000.0 ▇▁▁▁▁ ask_3_price 172 1 261.23 7957.87 0 0.01 0.06 0.61 999000.0 ▇▁▁▁▁ ask_3_quantity 172 1 280182.96 5203865.91 0 15.10 419.23 7187.30 518082000.0 ▇▁▁▁▁ ask_4_price 254 1 220104.32 39233169.89 0 0.01 0.06 0.61 7000000000.0 ▇▁▁▁▁ ask_4_quantity 254 1 294833.84 5122906.19 0 13.00 450.00 7700.00 546546000.0 ▇▁▁▁▁ ask_5_price 320 1 184.06 1398.56 0 0.01 0.07 0.62 24000.0 ▇▁▁▁▁ ask_5_quantity 320 1 301105.31 5453033.80 0 12.00 426.47 8700.00 549312000.0 ▇▁▁▁▁ bid_1_price 336 1 181.04 1385.99 0 0.00 0.05 0.52 20298.0 ▇▁▁▁▁ bid_1_quantity 336 1 166642.46 2746699.41 0 29.60 660.00 7734.00 296583000.0 ▇▁▁▁▁ bid_2_price 395 1 180.88 1385.45 0 0.00 0.05 0.51 20269.5 ▇▁▁▁▁ bid_2_quantity 395 1 172154.17 3298706.66 0 22.90 504.78 6201.90 562697873.0 ▇▁▁▁▁ bid_3_price 397 1 180.58 1384.61 0 0.00 0.05 0.50 20231.5 ▇▁▁▁▁ bid_3_quantity 397 1 235239.59 3369354.76 0 13.00 397.00 6139.77 347366000.0 ▇▁▁▁▁ bid_4_price 397 1 180.22 1383.63 0 0.00 0.04 0.48 20227.4 ▇▁▁▁▁ bid_4_quantity 397 1 296962.14 3806316.26 0 10.00 390.00 7500.00 331285000.0 ▇▁▁▁▁ bid_5_price 399 1 179.71 1381.87 0 0.00 0.04 0.47 20210.4 ▇▁▁▁▁ bid_5_quantity 399 1 346347.56 4531496.43 0 10.00 390.00 8664.00 384159000.0 ▇▁▁▁▁ Variable type: POSIXct skim_variable n_missing complete_rate min max median n_unique date_time_utc 498 1 2020-08-10 04:29:09 2020-11-05 17:03:31 2020-09-24 18:00:06 199752 This summary helps us understand things like how many null values are present in a given column and how those values are distributed. In this case there shouldn’t be any major data quality issues, for example the majority of values should not be NA/missing. If you are noticing something different Move on to the next section, where we make the adjustments necessary to the data before we can start making visualizations and predictive models. 3.5 Data Source Details [TODO - HERE add notes on data source] This section is optional. The data is pulled without an authentication requirement. See the example code below for an actual example of how the data was sourced. Below is an example for the Ethereum (ETH) cryptocurrency. The data is collected by iterating through all cryptocurrency options one by one at the start of every hour. get_response_content &lt;- function(api_response) { httr::content(api_response, type = &quot;text&quot;, encoding = &quot;UTF-8&quot;) %&gt;% jsonlite::fromJSON(simplifyDataFrame = FALSE) } # Get the data example_data &lt;- httr::GET(&quot;https://api.hitbtc.com/api/2/public/orderbook/ETHUSD&quot;, query=list(limit=5)) # Show data get_response_content(example_data) ## $symbol ## [1] &quot;ETHUSD&quot; ## ## $timestamp ## [1] &quot;2020-11-05T17:07:51.661Z&quot; ## ## $batchingTime ## [1] &quot;2020-11-05T17:07:51.692Z&quot; ## ## $ask ## $ask[[1]] ## $ask[[1]]$price ## [1] &quot;413.351&quot; ## ## $ask[[1]]$size ## [1] &quot;1.0123&quot; ## ## ## $ask[[2]] ## $ask[[2]]$price ## [1] &quot;413.352&quot; ## ## $ask[[2]]$size ## [1] &quot;30.0000&quot; ## ## ## $ask[[3]] ## $ask[[3]]$price ## [1] &quot;413.353&quot; ## ## $ask[[3]]$size ## [1] &quot;0.7828&quot; ## ## ## $ask[[4]] ## $ask[[4]]$price ## [1] &quot;413.369&quot; ## ## $ask[[4]]$size ## [1] &quot;0.4000&quot; ## ## ## $ask[[5]] ## $ask[[5]]$price ## [1] &quot;413.411&quot; ## ## $ask[[5]]$size ## [1] &quot;1.7153&quot; ## ## ## ## $bid ## $bid[[1]] ## $bid[[1]]$price ## [1] &quot;413.336&quot; ## ## $bid[[1]]$size ## [1] &quot;0.4000&quot; ## ## ## $bid[[2]] ## $bid[[2]]$price ## [1] &quot;413.297&quot; ## ## $bid[[2]]$size ## [1] &quot;30.0000&quot; ## ## ## $bid[[3]] ## $bid[[3]]$price ## [1] &quot;413.288&quot; ## ## $bid[[3]]$size ## [1] &quot;1.0800&quot; ## ## ## $bid[[4]] ## $bid[[4]]$price ## [1] &quot;413.262&quot; ## ## $bid[[4]]$size ## [1] &quot;3.0374&quot; ## ## ## $bid[[5]] ## $bid[[5]]$price ## [1] &quot;413.261&quot; ## ## $bid[[5]]$size ## [1] &quot;30.0000&quot; References "],["data-prep.html", "Section - 4 Data Prep 4.1 Remove Nulls 4.2 Calculate price_usd Column 4.3 Clean Data by Group 4.4 Calculate Target 4.5 Cross Validation 4.6 Fix Data by Split 4.7 Nest data 4.8 Functional Programming", " Section - 4 Data Prep [ADD HERE] 4.1 Remove Nulls [ADD HERE] (because we can’t do anything without ask_1_price or date_time_utc at the very least being there) Remove any rows where the ask_1_price is Null: cryptodata &lt;- cryptodata[!is.na(cryptodata$ask_1_price), ] And when the date_time_utc is Null: cryptodata &lt;- cryptodata[!is.na(cryptodata$date_time_utc), ] 4.2 Calculate price_usd Column [ADD HERE] Calculate the price_usd using the order books data and taking the cheapest price available from the ask side where at least $15 worth of the cryptocurrency are being sold. [ADD HERE] cryptodata &lt;- mutate(cryptodata, trade_usd_1 = ask_1_price * ask_1_quantity, trade_usd_2 = ask_2_price * ask_2_quantity, trade_usd_3 = ask_3_price * ask_3_quantity, trade_usd_4 = ask_4_price * ask_4_quantity, trade_usd_5 = ask_5_price * ask_5_quantity) We can look at an example [ADD HERE] head(select(cryptodata, symbol, date_time_utc, ask_1_price, ask_1_quantity, trade_usd_1)) ## [90m# A tibble: 6 x 5[39m ## symbol date_time_utc ask_1_price ask_1_quantity trade_usd_1 ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dttm&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m BTC 2020-11-05 [90m00:00:00[39m [4m1[24m[4m4[24m148. 2 [4m2[24m[4m8[24m295. ## [90m2[39m ETH 2020-11-05 [90m00:00:01[39m 403. 0.007[4m4[24m 2.98 ## [90m3[39m EOS 2020-11-05 [90m00:00:01[39m 2.35 70 165. ## [90m4[39m LTC 2020-11-05 [90m00:00:02[39m 54.7 60 [4m3[24m282. ## [90m5[39m BSV 2020-11-05 [90m00:00:03[39m 152. 0.6 90.9 ## [90m6[39m ADA 2020-11-05 [90m00:00:04[39m 0.095[4m6[24m 775 74.1 If none of the top 5 orders on the order book ask side are for at least $15, exclude the row. [ADD HERE] cryptodata &lt;- mutate(cryptodata, price_usd = case_when( cryptodata$trade_usd_1 &gt;= 15 ~ cryptodata$ask_1_price, cryptodata$trade_usd_2 &gt;= 15 ~ cryptodata$ask_2_price, cryptodata$trade_usd_3 &gt;= 15 ~ cryptodata$ask_3_price, cryptodata$trade_usd_4 &gt;= 15 ~ cryptodata$ask_4_price, cryptodata$trade_usd_5 &gt;= 15 ~ cryptodata$ask_5_price)) Now remove rows where we couldn’t find a price above $15 for any of the 5 cheapest orders in the order book. cryptodata &lt;- na.omit(cryptodata) This step removed 23553 rows on the latest run. 4.3 Clean Data by Group [ADD HERE] …group_by() from the dplyr (Wickham et al. 2020) package… cryptodata &lt;- group_by(cryptodata, symbol) 4.3.1 Remove symbols without enough rows [ADD HERE] cryptodata &lt;- dplyr::filter(cryptodata, n() &gt;= 700) The number of rows for the cryptodata dataset before the filtering step was 199034 and is now 174459. 4.3.2 Remove symbols without data from the last 3 days If there was no data collected for a cryptocurrency over the last 3 day period, let’s exclude that asset from the dataset since we are only looking to model data that is currently flowing through the process. If an asset is removed from the exchange (if a project is a scam for example) or is no longer being actively captured by the data collection process, we can’t make new predictions for it, so might as well exclude these ahead of time as well. cryptodata &lt;- dplyr::filter(cryptodata, max(date) &gt; Sys.Date()-3) The number of rows for the cryptodata dataset before this filtering step was 174459 and is now 134393. 4.4 Calculate Target [TODO - ADD HERE] 4.4.1 Any gaps? [TODO - ADD HERE] 4.4.2 Convert to tsibble … the tsibble package (Wang et al. 2020)… 4.4.2.1 Convert to hourly data and get rid of minutes and seconds … anytime() from the anytime package (Eddelbuettel 2020)… cryptodata$ts_index &lt;- anytime(paste0(cryptodata$pkDummy,&#39;:00:00&#39;)) … distinct() from the dplyr package (Wickham et al. 2020)… cryptodata &lt;- distinct(cryptodata, symbol, ts_index, .keep_all=TRUE) … as_tsibble() from the tsibble package (Wang et al. 2020)… cryptodata &lt;- as_tsibble(cryptodata, index=ts_index, key=symbol) 4.4.3 Scan gaps scan_gaps(cryptodata) ## [90m# A tsibble: 23,134 x 2 [1h] &lt;UTC&gt;[39m ## [90m# Key: symbol [97][39m ## symbol ts_index ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dttm&gt;[39m[23m ## [90m 1[39m AAB 2020-10-16 [90m22:00:00[39m ## [90m 2[39m AAB 2020-10-16 [90m23:00:00[39m ## [90m 3[39m AAB 2020-10-17 [90m00:00:00[39m ## [90m 4[39m AAB 2020-10-17 [90m01:00:00[39m ## [90m 5[39m AAB 2020-10-17 [90m02:00:00[39m ## [90m 6[39m AAB 2020-10-17 [90m03:00:00[39m ## [90m 7[39m AAB 2020-10-17 [90m04:00:00[39m ## [90m 8[39m AAB 2020-10-17 [90m05:00:00[39m ## [90m 9[39m AAB 2020-10-17 [90m06:00:00[39m ## [90m10[39m AAB 2020-10-17 [90m07:00:00[39m ## [90m# … with 23,124 more rows[39m 4.4.4 Fill gaps cryptodata &lt;- fill_gaps(cryptodata) Now looking at the data again, there are 23134 additional rows that were added as implicitly missing in the data: cryptodata ## [90m# A tsibble: 157,316 x 34 [1h] &lt;UTC&gt;[39m ## [90m# Key: symbol [97][39m ## [90m# Groups: symbol [97][39m ## pair symbol quote_currency ask_1_price ask_1_quantity ask_2_price ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m AABU… AAB USD 0.390 104. 0.39 ## [90m 2[39m AABU… AAB USD 0.390 104. 0.39 ## [90m 3[39m AABU… AAB USD 0.390 104. 0.39 ## [90m 4[39m AABU… AAB USD 0.390 104. 0.39 ## [90m 5[39m AABU… AAB USD 0.390 104. 0.39 ## [90m 6[39m AABU… AAB USD 0.390 104. 0.39 ## [90m 7[39m AABU… AAB USD 0.390 104. 0.39 ## [90m 8[39m AABU… AAB USD 0.390 104. 0.390 ## [90m 9[39m AABU… AAB USD 0.390 104. 0.390 ## [90m10[39m AABU… AAB USD 0.390 104. 0.390 ## [90m# … with 157,306 more rows, and 28 more variables: ask_2_quantity [3m[90m&lt;dbl&gt;[90m[23m,[39m ## [90m# ask_3_price [3m[90m&lt;dbl&gt;[90m[23m, ask_3_quantity [3m[90m&lt;dbl&gt;[90m[23m, ask_4_price [3m[90m&lt;dbl&gt;[90m[23m,[39m ## [90m# ask_4_quantity [3m[90m&lt;dbl&gt;[90m[23m, ask_5_price [3m[90m&lt;dbl&gt;[90m[23m, ask_5_quantity [3m[90m&lt;dbl&gt;[90m[23m,[39m ## [90m# bid_1_price [3m[90m&lt;dbl&gt;[90m[23m, bid_1_quantity [3m[90m&lt;dbl&gt;[90m[23m, bid_2_price [3m[90m&lt;dbl&gt;[90m[23m,[39m ## [90m# bid_2_quantity [3m[90m&lt;dbl&gt;[90m[23m, bid_3_price [3m[90m&lt;dbl&gt;[90m[23m, bid_3_quantity [3m[90m&lt;dbl&gt;[90m[23m,[39m ## [90m# bid_4_price [3m[90m&lt;dbl&gt;[90m[23m, bid_4_quantity [3m[90m&lt;dbl&gt;[90m[23m, bid_5_price [3m[90m&lt;dbl&gt;[90m[23m,[39m ## [90m# bid_5_quantity [3m[90m&lt;dbl&gt;[90m[23m, date_time_utc [3m[90m&lt;dttm&gt;[90m[23m, date [3m[90m&lt;date&gt;[90m[23m, pkDummy [3m[90m&lt;chr&gt;[90m[23m,[39m ## [90m# pkey [3m[90m&lt;chr&gt;[90m[23m, trade_usd_1 [3m[90m&lt;dbl&gt;[90m[23m, trade_usd_2 [3m[90m&lt;dbl&gt;[90m[23m, trade_usd_3 [3m[90m&lt;dbl&gt;[90m[23m,[39m ## [90m# trade_usd_4 [3m[90m&lt;dbl&gt;[90m[23m, trade_usd_5 [3m[90m&lt;dbl&gt;[90m[23m, price_usd [3m[90m&lt;dbl&gt;[90m[23m, ts_index [3m[90m&lt;dttm&gt;[90m[23m[39m For now, let’s convert the data back to a tibble instead of a tsibble: cryptodata &lt;- group_by(as_tibble(cryptodata), symbol) 4.4.5 Calculate Target [ADD HERE] Also adding lagged variables here! [ADD HERE] …mutate() from the dplyr (Wickham et al. 2020) package… cryptodata &lt;- mutate(cryptodata, target_price_24h = lead(price_usd, 24, order_by=ts_index), # Now all the lagged variables: lagged_price_1h = lag(price_usd, 1, order_by=ts_index), lagged_price_2h = lag(price_usd, 2, order_by=ts_index), lagged_price_3h = lag(price_usd, 3, order_by=ts_index), lagged_price_6h = lag(price_usd, 6, order_by=ts_index), lagged_price_12h = lag(price_usd, 12, order_by=ts_index), lagged_price_24h = lag(price_usd, 24, order_by=ts_index), lagged_price_3d = lag(price_usd, 24*3, order_by=ts_index)) Here is an example showing the results for the subset of data related to the Bitcoin cryptocurrency (using symbol == 'BTC') showing 30 rows and the relevant columns we just calculated: print(select(filter(cryptodata, symbol == &#39;BTC&#39;),ts_index, price_usd, lagged_price_1h, lagged_price_24h, target_price_24h), n=30) ## [90m# A tibble: 2,102 x 6[39m ## [90m# Groups: symbol [1][39m ## symbol ts_index price_usd lagged_price_1h lagged_price_24h ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dttm&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m BTC 2020-08-10 [90m04:00:00[39m [4m1[24m[4m1[24m997. [31mNA[39m [31mNA[39m ## [90m 2[39m BTC 2020-08-10 [90m05:00:00[39m [4m1[24m[4m1[24m986. [4m1[24m[4m1[24m997. [31mNA[39m ## [90m 3[39m BTC 2020-08-10 [90m06:00:00[39m [4m1[24m[4m1[24m973. [4m1[24m[4m1[24m986. [31mNA[39m ## [90m 4[39m BTC 2020-08-10 [90m07:00:00[39m [4m1[24m[4m1[24m994. [4m1[24m[4m1[24m973. [31mNA[39m ## [90m 5[39m BTC 2020-08-10 [90m08:00:00[39m [4m1[24m[4m1[24m984. [4m1[24m[4m1[24m994. [31mNA[39m ## [90m 6[39m BTC 2020-08-10 [90m09:00:00[39m [4m1[24m[4m1[24m962. [4m1[24m[4m1[24m984. [31mNA[39m ## [90m 7[39m BTC 2020-08-10 [90m10:00:00[39m [4m1[24m[4m1[24m989. [4m1[24m[4m1[24m962. [31mNA[39m ## [90m 8[39m BTC 2020-08-10 [90m11:00:00[39m [4m1[24m[4m1[24m709. [4m1[24m[4m1[24m989. [31mNA[39m ## [90m 9[39m BTC 2020-08-10 [90m12:00:00[39m [4m1[24m[4m1[24m721. [4m1[24m[4m1[24m709. [31mNA[39m ## [90m10[39m BTC 2020-08-10 [90m13:00:00[39m [4m1[24m[4m1[24m890. [4m1[24m[4m1[24m721. [31mNA[39m ## [90m11[39m BTC 2020-08-10 [90m14:00:00[39m [4m1[24m[4m1[24m908. [4m1[24m[4m1[24m890. [31mNA[39m ## [90m12[39m BTC 2020-08-10 [90m15:00:00[39m [4m1[24m[4m1[24m889. [4m1[24m[4m1[24m908. [31mNA[39m ## [90m13[39m BTC 2020-08-10 [90m16:00:00[39m [4m1[24m[4m1[24m918. [4m1[24m[4m1[24m889. [31mNA[39m ## [90m14[39m BTC 2020-08-10 [90m17:00:00[39m [4m1[24m[4m1[24m855. [4m1[24m[4m1[24m918. [31mNA[39m ## [90m15[39m BTC 2020-08-10 [90m18:00:00[39m [4m1[24m[4m1[24m887. [4m1[24m[4m1[24m855. [31mNA[39m ## [90m16[39m BTC 2020-08-10 [90m19:00:00[39m [4m1[24m[4m1[24m878. [4m1[24m[4m1[24m887. [31mNA[39m ## [90m17[39m BTC 2020-08-10 [90m20:00:00[39m [4m1[24m[4m1[24m855. [4m1[24m[4m1[24m878. [31mNA[39m ## [90m18[39m BTC 2020-08-10 [90m21:00:00[39m [4m1[24m[4m1[24m847. [4m1[24m[4m1[24m855. [31mNA[39m ## [90m19[39m BTC 2020-08-10 [90m22:00:00[39m [4m1[24m[4m1[24m820. [4m1[24m[4m1[24m847. [31mNA[39m ## [90m20[39m BTC 2020-08-10 [90m23:00:00[39m [4m1[24m[4m1[24m805. [4m1[24m[4m1[24m820. [31mNA[39m ## [90m21[39m BTC 2020-08-11 [90m00:00:00[39m [4m1[24m[4m1[24m906. [4m1[24m[4m1[24m805. [31mNA[39m ## [90m22[39m BTC 2020-08-11 [90m01:00:00[39m [4m1[24m[4m1[24m863. [4m1[24m[4m1[24m906. [31mNA[39m ## [90m23[39m BTC 2020-08-11 [90m02:00:00[39m [4m1[24m[4m1[24m901. [4m1[24m[4m1[24m863. [31mNA[39m ## [90m24[39m BTC 2020-08-11 [90m03:00:00[39m [4m1[24m[4m1[24m868. [4m1[24m[4m1[24m901. [31mNA[39m ## [90m25[39m BTC 2020-08-11 [90m04:00:00[39m [4m1[24m[4m1[24m840. [4m1[24m[4m1[24m868. [4m1[24m[4m1[24m997. ## [90m26[39m BTC 2020-08-11 [90m05:00:00[39m [4m1[24m[4m1[24m820. [4m1[24m[4m1[24m840. [4m1[24m[4m1[24m986. ## [90m27[39m BTC 2020-08-11 [90m06:00:00[39m [4m1[24m[4m1[24m847. [4m1[24m[4m1[24m820. [4m1[24m[4m1[24m973. ## [90m28[39m BTC 2020-08-11 [90m07:00:00[39m [4m1[24m[4m1[24m774. [4m1[24m[4m1[24m847. [4m1[24m[4m1[24m994. ## [90m29[39m BTC 2020-08-11 [90m08:00:00[39m [4m1[24m[4m1[24m761. [4m1[24m[4m1[24m774. [4m1[24m[4m1[24m984. ## [90m30[39m BTC 2020-08-11 [90m09:00:00[39m [4m1[24m[4m1[24m753. [4m1[24m[4m1[24m761. [4m1[24m[4m1[24m962. ## [90m# … with 2,072 more rows, and 1 more variable: target_price_24h [3m[90m&lt;dbl&gt;[90m[23m[39m The field target_price_24h shows the value of price_usd 24 hours into the future relative to the row of data. All the lagged_ fields show the price from the past relative to when the data was collected (where lagged_price_1h shows the price 1 hour before the ts_index timestamp of the data). So looking at the tail() end of the data, we should see 24 rows with no values for target_price_24h, while all the lagged_price_ columns should have values: tail(print(select(filter(cryptodata, symbol == &#39;BTC&#39;),ts_index, price_usd, lagged_price_1h, lagged_price_24h, target_price_24h), n=30)) ## [90m# A tibble: 2,102 x 6[39m ## [90m# Groups: symbol [1][39m ## symbol ts_index price_usd lagged_price_1h lagged_price_24h ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dttm&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m BTC 2020-08-10 [90m04:00:00[39m [4m1[24m[4m1[24m997. [31mNA[39m [31mNA[39m ## [90m 2[39m BTC 2020-08-10 [90m05:00:00[39m [4m1[24m[4m1[24m986. [4m1[24m[4m1[24m997. [31mNA[39m ## [90m 3[39m BTC 2020-08-10 [90m06:00:00[39m [4m1[24m[4m1[24m973. [4m1[24m[4m1[24m986. [31mNA[39m ## [90m 4[39m BTC 2020-08-10 [90m07:00:00[39m [4m1[24m[4m1[24m994. [4m1[24m[4m1[24m973. [31mNA[39m ## [90m 5[39m BTC 2020-08-10 [90m08:00:00[39m [4m1[24m[4m1[24m984. [4m1[24m[4m1[24m994. [31mNA[39m ## [90m 6[39m BTC 2020-08-10 [90m09:00:00[39m [4m1[24m[4m1[24m962. [4m1[24m[4m1[24m984. [31mNA[39m ## [90m 7[39m BTC 2020-08-10 [90m10:00:00[39m [4m1[24m[4m1[24m989. [4m1[24m[4m1[24m962. [31mNA[39m ## [90m 8[39m BTC 2020-08-10 [90m11:00:00[39m [4m1[24m[4m1[24m709. [4m1[24m[4m1[24m989. [31mNA[39m ## [90m 9[39m BTC 2020-08-10 [90m12:00:00[39m [4m1[24m[4m1[24m721. [4m1[24m[4m1[24m709. [31mNA[39m ## [90m10[39m BTC 2020-08-10 [90m13:00:00[39m [4m1[24m[4m1[24m890. [4m1[24m[4m1[24m721. [31mNA[39m ## [90m11[39m BTC 2020-08-10 [90m14:00:00[39m [4m1[24m[4m1[24m908. [4m1[24m[4m1[24m890. [31mNA[39m ## [90m12[39m BTC 2020-08-10 [90m15:00:00[39m [4m1[24m[4m1[24m889. [4m1[24m[4m1[24m908. [31mNA[39m ## [90m13[39m BTC 2020-08-10 [90m16:00:00[39m [4m1[24m[4m1[24m918. [4m1[24m[4m1[24m889. [31mNA[39m ## [90m14[39m BTC 2020-08-10 [90m17:00:00[39m [4m1[24m[4m1[24m855. [4m1[24m[4m1[24m918. [31mNA[39m ## [90m15[39m BTC 2020-08-10 [90m18:00:00[39m [4m1[24m[4m1[24m887. [4m1[24m[4m1[24m855. [31mNA[39m ## [90m16[39m BTC 2020-08-10 [90m19:00:00[39m [4m1[24m[4m1[24m878. [4m1[24m[4m1[24m887. [31mNA[39m ## [90m17[39m BTC 2020-08-10 [90m20:00:00[39m [4m1[24m[4m1[24m855. [4m1[24m[4m1[24m878. [31mNA[39m ## [90m18[39m BTC 2020-08-10 [90m21:00:00[39m [4m1[24m[4m1[24m847. [4m1[24m[4m1[24m855. [31mNA[39m ## [90m19[39m BTC 2020-08-10 [90m22:00:00[39m [4m1[24m[4m1[24m820. [4m1[24m[4m1[24m847. [31mNA[39m ## [90m20[39m BTC 2020-08-10 [90m23:00:00[39m [4m1[24m[4m1[24m805. [4m1[24m[4m1[24m820. [31mNA[39m ## [90m21[39m BTC 2020-08-11 [90m00:00:00[39m [4m1[24m[4m1[24m906. [4m1[24m[4m1[24m805. [31mNA[39m ## [90m22[39m BTC 2020-08-11 [90m01:00:00[39m [4m1[24m[4m1[24m863. [4m1[24m[4m1[24m906. [31mNA[39m ## [90m23[39m BTC 2020-08-11 [90m02:00:00[39m [4m1[24m[4m1[24m901. [4m1[24m[4m1[24m863. [31mNA[39m ## [90m24[39m BTC 2020-08-11 [90m03:00:00[39m [4m1[24m[4m1[24m868. [4m1[24m[4m1[24m901. [31mNA[39m ## [90m25[39m BTC 2020-08-11 [90m04:00:00[39m [4m1[24m[4m1[24m840. [4m1[24m[4m1[24m868. [4m1[24m[4m1[24m997. ## [90m26[39m BTC 2020-08-11 [90m05:00:00[39m [4m1[24m[4m1[24m820. [4m1[24m[4m1[24m840. [4m1[24m[4m1[24m986. ## [90m27[39m BTC 2020-08-11 [90m06:00:00[39m [4m1[24m[4m1[24m847. [4m1[24m[4m1[24m820. [4m1[24m[4m1[24m973. ## [90m28[39m BTC 2020-08-11 [90m07:00:00[39m [4m1[24m[4m1[24m774. [4m1[24m[4m1[24m847. [4m1[24m[4m1[24m994. ## [90m29[39m BTC 2020-08-11 [90m08:00:00[39m [4m1[24m[4m1[24m761. [4m1[24m[4m1[24m774. [4m1[24m[4m1[24m984. ## [90m30[39m BTC 2020-08-11 [90m09:00:00[39m [4m1[24m[4m1[24m753. [4m1[24m[4m1[24m761. [4m1[24m[4m1[24m962. ## [90m# … with 2,072 more rows, and 1 more variable: target_price_24h [3m[90m&lt;dbl&gt;[90m[23m[39m ## [90m# A tibble: 6 x 6[39m ## [90m# Groups: symbol [1][39m ## symbol ts_index price_usd lagged_price_1h lagged_price_24h ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dttm&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m BTC 2020-11-05 [90m12:00:00[39m [4m1[24m[4m4[24m716. [4m1[24m[4m4[24m650 [4m1[24m[4m3[24m724. ## [90m2[39m BTC 2020-11-05 [90m13:00:00[39m [4m1[24m[4m4[24m868. [4m1[24m[4m4[24m716. [4m1[24m[4m3[24m828. ## [90m3[39m BTC 2020-11-05 [90m14:00:00[39m [4m1[24m[4m4[24m814. [4m1[24m[4m4[24m868. [4m1[24m[4m3[24m837. ## [90m4[39m BTC 2020-11-05 [90m15:00:00[39m [4m1[24m[4m4[24m897. [4m1[24m[4m4[24m814. [4m1[24m[4m3[24m840. ## [90m5[39m BTC 2020-11-05 [90m16:00:00[39m [4m1[24m[4m5[24m078. [4m1[24m[4m4[24m897. [4m1[24m[4m3[24m914. ## [90m6[39m BTC 2020-11-05 [90m17:00:00[39m [4m1[24m[4m5[24m150. [4m1[24m[4m5[24m078. [4m1[24m[4m4[24m032. ## [90m# … with 1 more variable: target_price_24h [3m[90m&lt;dbl&gt;[90m[23m[39m Reading the code shown above is less than ideal. One of the more popular tools introduced by the tidyverse is the %&gt;% operator, which works by starting with the object/data you want to make changes to first, and then apply each transformation step by step. It’s simply a way of re-writing the same code in a way that is easier to read by splitting the way the function is called rather than adding functions onto each other into a single line that becomes really hard to read. In the example above it becomes difficult to keep track of where things begin, the order of operations, and the parameters associated with the specific functions. Compare that to the code below: # Start with the object/data to manipulate cryptodata %&gt;% # Filter the data filter(symbol == &#39;BTC&#39;) %&gt;% # Select columns select(ts_index, price_usd, lagged_price_1h, lagged_price_24h, target_price_24h) %&gt;% # Print 30 elements instead of only default 10 for a tibble dataframe print(n = 30) %&gt;% # Show the last 30 elements of the data tail(30) ## [90m# A tibble: 2,102 x 6[39m ## [90m# Groups: symbol [1][39m ## symbol ts_index price_usd lagged_price_1h lagged_price_24h ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dttm&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m BTC 2020-08-10 [90m04:00:00[39m [4m1[24m[4m1[24m997. [31mNA[39m [31mNA[39m ## [90m 2[39m BTC 2020-08-10 [90m05:00:00[39m [4m1[24m[4m1[24m986. [4m1[24m[4m1[24m997. [31mNA[39m ## [90m 3[39m BTC 2020-08-10 [90m06:00:00[39m [4m1[24m[4m1[24m973. [4m1[24m[4m1[24m986. [31mNA[39m ## [90m 4[39m BTC 2020-08-10 [90m07:00:00[39m [4m1[24m[4m1[24m994. [4m1[24m[4m1[24m973. [31mNA[39m ## [90m 5[39m BTC 2020-08-10 [90m08:00:00[39m [4m1[24m[4m1[24m984. [4m1[24m[4m1[24m994. [31mNA[39m ## [90m 6[39m BTC 2020-08-10 [90m09:00:00[39m [4m1[24m[4m1[24m962. [4m1[24m[4m1[24m984. [31mNA[39m ## [90m 7[39m BTC 2020-08-10 [90m10:00:00[39m [4m1[24m[4m1[24m989. [4m1[24m[4m1[24m962. [31mNA[39m ## [90m 8[39m BTC 2020-08-10 [90m11:00:00[39m [4m1[24m[4m1[24m709. [4m1[24m[4m1[24m989. [31mNA[39m ## [90m 9[39m BTC 2020-08-10 [90m12:00:00[39m [4m1[24m[4m1[24m721. [4m1[24m[4m1[24m709. [31mNA[39m ## [90m10[39m BTC 2020-08-10 [90m13:00:00[39m [4m1[24m[4m1[24m890. [4m1[24m[4m1[24m721. [31mNA[39m ## [90m11[39m BTC 2020-08-10 [90m14:00:00[39m [4m1[24m[4m1[24m908. [4m1[24m[4m1[24m890. [31mNA[39m ## [90m12[39m BTC 2020-08-10 [90m15:00:00[39m [4m1[24m[4m1[24m889. [4m1[24m[4m1[24m908. [31mNA[39m ## [90m13[39m BTC 2020-08-10 [90m16:00:00[39m [4m1[24m[4m1[24m918. [4m1[24m[4m1[24m889. [31mNA[39m ## [90m14[39m BTC 2020-08-10 [90m17:00:00[39m [4m1[24m[4m1[24m855. [4m1[24m[4m1[24m918. [31mNA[39m ## [90m15[39m BTC 2020-08-10 [90m18:00:00[39m [4m1[24m[4m1[24m887. [4m1[24m[4m1[24m855. [31mNA[39m ## [90m16[39m BTC 2020-08-10 [90m19:00:00[39m [4m1[24m[4m1[24m878. [4m1[24m[4m1[24m887. [31mNA[39m ## [90m17[39m BTC 2020-08-10 [90m20:00:00[39m [4m1[24m[4m1[24m855. [4m1[24m[4m1[24m878. [31mNA[39m ## [90m18[39m BTC 2020-08-10 [90m21:00:00[39m [4m1[24m[4m1[24m847. [4m1[24m[4m1[24m855. [31mNA[39m ## [90m19[39m BTC 2020-08-10 [90m22:00:00[39m [4m1[24m[4m1[24m820. [4m1[24m[4m1[24m847. [31mNA[39m ## [90m20[39m BTC 2020-08-10 [90m23:00:00[39m [4m1[24m[4m1[24m805. [4m1[24m[4m1[24m820. [31mNA[39m ## [90m21[39m BTC 2020-08-11 [90m00:00:00[39m [4m1[24m[4m1[24m906. [4m1[24m[4m1[24m805. [31mNA[39m ## [90m22[39m BTC 2020-08-11 [90m01:00:00[39m [4m1[24m[4m1[24m863. [4m1[24m[4m1[24m906. [31mNA[39m ## [90m23[39m BTC 2020-08-11 [90m02:00:00[39m [4m1[24m[4m1[24m901. [4m1[24m[4m1[24m863. [31mNA[39m ## [90m24[39m BTC 2020-08-11 [90m03:00:00[39m [4m1[24m[4m1[24m868. [4m1[24m[4m1[24m901. [31mNA[39m ## [90m25[39m BTC 2020-08-11 [90m04:00:00[39m [4m1[24m[4m1[24m840. [4m1[24m[4m1[24m868. [4m1[24m[4m1[24m997. ## [90m26[39m BTC 2020-08-11 [90m05:00:00[39m [4m1[24m[4m1[24m820. [4m1[24m[4m1[24m840. [4m1[24m[4m1[24m986. ## [90m27[39m BTC 2020-08-11 [90m06:00:00[39m [4m1[24m[4m1[24m847. [4m1[24m[4m1[24m820. [4m1[24m[4m1[24m973. ## [90m28[39m BTC 2020-08-11 [90m07:00:00[39m [4m1[24m[4m1[24m774. [4m1[24m[4m1[24m847. [4m1[24m[4m1[24m994. ## [90m29[39m BTC 2020-08-11 [90m08:00:00[39m [4m1[24m[4m1[24m761. [4m1[24m[4m1[24m774. [4m1[24m[4m1[24m984. ## [90m30[39m BTC 2020-08-11 [90m09:00:00[39m [4m1[24m[4m1[24m753. [4m1[24m[4m1[24m761. [4m1[24m[4m1[24m962. ## [90m# … with 2,072 more rows, and 1 more variable: target_price_24h [3m[90m&lt;dbl&gt;[90m[23m[39m ## [90m# A tibble: 30 x 6[39m ## [90m# Groups: symbol [1][39m ## symbol ts_index price_usd lagged_price_1h lagged_price_24h ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dttm&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m BTC 2020-11-04 [90m12:00:00[39m [4m1[24m[4m3[24m724. [4m1[24m[4m3[24m689. [4m1[24m[4m3[24m532. ## [90m 2[39m BTC 2020-11-04 [90m13:00:00[39m [4m1[24m[4m3[24m828. [4m1[24m[4m3[24m724. [4m1[24m[4m3[24m517. ## [90m 3[39m BTC 2020-11-04 [90m14:00:00[39m [4m1[24m[4m3[24m837. [4m1[24m[4m3[24m828. [4m1[24m[4m3[24m699. ## [90m 4[39m BTC 2020-11-04 [90m15:00:00[39m [4m1[24m[4m3[24m840. [4m1[24m[4m3[24m837. [4m1[24m[4m3[24m787. ## [90m 5[39m BTC 2020-11-04 [90m16:00:00[39m [4m1[24m[4m3[24m914. [4m1[24m[4m3[24m840. [4m1[24m[4m3[24m707. ## [90m 6[39m BTC 2020-11-04 [90m17:00:00[39m [4m1[24m[4m4[24m032. [4m1[24m[4m3[24m914. [4m1[24m[4m3[24m716. ## [90m 7[39m BTC 2020-11-04 [90m18:00:00[39m [4m1[24m[4m4[24m072. [4m1[24m[4m4[24m032. [4m1[24m[4m3[24m733. ## [90m 8[39m BTC 2020-11-04 [90m19:00:00[39m [4m1[24m[4m4[24m090. [4m1[24m[4m4[24m072. [4m1[24m[4m3[24m740. ## [90m 9[39m BTC 2020-11-04 [90m20:00:00[39m [4m1[24m[4m4[24m106. [4m1[24m[4m4[24m090. [4m1[24m[4m3[24m722. ## [90m10[39m BTC 2020-11-04 [90m21:00:00[39m [4m1[24m[4m4[24m039. [4m1[24m[4m4[24m106. [4m1[24m[4m3[24m732. ## [90m# … with 20 more rows, and 1 more variable: target_price_24h [3m[90m&lt;dbl&gt;[90m[23m[39m There are several advantages to writing code the tidy way, but while some love it others hate it, so we won’t force anyone to have to understand how the %&gt;% operator works and we have stayed away from its use for the rest of the code shown, but we do encourage the use of this tool: https://magrittr.tidyverse.org/reference/pipe.html 4.5 Cross Validation [ADD HERE] (explain step below) NEW CV METHOD - need to explain: # Remove rows with null date_time_utc to exclude missing data from next steps cryptodata &lt;- drop_na(cryptodata, date_time_utc) # Counts by symbol cryptodata &lt;- mutate(group_by(cryptodata, symbol), tot_rows = n()) # Add row index by symbol cryptodata &lt;- mutate(arrange(cryptodata, date_time_utc), row_id = seq_along(date_time_utc)) # Calculate what rows belong in the first split cryptodata &lt;- cryptodata %&gt;% mutate(split_rows_1 = as.integer(n()/5), split_rows_2 = as.integer(split_rows_1*2), split_rows_3 = as.integer(split_rows_1*3), split_rows_4 = as.integer(split_rows_1*4), split_rows_5 = as.integer(split_rows_1*5)) # Now calculate what split the current row_id belongs into cryptodata &lt;- mutate(cryptodata, split = case_when( row_id &lt;= split_rows_1 ~ 1, row_id &lt;= split_rows_2 ~ 2, row_id &lt;= split_rows_3 ~ 3, row_id &lt;= split_rows_4 ~ 4, row_id &gt; split_rows_4 ~ 5)) # Now figure out train/test groups cryptodata &lt;- cryptodata %&gt;% mutate(train_rows_1 = (as.integer(n()/5))*0.8, test_rows_1 = train_rows_1 + (as.integer(n()/5))*0.2, train_rows_2 = test_rows_1 + train_rows_1, test_rows_2 = train_rows_2 + (as.integer(n()/5))*0.2, train_rows_3 = test_rows_2 + train_rows_1, test_rows_3 = train_rows_3 + (as.integer(n()/5))*0.2, train_rows_4 = test_rows_3 + train_rows_1, test_rows_4 = train_rows_4 + (as.integer(n()/5))*0.2, train_rows_5 = test_rows_4 + train_rows_1, test_rows_5 = train_rows_5 + (as.integer(n()/5))*0.2) # Now assign train/test groups cryptodata &lt;- mutate(cryptodata, training = case_when( row_id &lt;= train_rows_1 ~ &#39;train&#39;, row_id &lt;= test_rows_1 ~ &#39;test&#39;, row_id &lt;= train_rows_2 ~ &#39;train&#39;, row_id &lt;= test_rows_2 ~ &#39;test&#39;, row_id &lt;= train_rows_3 ~ &#39;train&#39;, row_id &lt;= test_rows_3 ~ &#39;test&#39;, row_id &lt;= train_rows_4 ~ &#39;train&#39;, row_id &lt;= test_rows_4 ~ &#39;test&#39;, row_id &lt;= train_rows_5 ~ &#39;train&#39;, row_id &gt; train_rows_5 ~ &#39;holdout&#39;)) # Remove all columns that are no longer needed now cryptodata &lt;- select(cryptodata, -(tot_rows:test_rows_5), -(trade_usd_1:trade_usd_5), -(ask_1_price:bid_5_quantity), -pair, -quote_currency, -pkDummy, -pkey, -ts_index, split) Our data now has the new columns training (train, test or holdout) and split (numbers 1-5) added to it, let’s take a look at the new columns: select(cryptodata, training, split) ## [90m# A tibble: 134,182 x 3[39m ## [90m# Groups: symbol [97][39m ## symbol training split ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m LEO train 1 ## [90m 2[39m GBX train 1 ## [90m 3[39m ACT train 1 ## [90m 4[39m MBL train 1 ## [90m 5[39m BTG train 1 ## [90m 6[39m ACAT train 1 ## [90m 7[39m LINK train 1 ## [90m 8[39m VET train 1 ## [90m 9[39m IHF train 1 ## [90m10[39m DGB train 1 ## [90m# … with 134,172 more rows[39m Notice that even though we left symbol variables out of our selection, because it is part of the way we grouped our data, it was added back in with the message “Adding missing grouping variables symbol”. The data is tied to its groupings when performing all operations until we use ungroup() to undo them. Let’s add the new split column to the way the data is grouped: cryptodata &lt;- group_by(cryptodata, symbol, split) The new field split, helps us split the data into 5 different datasets based on the date, and contains a number from 1-5. The new field training flags the data as being part of the train dataset, or the test (or holdout for the first split) dataset for each of the 5 splits/datasets. Running the same code as before with tail() added, we should see rows associated with the test data of the 5th split (again remember, each of the 5 splits has a training and testing dataset): tail( select(cryptodata, training, split) ) ## [90m# A tibble: 6 x 3[39m ## [90m# Groups: symbol, split [6][39m ## symbol training split ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m IPL holdout 5 ## [90m2[39m VET holdout 5 ## [90m3[39m AAB holdout 5 ## [90m4[39m CRPT holdout 5 ## [90m5[39m REP holdout 5 ## [90m6[39m BYTZ holdout 5 The easiest way to understand these groupings, is to visualize them. In the next section, you will learn powerful tools for visualizing data in R. Do not worry if you do not understand the code below and are not familiar with ggplot(), we will explain this framework in the next section, for now review the charts below and try to follow along with the way we are grouping the data for the predictive models by looking at what the x and y axis represent, as well as the colors. On the x-axis we are plotting the DateTime of when a data point was collected, and on the y-axis the split (1-5) as described in this section. The data is then colored based on the category assigned for the training variable (“train”,“test” or “holdout”). We can visualize the new grouping variables: groups_chart &lt;- ggplot(cryptodata, aes(x = date_time_utc, y = split, color = training)) + geom_point() #+ #scale_y_reverse() # now show the chart we just saved: groups_chart The chart above looks strange, we can view the results for the BTC cryptocurrency by running the same code as above, but instead of visualizing the dataset as a whole cryptodata, we will limit the data using filter(cryptodata, symbol == \"BTC\"). This time we are not saving these results as an R object. ggplot(filter(cryptodata, symbol == &#39;BTC&#39;), aes(x = date_time_utc, y = split, color = training)) + geom_point() The code above might seem difficult, but there is a standardized mathod to using these tools that is relatively easy to learn as we will see in the next section. We can check on the groupings for each cryptocurrency by animating the cryptodata object: library(gganimate) animated_chart &lt;- groups_chart + transition_states(symbol) + ggtitle(&#39;Now showing: {closest_state}&#39;) # show the new animated chart animate(animated_chart, fps = 2) This is another tool that we will walk through in the next section. It can be a bit hard to tell how many data points there are because they end up looking like lines. Let’s change the plot to use geom_jitter() instead of geom_point(), which will manually offset the points and give us a better impression of how many data points there are: animated_chart &lt;- animated_chart + geom_jitter() # show the new animated chart animate(animated_chart, fps = 2) 4.6 Fix Data by Split Now that we have split the data into many different subsets, those subsets themselves may have issues that prevent the predictive models from working as expected. 4.6.1 Zero Variance One of the first models we will make is a simple linear model. The regular R function for this will not work if the data contains any columns that have “zero variance”, meaning the value of the column never changes throughout the data being given to the model. Therefore, let’s fix any issues relating to zero variance columns in any dataset before we change the structure of the data in the step after this one. First change the grouping of the data, we are interested in calculating the zero variance based on the symbol, split and training fields. cryptodata &lt;- group_by(cryptodata, symbol, split, training) Now let’s create a new object called find_zero_var which shows the value of the minimum standard deviation across all columns and calculated based on the grouping of symbol, split and train. find_zero_var &lt;- select(mutate(cryptodata, min_sd = min(sd(price_usd, na.rm=T), sd(target_price_24h, na.rm=T), sd(lagged_price_1h, na.rm=T), sd(lagged_price_2h, na.rm=T), sd(lagged_price_3h, na.rm=T), sd(lagged_price_6h, na.rm=T), sd(lagged_price_12h, na.rm=T), sd(lagged_price_24h, na.rm=T))), min_sd) # Show data find_zero_var ## [90m# A tibble: 134,182 x 4[39m ## [90m# Groups: symbol, split, training [970][39m ## symbol split training min_sd ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m LEO 1 train 0.110 ## [90m 2[39m GBX 1 train 0.002[4m6[24m[4m6[24m ## [90m 3[39m ACT 1 train 0.000[4m5[24m[4m8[24m[4m1[24m ## [90m 4[39m MBL 1 train 0.000[4m0[24m[4m0[24m[4m6[24m88 ## [90m 5[39m BTG 1 train 0.389 ## [90m 6[39m ACAT 1 train 0.000[4m0[24m[4m1[24m[4m5[24m2 ## [90m 7[39m LINK 1 train 1.93 ## [90m 8[39m VET 1 train 0.000[4m9[24m[4m1[24m[4m3[24m ## [90m 9[39m IHF 1 train 0.003[4m2[24m[4m1[24m ## [90m10[39m DGB 1 train 0.002[4m1[24m[4m2[24m ## [90m# … with 134,172 more rows[39m Now let’s get to a list of cryptocurrency symbols where the minimum standard deviation across all columns for all splits of the data is 0, which is the list of cryptocurrencies to remove from the data. minimum_sd &lt;- filter(distinct(mutate(group_by(ungroup(find_zero_var), symbol), min_sd = min(min_sd, na.rm=T)), min_sd),min_sd &lt; 0.000001)$symbol # Show result minimum_sd ## [1] &quot;LEO&quot; &quot;ACT&quot; &quot;MBL&quot; &quot;ACAT&quot; &quot;DENT&quot; &quot;DAY&quot; &quot;IPL&quot; &quot;APPC&quot; &quot;IHT&quot; ## [10] &quot;IQ&quot; &quot;CDT&quot; &quot;MESH&quot; &quot;NAV&quot; &quot;FDZ&quot; &quot;REX&quot; &quot;SPC&quot; &quot;SBD&quot; &quot;GST&quot; ## [19] &quot;CND&quot; &quot;MG&quot; &quot;PHX&quot; &quot;CUTE&quot; &quot;BYTZ&quot; &quot;SENT&quot; &quot;WAXP&quot; &quot;BNK&quot; &quot;CHAT&quot; ## [28] &quot;WIKI&quot; &quot;NEU&quot; &quot;ECA&quot; &quot;JST&quot; &quot;SEELE&quot; &quot;AAB&quot; &quot;UTT&quot; &quot;DDR&quot; &quot;CMCT&quot; Now we can remove these symbols from appearing in the dataset: cryptodata &lt;- filter(cryptodata, !(symbol %in% minimum_sd)) In the code above we match all rows where the symbol is part of the minimum_sd object with the list of cryptocurrency symbols to remove from the data, and we then negate the selection using the ! operator to only keep rows with symbols not in the list we found. 4.7 Nest data [ADD HERE] … explain goal and method … First make sure groupings are correct cryptodata &lt;- group_by(cryptodata, symbol, split, training) Example nesting data: nest(cryptodata) ## [90m# A tibble: 610 x 4[39m ## [90m# Groups: symbol, training, split [610][39m ## symbol training split data ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;list&gt;[39m[23m ## [90m 1[39m GBX train 1 [90m&lt;tibble [215 × 11]&gt;[39m ## [90m 2[39m BTG train 1 [90m&lt;tibble [310 × 11]&gt;[39m ## [90m 3[39m LINK train 1 [90m&lt;tibble [228 × 11]&gt;[39m ## [90m 4[39m VET train 1 [90m&lt;tibble [313 × 11]&gt;[39m ## [90m 5[39m IHF train 1 [90m&lt;tibble [247 × 11]&gt;[39m ## [90m 6[39m DGB train 1 [90m&lt;tibble [335 × 11]&gt;[39m ## [90m 7[39m RCN train 1 [90m&lt;tibble [300 × 11]&gt;[39m ## [90m 8[39m PAXG train 1 [90m&lt;tibble [196 × 11]&gt;[39m ## [90m 9[39m LTC train 1 [90m&lt;tibble [334 × 11]&gt;[39m ## [90m10[39m NUT train 1 [90m&lt;tibble [308 × 11]&gt;[39m ## [90m# … with 600 more rows[39m First make training data nested: cryptodata_train &lt;- rename(nest(filter(cryptodata, training==&#39;train&#39;)), train_data = &#39;data&#39;) # Now remove training column cryptodata_train &lt;- select(ungroup(cryptodata_train, training), -training) # Fix issues with individual groups of the data cryptodata_train$train_data &lt;- lapply(cryptodata_train$train_data, na.omit) # Remove elements with no rows after na.omit step. CONFIRM THIS WORKS!!! # First add new column with nrow of train dataset cryptodata_train &lt;- group_by(ungroup(mutate(rowwise(cryptodata_train), train_rows = nrow(train_data))), symbol, split) # Remove all symbols where their train data has less than 20 rows at least once symbols_rm &lt;- unique(filter(cryptodata_train, split &lt; 5, train_rows &lt; 20)$symbol) # Remove all data relating to the symbols found above cryptodata_train &lt;- filter(cryptodata_train, ! symbol %in% symbols_rm) # ! is to make %not in% operator # Drop train_rows column cryptodata_train &lt;- select(cryptodata_train, -train_rows) Now nest test data: cryptodata_test &lt;- select(rename(nest(filter(cryptodata, training==&#39;test&#39;)), test_data = &#39;data&#39;), -training) # Now remove training column cryptodata_test &lt;- select(ungroup(cryptodata_test, training), -training) Also do holdout: cryptodata_holdout &lt;- rename(nest(filter(cryptodata, training==&#39;holdout&#39;)), holdout_data = &#39;data&#39;) # Remove split and training columns from holdout cryptodata_holdout &lt;- select(ungroup(cryptodata_holdout, split, training), -split, -training) Now join all nested data into the same dataframe # Join train and test cryptodata_nested &lt;- left_join(cryptodata_train, cryptodata_test,by = c(&quot;symbol&quot;, &quot;split&quot;)) # Join holdout cryptodata_nested &lt;- left_join(cryptodata_nested, cryptodata_holdout, by = c(&quot;symbol&quot;)) New data: cryptodata_nested ## [90m# A tibble: 260 x 5[39m ## [90m# Groups: symbol, split [260][39m ## symbol split train_data test_data holdout_data ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;list&gt;[39m[23m [3m[90m&lt;list&gt;[39m[23m [3m[90m&lt;list&gt;[39m[23m ## [90m 1[39m BTG 1 [90m&lt;tibble [223 × 11]&gt;[39m [90m&lt;tibble [78 × 11]&gt;[39m [90m&lt;tibble [82 × 11]&gt;[39m ## [90m 2[39m VET 1 [90m&lt;tibble [241 × 11]&gt;[39m [90m&lt;tibble [79 × 11]&gt;[39m [90m&lt;tibble [83 × 11]&gt;[39m ## [90m 3[39m IHF 1 [90m&lt;tibble [163 × 11]&gt;[39m [90m&lt;tibble [62 × 11]&gt;[39m [90m&lt;tibble [63 × 11]&gt;[39m ## [90m 4[39m DGB 1 [90m&lt;tibble [262 × 11]&gt;[39m [90m&lt;tibble [84 × 11]&gt;[39m [90m&lt;tibble [84 × 11]&gt;[39m ## [90m 5[39m LTC 1 [90m&lt;tibble [254 × 11]&gt;[39m [90m&lt;tibble [84 × 11]&gt;[39m [90m&lt;tibble [88 × 11]&gt;[39m ## [90m 6[39m NUT 1 [90m&lt;tibble [236 × 11]&gt;[39m [90m&lt;tibble [77 × 11]&gt;[39m [90m&lt;tibble [80 × 11]&gt;[39m ## [90m 7[39m NEXO 1 [90m&lt;tibble [259 × 11]&gt;[39m [90m&lt;tibble [84 × 11]&gt;[39m [90m&lt;tibble [86 × 11]&gt;[39m ## [90m 8[39m XMR 1 [90m&lt;tibble [244 × 11]&gt;[39m [90m&lt;tibble [84 × 11]&gt;[39m [90m&lt;tibble [84 × 11]&gt;[39m ## [90m 9[39m VIB 1 [90m&lt;tibble [215 × 11]&gt;[39m [90m&lt;tibble [72 × 11]&gt;[39m [90m&lt;tibble [73 × 11]&gt;[39m ## [90m10[39m XDN 1 [90m&lt;tibble [219 × 11]&gt;[39m [90m&lt;tibble [81 × 11]&gt;[39m [90m&lt;tibble [82 × 11]&gt;[39m ## [90m# … with 250 more rows[39m [ADD HERE - Intro to Visualization and explain we will use grouped data in PredictiveModeling] [ADD HERE - Worth mentioning the fact that some data will be higher quality, etc…? Give more background on those steps as “catch-alls”?] 4.8 Functional Programming [TODO - ADD HERE] –&gt; References "],["visualization.html", "Section - 5 Visualization 📉 5.1 Basics - ggplot2 5.2 Using Extensions", " Section - 5 Visualization 📉 Making visualizations using ggplot2 is one of the very best tools available in the R ecosystem. The gg in ggplot2 stands for the Grammar of Graphics, which is essentially the idea that many different types of charts share the same underlying building blocks, and that they can be put together in different ways to make charts that look very different from each other. In Hadley’s own words, “a pie chart is just a bar chart drawn in polar coordinates”, “They look very different, but in terms of the grammar they have a lot of underlying similarities.” 5.1 Basics - ggplot2 So how does ggplot2 actually work? “…in most cases you start with ggplot(), supply a dataset and aesthetic mapping (with aes()). You then add on layers (like geom_point() or geom_histogram()), scales (like scale_colour_brewer()), faceting specifications (like facet_wrap()) and coordinate systems (like coord_flip()).” - ggplot2.tidyverse.org/. Let’s break this down step by step. \"start with ggplot(), supply a dataset and aesthetic mapping (with aes()) Using the ggplot() function we supply the dataset first, and then define the aesthetic mapping (the visual properties of the chart) as having the date_time_utc on the x-axis, and the price_usd on the y-axis: ggplot(data = cryptodata, aes(x = date_time_utc, y = price_usd)) We were expecting a chart showing price over time, but the chart now shows up but is blank because we need to perform an additional step to determine how the data points are actually shown on the chart: “You then add on layers (like geom_point() or geom_histogram())…” We can take the exact same code as above and add + geom_point() to show the data on the chart as points: ggplot(data = cryptodata, aes(x = date_time_utc, y = price_usd)) + # adding geom_point(): geom_point() The most expensive cryptocurrency being shown, YFI in this case, makes it difficult to take a look at any of the other ones. Let’s try zooming-in on a single one by using the same code but making an adjustment to the data parameter to only show data for the cryptocurrency with the symbol ETH. Let’s filter the data down to the ETH cryptocurrency only and make the new dataset eth_data: eth_data &lt;- subset(cryptodata, symbol == &#39;ETH&#39;) We can now use the exact same code from earlier supplying the new filtered dataset for the data argument: ggplot(data = eth_data, aes(x = date_time_utc, y = price_usd)) + geom_point() This is better, but geom_point() might not be the best choice for this chart, let’s change geom_point with geom_line and see what that looks like: ggplot(data = eth_data, aes(x = date_time_utc, y = price_usd)) + # changing geom_point() into geom_line(): geom_line() Let’s save the results as an object called crypto_chart: crypto_chart &lt;- ggplot(data = eth_data, aes(x = date_time_utc, y = price_usd)) + geom_line() We can add a line showing the trend over time: crypto_chart &lt;- crypto_chart + stat_smooth() And we can show the new results by calling the crypto_chart object again: crypto_chart One particularly nice aspect of using the ggplot framework, is that we can keep adding as many elements and transformations to the chart as we would like with no limitations. We will not save the result shown below this time, but to illustrate this point, we can add a new line showing a linear regression fit going through the data using stat_smooth(method = 'lm'). And let’s also show the individual points in green. We could keep layering things on as much as we want: crypto_chart + # Add linear regression line stat_smooth(method = &#39;lm&#39;, color=&#39;red&#39;) + # Add points geom_point(color=&#39;dark green&#39;, size=0.8) By not providing any method option, the stat_smooth() function defaults to use the method called loess, which shows the local trends, while the lm model fits the best fitting linear regression line for the data as a whole. Of course we can add other components that make a visualization effective, let’s add labels to the chart now: crypto_chart &lt;- crypto_chart + ggtitle(paste(&#39;Price Change Over Time -&#39;, eth_data$symbol), subtitle = paste(&#39;Most recent data collected on:&#39;, max(eth_data$date_time_utc))) + xlab(&#39;Date Time (UTC)&#39;) + ylab(&#39;Price ($)&#39;) # display the new chart crypto_chart The ggplot2 package comes with a large amount of functionality. You can find a full reference of the functions you can use here: https://ggplot2.tidyverse.org/reference/ What makes the ggplot2 package even better is the fact that it also comes with a framework for anyone to develop their own extensions. Meaning there is a lot more functionality that the community has created that can be added in importing other packages that provide extensions to ggplot. 5.2 Using Extensions 5.2.1 ggthemes To use an extension, we just need to import it into our R session like we did with ggplot2 and the rest of the packages we want to use. We already loaded the ggthemes (Arnold 2019) package in the Setup section so we do not need to run library(ggthemes) to import the package into the session. We can apply a theme to the chart now and change the way it looks: crypto_chart &lt;- crypto_chart + theme_economist() # display the new chart crypto_chart See below for a full list of themes you can test. If you followed to this point try running the code crypto_chart + theme_excel() or any of the other options listed below instead of + theme_excel(): https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggthemes/ 5.2.2 plotly In some cases, it’s helpful to make a chart responsive to a cursor hovering over it. We can convert any ggplot into an interactive chart by using the plotly(Sievert et al. 2020) package, and it is super easy! We already imported the plotly package in the setup section, so all we need to do is wrap our chart in the function ggplotly(): ggplotly(crypto_chart) Use your mouse to hover over specific points on the chart above. Also notice that we did not overwrite the crypto_chart object, but are just displaying the results. If you are not looking to convert a ggplot to be interactive, plotly also provides its own framework for making charts from scratch, you can find out more about it here: https://plotly.com/r/ 5.2.3 ggpubr The ggpubr (Kassambara 2020) extension provides a lot of functionality that we won’t cover here, but one function we can use from this extension is stat_cor(), which allows us to add a correlation coefficient (R) and p-value to the chart. crypto_chart &lt;- crypto_chart + stat_cor() # Show chart crypto_chart We will dive deeper into these metrics in the section where we evaluate the performance of the models. 5.2.4 ggforce The ggforce package (Pedersen 2020) is a useful tool for annotating charts. We can annotate outliers for example: crypto_chart &lt;- crypto_chart + geom_mark_ellipse(aes(filter = price_usd == max(price_usd), label = date_time_utc, description = paste0(&#39;Price spike to $&#39;, price_usd))) + # Now the same to circle the minimum price: geom_mark_ellipse(aes(filter = price_usd == min(price_usd), label = date_time_utc, description = paste0(&#39;Price drop to $&#39;, price_usd))) When using the geom_mark_ellipse() function we are passing the data argument, the label and the description through the aes() function. We are marking two points, one for the minimum price during the time period, and one for the maximum price. For the first point we filter the data to only the point where the price_usd was equal to the max(price_usd) and add the labels accordingly. The same is done for the second point, but showing the lowest price point for the given date range. Now view the new chart: crypto_chart Notice that this chart is specifically annotated around these points, but we never specified the specific dates to circle, and we are always circling the maximum and minimum values regardless of the specific data. One of the points of this document is to show the idea that when it comes to data analysis, visualizations, and reporting, most people in the workplace approach these as one time tasks, but with the proper (open source/free) tools automation and reproducibility becomes a given, and any old analysis can be run again to get the exact same results, or could be performed on the most recent view of the data using the same exact methodology. 5.2.5 gganimate We can also extend the functionality of ggplot by using the gganimate (Pedersen and Robinson 2020) package, which allows us to create an animated GIF that iterates over groups in the data through the use of the transition_states() function. animated_prices &lt;- ggplot(data = mutate(cryptodata, groups=symbol), aes(x = date_time_utc, y = price_usd)) + geom_line() + theme_economist() + transition_states(groups) + ggtitle(&#39;Price Over Time&#39;,subtitle = &#39;{closest_state}&#39;) + stat_smooth() + view_follow() # this adjusts the axis based on the group # Show animation (slowed to 1 frame per second): animate(animated_prices,fps=1) We recommend consulting this documentation for simple and straightforward examples on using gganimate: https://gganimate.com/articles/gganimate.html 5.2.6 ggTimeSeries The ggTimeSeries (Kothari 2018) package has functionality that is helpful in plotting time series data. We can create a calendar heatmap of the price over time using the ggplot_calendar_heatmap() function. calendar_heatmap &lt;- ggplot_calendar_heatmap(eth_data,&#39;date_time_utc&#39;,&#39;price_usd&#39;) #or do target_percent_change here? calendar_heatmap DoW on the y-axis stands for Day Of the Week To read this chart in the correct date order start from the top left and work your way down and to the right once you reach the bottom of the column. The lighter the color the higher the price on the specific day. 5.2.7 Rayshader The previous chart is helpful, but a color scale like that can be a bit difficult to interpret. We could convert the previous chart into a 3d figure that is easier to visually interpret by using the amazing rayshader (Morgan-Wall 2020) package. This document runs automatically through GitHub Actions, which does not have a graphical environment to run the code below, which prevents it from refreshing the results with the latest data. We are showing old results for the Rayshader section below. If you have gotten to this point, run the code below yourself on the latest data to see this amazing package in action! # First remove the title from the legend to avoid visual issues calendar_heatmap &lt;- calendar_heatmap + theme(legend.title = element_blank()) # Add the date to the title to make it clear these refresh twice daily calendar_heatmap &lt;- calendar_heatmap + ggtitle(paste0(&#39;Through: &#39;,substr(max(eth_data$date_time_utc),1,10))) # Convert to 3d plot plot_gg(calendar_heatmap, zoom = 0.60, phi = 35, theta = 45) # Render snapshot render_snapshot(&#39;rayshader_image.png&#39;) # Close RGL (which opens on plot_gg() command in a separate window) rgl.close() This is the same two dimensional calendar heatmap that was made earlier. Because we can programmatically adjust the camera as shown above, that means that we can also create a snapshot, move the camera and take another one, and keep going until we have enough to make it look like a video! This is not difficult to do using the render_movie() function, which will take care of everything behind the scenes for the same plot as before: # This time let&#39;s remove the scale too since we aren&#39;t changing it: calendar_heatmap &lt;- calendar_heatmap + theme(legend.position = &quot;none&quot;) # Same 3d plot as before plot_gg(calendar_heatmap, zoom = 0.60, phi = 35, theta = 45) # Render movie render_movie(&#39;rayshader_video.mp4&#39;) # Close RGL rgl.close() Click on the video below to play the output Video Awesome! Move on to the next section to start making predictive models for the data.. References "],["predictive-modeling.html", "Section - 6 Predictive Modeling 6.1 What is it? 6.2 Example Simple Model 6.3 Caret 6.4 Make Predictions 6.5 Traditional Timeseries", " Section - 6 Predictive Modeling 6.1 What is it? 6.2 Example Simple Model …First need to run example normal lm model, etc… 6.2.1 Using Functional Programming linear_model &lt;- function(df){ lm(price_usd ~ . -date_time_utc -date, data = df) } Can now use it for map() cryptodata_nested %&gt;% mutate(lm_model = map(train_data, linear_model)) ## [90m# A tibble: 260 x 6[39m ## [90m# Groups: symbol, split [260][39m ## symbol split train_data test_data holdout_data lm_model ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;list&gt;[39m[23m [3m[90m&lt;list&gt;[39m[23m [3m[90m&lt;list&gt;[39m[23m [3m[90m&lt;list&gt;[39m[23m ## [90m 1[39m BTG 1 [90m&lt;tibble [223 × 11]&gt;[39m [90m&lt;tibble [78 × 11[0m… [90m&lt;tibble [82 × 11[0m… [90m&lt;lm&gt;[39m ## [90m 2[39m VET 1 [90m&lt;tibble [241 × 11]&gt;[39m [90m&lt;tibble [79 × 11[0m… [90m&lt;tibble [83 × 11[0m… [90m&lt;lm&gt;[39m ## [90m 3[39m IHF 1 [90m&lt;tibble [163 × 11]&gt;[39m [90m&lt;tibble [62 × 11[0m… [90m&lt;tibble [63 × 11[0m… [90m&lt;lm&gt;[39m ## [90m 4[39m DGB 1 [90m&lt;tibble [262 × 11]&gt;[39m [90m&lt;tibble [84 × 11[0m… [90m&lt;tibble [84 × 11[0m… [90m&lt;lm&gt;[39m ## [90m 5[39m LTC 1 [90m&lt;tibble [254 × 11]&gt;[39m [90m&lt;tibble [84 × 11[0m… [90m&lt;tibble [88 × 11[0m… [90m&lt;lm&gt;[39m ## [90m 6[39m NUT 1 [90m&lt;tibble [236 × 11]&gt;[39m [90m&lt;tibble [77 × 11[0m… [90m&lt;tibble [80 × 11[0m… [90m&lt;lm&gt;[39m ## [90m 7[39m NEXO 1 [90m&lt;tibble [259 × 11]&gt;[39m [90m&lt;tibble [84 × 11[0m… [90m&lt;tibble [86 × 11[0m… [90m&lt;lm&gt;[39m ## [90m 8[39m XMR 1 [90m&lt;tibble [244 × 11]&gt;[39m [90m&lt;tibble [84 × 11[0m… [90m&lt;tibble [84 × 11[0m… [90m&lt;lm&gt;[39m ## [90m 9[39m VIB 1 [90m&lt;tibble [215 × 11]&gt;[39m [90m&lt;tibble [72 × 11[0m… [90m&lt;tibble [73 × 11[0m… [90m&lt;lm&gt;[39m ## [90m10[39m XDN 1 [90m&lt;tibble [219 × 11]&gt;[39m [90m&lt;tibble [81 × 11[0m… [90m&lt;tibble [82 × 11[0m… [90m&lt;lm&gt;[39m ## [90m# … with 250 more rows[39m 6.3 Caret 6.3.1 Parallel Processing [ADD HERE About R only using one CPU as deafault but can use more enabling parallel processing] library(doParallel) cl &lt;- makePSOCKcluster(12) registerDoParallel(cl) 6.3.2 Functional Programming - here or elsewhere? [ADD HERE] Here use Caret + purrr to make models linear_model_caret &lt;- function(df){ train(price_usd ~ . -date_time_utc -date, data = df, method = &#39;lm&#39;, trControl=trainControl(method=&quot;none&quot;)) } Can now use it for map() cryptodata_nested &lt;- mutate(cryptodata_nested, lm_model = map(train_data, linear_model_caret)) 6.3.3 Cross Validation Within each split we created, we can set caret to perform an additional cross-validation step to allow it to do a minimal level of automated hyperparameter tuning as it creates the models (the more we do the longer it will take). fitControl &lt;- trainControl(## 3-fold CV method = &quot;repeatedcv&quot;, number = 3, ## repeated three times repeats = 3) Now create more generalized version model_caret &lt;- function(df, method_choice){ train(price_usd ~ . -date_time_utc -date, data = df, method = method_choice, trControl=fitControl) } 6.3.4 XGBoost models Will now need to use map2() cryptodata_nested &lt;- mutate(cryptodata_nested, xgb_model = map2(train_data, &quot;xgbLinear&quot;,model_caret)) Can also make a tree based XGBoost model: cryptodata_nested &lt;- mutate(cryptodata_nested, xgbTree_model = map2(train_data, &quot;xgbTree&quot;,model_caret)) [TODO - Reference xgboost documentation. Can I embed it? https://xgboost.readthedocs.io/en/latest/parameter.html] 6.3.5 All Other Models Can use the same function and methodology to keep adding models. Could also all be done in one step adding more arguments to mutate(), but broken up to be verbose and take it step by step. 6.3.5.1 Neural Network models cryptodata_nested &lt;- mutate(cryptodata_nested, nnet_model = map2(train_data, &quot;dnn&quot;, model_caret)) 6.3.5.2 Gradient Boosting Machines [TODO - Here could also mention using preProcess function to do things like center and scale data ] 6.4 Make Predictions [TODO] … first one example… predict(object = cryptodata_nested$lm_model[[1]], newdata = cryptodata_nested$test_data[[1]], na.action = na.pass) ## 1 2 3 4 5 6 7 8 ## 10.654296 10.484721 10.745612 10.627524 10.761840 10.715940 10.563616 10.603835 ## 9 10 11 12 13 14 15 16 ## 10.524759 10.568445 10.593996 NA 10.672922 10.769106 10.769692 10.802986 ## 17 18 19 20 21 22 23 24 ## 10.756915 10.826577 10.810058 10.794221 10.784459 10.882697 10.644316 10.716933 ## 25 26 27 28 29 30 31 32 ## 10.730797 10.749536 10.740557 10.695554 10.874845 10.823428 10.841033 10.931323 ## 33 34 35 36 37 38 39 40 ## 10.905056 10.869453 10.839193 NA NA NA 10.837905 10.831368 ## 41 42 43 44 45 46 47 48 ## NA 10.909620 10.901146 10.890353 10.872610 10.863503 NA 10.762999 ## 49 50 51 52 53 54 55 56 ## 10.796460 10.783239 10.736807 10.687971 10.572365 10.619417 10.747637 10.777636 ## 57 58 59 60 61 62 63 64 ## 10.664306 10.555497 NA 10.451262 10.388756 10.341541 10.161545 10.271305 ## 65 66 67 68 69 70 71 72 ## 9.950323 9.692075 9.945451 9.839380 9.945306 9.415429 9.612833 9.718127 ## 73 74 75 76 77 78 ## 9.633075 9.710518 9.636421 9.657263 9.788078 9.907188 … now make function to use for map make_predictions &lt;- function(model, test){ predict(object = model, newdata = test, na.action = na.pass) } And use map2() to use it to make predictions and create new columns for both the test data and the holdout: cryptodata_nested &lt;- mutate(cryptodata_nested, lm_test_predictions = map2(lm_model, test_data, make_predictions), lm_holdout_predictions = map2(lm_model, holdout_data, make_predictions)) We can view the results: select(cryptodata_nested, lm_test_predictions, lm_holdout_predictions) ## [90m# A tibble: 260 x 4[39m ## [90m# Groups: symbol, split [260][39m ## symbol split lm_test_predictions lm_holdout_predictions ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;list&gt;[39m[23m [3m[90m&lt;list&gt;[39m[23m ## [90m 1[39m BTG 1 [90m&lt;dbl [78]&gt;[39m [90m&lt;dbl [82]&gt;[39m ## [90m 2[39m VET 1 [90m&lt;dbl [79]&gt;[39m [90m&lt;dbl [83]&gt;[39m ## [90m 3[39m IHF 1 [90m&lt;dbl [62]&gt;[39m [90m&lt;dbl [63]&gt;[39m ## [90m 4[39m DGB 1 [90m&lt;dbl [84]&gt;[39m [90m&lt;dbl [84]&gt;[39m ## [90m 5[39m LTC 1 [90m&lt;dbl [84]&gt;[39m [90m&lt;dbl [88]&gt;[39m ## [90m 6[39m NUT 1 [90m&lt;dbl [77]&gt;[39m [90m&lt;dbl [80]&gt;[39m ## [90m 7[39m NEXO 1 [90m&lt;dbl [84]&gt;[39m [90m&lt;dbl [86]&gt;[39m ## [90m 8[39m XMR 1 [90m&lt;dbl [84]&gt;[39m [90m&lt;dbl [84]&gt;[39m ## [90m 9[39m VIB 1 [90m&lt;dbl [72]&gt;[39m [90m&lt;dbl [73]&gt;[39m ## [90m10[39m XDN 1 [90m&lt;dbl [81]&gt;[39m [90m&lt;dbl [82]&gt;[39m ## [90m# … with 250 more rows[39m Now we can do the same for the rest of the models: cryptodata_nested &lt;- mutate(cryptodata_nested, # XGBoost: xgb_test_predictions = map2(xgb_model, test_data, make_predictions), # holdout xgb_holdout_predictions = map2(xgb_model, holdout_data, make_predictions), # XGBoost Trees: xgbTree_test_predictions = map2(xgbTree_model, test_data, make_predictions), # holdout xgbTree_holdout_predictions = map2(xgbTree_model, holdout_data, make_predictions), # Neural Network: nnet_test_predictions = map2(nnet_model, test_data, make_predictions), # holdout nnet_holdout_predictions = map2(nnet_model, holdout_data, make_predictions)) Done with the parallel processing now: stopCluster(cl) 6.5 Traditional Timeseries "],["evaluate-model-performance.html", "Section - 7 Evaluate Model Performance 7.1 Summarizing models 7.2 Adjust Prices 7.3 Review Summary Statistics 7.4 Adjust Prices - All Models 7.5 Eval RMSE 7.6 Eval R^2 7.7 Visualize Results", " Section - 7 Evaluate Model Performance 7.1 Summarizing models Example for one model: postResample(pred = cryptodata_nested$lm_test_predictions[[1]], obs = cryptodata_nested$test_data[[1]]$target_price_24h) ## RMSE Rsquared MAE ## 0.5240425 0.1462679 0.3762959 We can extract the first element to return the RMSE metric, and the second element for the R Squared (R^2) metric. We are using [[1]] to extract the first element of the lm_test_predictions and test_data and compare the predictions to the actual value of the target_price24h column. print(paste(&#39;Now showing RMSE example:&#39;, postResample(pred = cryptodata_nested$lm_test_predictions[[1]], obs = cryptodata_nested$test_data[[1]]$target_price_24h)[[1]])) ## [1] &quot;Now showing RMSE example: 0.5240424858001&quot; print(paste(&#39;Now showing R Squared example:&#39;, postResample(pred = cryptodata_nested$lm_test_predictions[[1]], obs = cryptodata_nested$test_data[[1]]$target_price_24h)[[2]])) ## [1] &quot;Now showing R Squared example: 0.146267897057743&quot; This model used the earliest subset of the data available for the cryptocurrency. How does the same model used to predict this older subset of the data perform when applied to the most recent subset of the data? We can get the same summary of results on the holdout: postResample(pred = cryptodata_nested$lm_holdout_predictions[[1]], obs = cryptodata_nested$holdout_data[[1]]$target_price_24h) ## RMSE Rsquared MAE ## 0.136656412 0.001451242 0.107768247 7.1.1 Comparing Metrics How do these two results compare and why is this comparison important? The RMSE 7.2 Adjust Prices Because cryptocurrencies can vary dramatically in their prices with some trading in the tens of thousands of dollars and others trading for less than a cent, we need to make sure to standardize the RMSE columns to provide a fair comparison for the metric. Therefore, before using the postResample() function, let’s convert both the predictions and the target to be the % change in price over the 24 hour period, rather than the change in price ($). This step is particularly tedious, but it is important. As with the rest of this tutorial, try to understand what we are doing and why even if you find the code overwhelming. All we are doing in this “Adjust Prices” section is we are adjusting all of the prices to be represented as percentage change between observations, which will allow us to draw a fair comparison of the metrics across all cryptocurrencies, which would not be possible using the prices themselves. 7.2.1 Add Last Price In order to convert the first prediction made to be a percentage, we need to know the previous price. Function to add last_price_train column and append it to the predictions made so we can calculate the % change of the first element, before later removing the value not associated with the predictions: last_train_price &lt;- function(train_data, predictions){ c(tail(train_data$price_usd,1), predictions) } Now overwrite the old predictions: cryptodata_nested &lt;- mutate(cryptodata_nested, lm_test_predictions = ifelse(split &lt; 5, map2(train_data, lm_test_predictions, last_train_price), NA)) 7.2.1.1 Holdout Do the same for the holdout. For all of holdout need to take last price point of 5th training split. cryptodata_nested_holdout &lt;- mutate(filter(cryptodata_nested, split == 5), lm_holdout_predictions = map2(train_data, lm_holdout_predictions, last_train_price)) Join the holdout data to all rows based on the cryptocurrency symbol alone: cryptodata_nested &lt;- left_join(cryptodata_nested, select(cryptodata_nested_holdout, symbol, lm_holdout_predictions), by=&#39;symbol&#39;) # Remove unwanted columns cryptodata_nested &lt;- select(cryptodata_nested, -lm_holdout_predictions.x, -split.y) # Rename the columns kept cryptodata_nested &lt;- rename(cryptodata_nested, lm_holdout_predictions = &#39;lm_holdout_predictions.y&#39;, split = &#39;split.x&#39;) # Reset the correct grouping structure cryptodata_nested &lt;- group_by(cryptodata_nested, symbol, split) 7.2.2 Convert to Percentage Change standardize_perc_change &lt;- function(predictions){ results &lt;- (diff(c(lag(predictions, 1), predictions)) / lag(predictions, 1))*100 # Exclude the first element, next element will be % change of first prediction tail(head(results, length(predictions)), length(predictions)-1) } Overwrite the old predictions with the new predictions adjusted as a percentage now: cryptodata_nested &lt;- mutate(cryptodata_nested, lm_test_predictions = ifelse(split &lt; 5, map(lm_test_predictions, standardize_perc_change), NA), # Holdout for all splits lm_holdout_predictions = map(lm_holdout_predictions, standardize_perc_change)) 7.2.3 Actuals Now do the same thing to the actual prices. Let’s make a new column called actuals containing the real price values (rather than the predicted ones): actuals_create &lt;- function(train_data, test_data){ c(tail(train_data$price_usd,1), as.numeric(unlist(select(test_data, price_usd)))) } Use the new function to create the new column actuals: cryptodata_nested &lt;- mutate(cryptodata_nested, actuals_test = ifelse(split &lt; 5, map2(train_data, test_data, actuals_create), NA)) 7.2.3.1 Holdout Again, for the holdout we need the price from the training data of the 5th split cryptodata_nested_holdout &lt;- mutate(filter(cryptodata_nested, split == 5), actuals_holdout = map2(train_data, holdout_data, actuals_create)) Join the holdout data to all rows based on the cryptocurrency symbol alone: cryptodata_nested &lt;- left_join(cryptodata_nested, select(cryptodata_nested_holdout, symbol, actuals_holdout), by=&#39;symbol&#39;) # Remove unwanted columns cryptodata_nested &lt;- select(cryptodata_nested, -split.y) # Rename the columns kept cryptodata_nested &lt;- rename(cryptodata_nested, split = &#39;split.x&#39;) # Reset the correct grouping structure cryptodata_nested &lt;- group_by(cryptodata_nested, symbol, split) 7.2.4 Actuals as % Change Now we can convert the new actuals to express the price_usd as a % change relative to the previous value using adapting the function from earlier: actuals_perc_change &lt;- function(predictions){ results &lt;- (diff(c(lag(predictions, 1), predictions)) / lag(predictions, 1))*100 # Exclude the first element, next element will be % change of first prediction tail(head(results, length(predictions)), length(predictions)-1) } cryptodata_nested &lt;- mutate(cryptodata_nested, actuals_test = ifelse(split &lt; 5, map(actuals_test, actuals_perc_change), NA), actuals_holdout = map(actuals_holdout, actuals_perc_change)) 7.3 Review Summary Statistics Now that we standardized the price to show the percentage change relative to the previous period instead of the price in dollars, we can actually compare the summary statistics across all cryptocurrencies and have it be a fair comparison. Let’s get the same statistic as we did at the beginning of this section, but this time on the standardized values: hydroGOF::rmse(cryptodata_nested$lm_test_predictions[[1]], cryptodata_nested$actuals_test[[1]], na.rm=T) ## [1] 1.843896 7.3.1 Calculate RMSE Now make a function to get the RMSE metric for all models: evaluate_preds_rmse &lt;- function(predictions, actuals){ hydroGOF::rmse(predictions, actuals, na.rm=T) } Now we can use map2() to use it to get the RMSE metric for both the test data and the holdout: cryptodata_nested &lt;- mutate(cryptodata_nested, lm_rmse_test = unlist(ifelse(split &lt; 5, map2(lm_test_predictions, actuals_test, evaluate_preds_rmse), NA)), lm_rmse_holdout = unlist(map2(lm_holdout_predictions, actuals_holdout, evaluate_preds_rmse))) Look at the results: select(cryptodata_nested, lm_rmse_test, lm_rmse_holdout) ## [90m# A tibble: 260 x 4[39m ## [90m# Groups: symbol, split [260][39m ## symbol split lm_rmse_test lm_rmse_holdout ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m BTG 1 1.84 1.05 ## [90m 2[39m VET 1 1.04 1.76 ## [90m 3[39m IHF 1 0.734 0.982 ## [90m 4[39m DGB 1 1.62 1.35 ## [90m 5[39m LTC 1 0.858 1.25 ## [90m 6[39m NUT 1 1.70 0.380 ## [90m 7[39m NEXO 1 2.04 1.87 ## [90m 8[39m XMR 1 0.733 1.79 ## [90m 9[39m VIB 1 0.945 3.92 ## [90m10[39m XDN 1 1.28 3.10 ## [90m# … with 250 more rows[39m Out of 260 groups, 88 had an equal or lower RMSE score for the holdout than the test set. 7.3.2 Calculate R^2 Now we can do the same for the R Squared metric: evaluate_preds_rsq &lt;- function(predictions, actuals){ postResample(pred = predictions, obs = actuals)[[2]] } cryptodata_nested &lt;- mutate(cryptodata_nested, lm_rsq_test = unlist(ifelse(split &lt; 5, map2(lm_test_predictions, actuals_test, evaluate_preds_rsq), NA)), lm_rsq_holdout = unlist(map2(lm_holdout_predictions, actuals_holdout, evaluate_preds_rsq))) Look at the results. Wrapping them in print(n=500) overwrites the behavior to only give a preview of the data so we can view the full results (up to 500 observations). print(select(cryptodata_nested, lm_rmse_test, lm_rmse_holdout, lm_rsq_test, lm_rsq_holdout), n=500) ## [90m# A tibble: 260 x 6[39m ## [90m# Groups: symbol, split [260][39m ## symbol split lm_rmse_test lm_rmse_holdout lm_rsq_test lm_rsq_holdout ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m BTG 1 1.84 1.05 0.092[4m4[24m 0.016[4m1[24m ## [90m 2[39m VET 1 1.04 1.76 0.298 0.973 ## [90m 3[39m IHF 1 0.734 0.982 0.000[4m4[24m[4m4[24m[4m9[24m [31mNA[39m ## [90m 4[39m DGB 1 1.62 1.35 0.003[4m2[24m[4m1[24m 0.007[4m8[24m[4m9[24m ## [90m 5[39m LTC 1 0.858 1.25 0.000[4m0[24m[4m7[24m[4m7[24m1 0.015[4m9[24m ## [90m 6[39m NUT 1 1.70 0.380 0.008[4m1[24m[4m6[24m 0.136 ## [90m 7[39m NEXO 1 2.04 1.87 0.435 0.007[4m4[24m[4m9[24m ## [90m 8[39m XMR 1 0.733 1.79 0.002[4m3[24m[4m5[24m 0.002[4m0[24m[4m2[24m ## [90m 9[39m VIB 1 0.945 3.92 0.010[4m2[24m 0.263 ## [90m 10[39m XDN 1 1.28 3.10 0.454 0.056[4m3[24m ## [90m 11[39m XNS 1 0.864 10.6 1 0.015[4m8[24m ## [90m 12[39m REP 1 4.12 1.70 0.097[4m8[24m 0.058[4m7[24m ## [90m 13[39m BAT 1 1.78 1.03 0.825 0.019[4m5[24m ## [90m 14[39m BTC 1 0.485 0.947 0.000[4m1[24m[4m0[24m[4m0[24m 0.016[4m3[24m ## [90m 15[39m DCR 1 1.66 1.73 0.469 0.010[4m9[24m ## [90m 16[39m ENJ 1 2.14 1.14 0.031[4m0[24m 0.011[4m5[24m ## [90m 17[39m XEM 1 2.98 1.30 0.012[4m3[24m 0.000[4m3[24m[4m2[24m[4m5[24m ## [90m 18[39m HT 1 1.51 1.62 0.057[4m3[24m 0.011[4m5[24m ## [90m 19[39m EOS 1 1.39 1.02 0.242 0.003[4m3[24m[4m8[24m ## [90m 20[39m EDG 1 3.74 8.44 0.107 0.086[4m6[24m ## [90m 21[39m SRN 1 2.77 5.82 0.003[4m9[24m[4m8[24m 0.299 ## [90m 22[39m CUR 1 0.012[4m9[24m 0.018[4m8[24m 0.021[4m5[24m 0.020[4m8[24m ## [90m 23[39m XVG 1 1.38 0.924 0.009[4m8[24m[4m8[24m 0.000[4m1[24m[4m4[24m[4m5[24m ## [90m 24[39m NCT 1 1.26 1.36 0.000[4m0[24m[4m0[24m[4m6[24m60 [31mNA[39m ## [90m 25[39m STORJ 1 0.540 1.49 [31mNA[39m 0.063[4m1[24m ## [90m 26[39m BRD 1 0.738 1.29 0.118 0.015[4m8[24m ## [90m 27[39m NCT 2 2.77 1.36 0.387 [31mNA[39m ## [90m 28[39m IHF 2 1.51 0.982 0.004[4m9[24m[4m0[24m [31mNA[39m ## [90m 29[39m CUR 2 0.008[4m6[24m[4m9[24m 0.018[4m8[24m 0.975 0.020[4m8[24m ## [90m 30[39m REP 2 2.49 1.70 0.002[4m1[24m[4m7[24m 0.058[4m7[24m ## [90m 31[39m VIB 2 2.69 3.92 0.158 0.263 ## [90m 32[39m XVG 2 2.09 0.924 0.000[4m4[24m[4m3[24m[4m2[24m 0.000[4m1[24m[4m4[24m[4m5[24m ## [90m 33[39m NUT 2 1.49 0.380 0.031[4m7[24m 0.136 ## [90m 34[39m BTG 2 1.08 1.05 0.000[4m0[24m[4m0[24m[4m1[24m74 0.016[4m1[24m ## [90m 35[39m VET 2 1.38 1.76 0.111 0.973 ## [90m 36[39m ZEC 1 1.48 1.62 0.445 0.106 ## [90m 37[39m XEM 2 1.61 1.30 0.009[4m1[24m[4m4[24m 0.000[4m3[24m[4m2[24m[4m5[24m ## [90m 38[39m HT 2 0.783 1.62 0.171 0.011[4m5[24m ## [90m 39[39m XNS 2 1.36 10.6 0.544 0.015[4m8[24m ## [90m 40[39m XDN 2 0.692 3.10 0.467 0.056[4m3[24m ## [90m 41[39m EOS 2 0.754 1.02 0.000[4m0[24m[4m9[24m[4m0[24m4 0.003[4m3[24m[4m8[24m ## [90m 42[39m ENJ 2 1.55 1.14 0.017[4m5[24m 0.011[4m5[24m ## [90m 43[39m NEXO 2 1.09 1.87 0.163 0.007[4m4[24m[4m9[24m ## [90m 44[39m EDG 2 21.9 8.44 0.047[4m1[24m 0.086[4m6[24m ## [90m 45[39m DGB 2 1.09 1.35 0.209 0.007[4m8[24m[4m9[24m ## [90m 46[39m LTC 2 0.817 1.25 0.030[4m3[24m 0.015[4m9[24m ## [90m 47[39m XMR 2 0.841 1.79 0.000[4m6[24m[4m9[24m[4m8[24m 0.002[4m0[24m[4m2[24m ## [90m 48[39m BTC 2 0.475 0.947 0.003[4m2[24m[4m7[24m 0.016[4m3[24m ## [90m 49[39m BAT 2 1.03 1.03 0.091[4m2[24m 0.019[4m5[24m ## [90m 50[39m DCR 2 1.33 1.73 0.001[4m0[24m[4m0[24m 0.010[4m9[24m ## [90m 51[39m SRN 2 3.69 5.82 0.188 0.299 ## [90m 52[39m STORJ 2 1.72 1.49 0.672 0.063[4m1[24m ## [90m 53[39m BRD 2 [31mNaN[39m 1.29 [31mNA[39m 0.015[4m8[24m ## [90m 54[39m NCT 3 3.32 1.36 0.076[4m8[24m [31mNA[39m ## [90m 55[39m IHF 3 0.442 0.982 0.030[4m8[24m [31mNA[39m ## [90m 56[39m CUR 3 6.69 0.018[4m8[24m 0.000[4m0[24m[4m0[24m[4m0[24m510 0.020[4m8[24m ## [90m 57[39m REP 3 0.942 1.70 0.392 0.058[4m7[24m ## [90m 58[39m CBC 1 0.583 2.95 0.612 0.081[4m4[24m ## [90m 59[39m ETH 1 0.822 1.16 0.009[4m0[24m[4m8[24m 0.003[4m8[24m[4m1[24m ## [90m 60[39m BNT 1 1.33 1.81 0.104 0.007[4m0[24m[4m3[24m ## [90m 61[39m MANA 1 0.913 0.814 0.032[4m6[24m 0.002[4m6[24m[4m2[24m ## [90m 62[39m OAX 1 1.10 6.82 0.120 0.159 ## [90m 63[39m ZRX 1 1.08 1.13 0.600 0.001[4m8[24m[4m9[24m ## [90m 64[39m TRX 1 1.08 0.869 0.118 0.004[4m8[24m[4m8[24m ## [90m 65[39m BSV 1 0.965 1.15 0.028[4m0[24m 0.010[4m8[24m ## [90m 66[39m SUB 1 3.05 3.02 0.354 [31mNA[39m ## [90m 67[39m BTT 1 0.925 1.11 0.208 0.000[4m7[24m[4m5[24m[4m4[24m ## [90m 68[39m SMART 1 1.67 0.819 0.222 0.039[4m7[24m ## [90m 69[39m ETP 1 0.663 0.769 0.042[4m2[24m 0.026[4m9[24m ## [90m 70[39m CRO 1 0.382 1.03 0.480 0.004[4m7[24m[4m6[24m ## [90m 71[39m BCN 1 1.28 1.10 0.031[4m3[24m 0.679 ## [90m 72[39m ELF 1 0.167 0.836 0.452 0.024[4m4[24m ## [90m 73[39m VIB 3 9.73 3.92 [31mNA[39m 0.263 ## [90m 74[39m XVG 3 2.43 0.924 0.099[4m4[24m 0.000[4m1[24m[4m4[24m[4m5[24m ## [90m 75[39m NUT 3 0.456 0.380 0.247 0.136 ## [90m 76[39m BTG 3 0.434 1.05 0.152 0.016[4m1[24m ## [90m 77[39m VET 3 1.34 1.76 0.154 0.973 ## [90m 78[39m XEM 3 1.89 1.30 0.979 0.000[4m3[24m[4m2[24m[4m5[24m ## [90m 79[39m HT 3 0.429 1.62 0.000[4m9[24m[4m9[24m[4m2[24m 0.011[4m5[24m ## [90m 80[39m SRN 3 5.35 5.82 0.122 0.299 ## [90m 81[39m STORJ 3 1.11 1.49 [31mNA[39m 0.063[4m1[24m ## [90m 82[39m EOS 3 0.586 1.02 0.065[4m2[24m 0.003[4m3[24m[4m8[24m ## [90m 83[39m ENJ 3 4.99 1.14 0.266 0.011[4m5[24m ## [90m 84[39m XDN 3 0.273 3.10 0.022[4m1[24m 0.056[4m3[24m ## [90m 85[39m NEXO 3 1.63 1.87 0.024[4m2[24m 0.007[4m4[24m[4m9[24m ## [90m 86[39m XMR 3 1.39 1.79 0.349 0.002[4m0[24m[4m2[24m ## [90m 87[39m EDG 3 18.5 8.44 0.335 0.086[4m6[24m ## [90m 88[39m LTC 3 0.720 1.25 0.003[4m6[24m[4m8[24m 0.015[4m9[24m ## [90m 89[39m DGB 3 1.50 1.35 0.414 0.007[4m8[24m[4m9[24m ## [90m 90[39m BTC 3 0.502 0.947 0.000[4m1[24m[4m6[24m[4m5[24m 0.016[4m3[24m ## [90m 91[39m BAT 3 1.73 1.03 0.097[4m3[24m 0.019[4m5[24m ## [90m 92[39m DCR 3 0.751 1.73 0.008[4m1[24m[4m4[24m 0.010[4m9[24m ## [90m 93[39m ZEC 2 0.811 1.62 0.011[4m1[24m 0.106 ## [90m 94[39m SUB 2 [31mNaN[39m 3.02 [31mNA[39m [31mNA[39m ## [90m 95[39m CRO 2 [31mNaN[39m 1.03 [31mNA[39m 0.004[4m7[24m[4m6[24m ## [90m 96[39m ETH 2 1.23 1.16 0.356 0.003[4m8[24m[4m1[24m ## [90m 97[39m BNT 2 0.993 1.81 1 0.007[4m0[24m[4m3[24m ## [90m 98[39m OAX 2 21.9 6.82 0.040[4m8[24m 0.159 ## [90m 99[39m TRX 2 0.578 0.869 0.046[4m6[24m 0.004[4m8[24m[4m8[24m ## [90m100[39m BSV 2 2.32 1.15 0.226 0.010[4m8[24m ## [90m101[39m SMART 2 0.937 0.819 0.042[4m7[24m 0.039[4m7[24m ## [90m102[39m ETP 2 0.976 0.769 0.126 0.026[4m9[24m ## [90m103[39m MANA 2 2.32 0.814 0.443 0.002[4m6[24m[4m2[24m ## [90m104[39m BTT 2 1.36 1.11 0.366 0.000[4m7[24m[4m5[24m[4m4[24m ## [90m105[39m ZRX 2 1.35 1.13 0.440 0.001[4m8[24m[4m9[24m ## [90m106[39m CBC 2 8.14 2.95 0.062[4m6[24m 0.081[4m4[24m ## [90m107[39m ELF 2 0.011[4m7[24m 0.836 [31mNA[39m 0.024[4m4[24m ## [90m108[39m BCN 2 1.43 1.10 0.039[4m1[24m 0.679 ## [90m109[39m ZEC 3 1.55 1.62 0.137 0.106 ## [90m110[39m CHZ 1 1.38 0.881 0.012[4m3[24m 0.007[4m4[24m[4m6[24m ## [90m111[39m ADA 1 1.74 1.26 0.002[4m7[24m[4m8[24m 0.010[4m4[24m ## [90m112[39m ARDR 1 1.11 1.02 0.003[4m8[24m[4m1[24m 0.003[4m7[24m[4m9[24m ## [90m113[39m SOLO 1 0.964 2.46 0.123 0.004[4m5[24m[4m5[24m ## [90m114[39m ZAP 1 4.05 14.0 0.110 0.509 ## [90m115[39m IPX 1 2.86 0.170 0.004[4m7[24m[4m9[24m 1 ## [90m116[39m KNC 1 2.35 1.29 0.000[4m0[24m[4m0[24m[4m0[24m065[4m5[24m 0.017[4m9[24m ## [90m117[39m XPR 1 0.533 2.31 0.181 0.085[4m6[24m ## [90m118[39m KMD 1 0.880 1.58 0.013[4m2[24m 0.000[4m6[24m[4m9[24m[4m3[24m ## [90m119[39m XVG 4 0.496 0.924 0.220 0.000[4m1[24m[4m4[24m[4m5[24m ## [90m120[39m BRD 3 4.10 1.29 0.252 0.015[4m8[24m ## [90m121[39m NUT 4 3.17 0.380 0.010[4m0[24m 0.136 ## [90m122[39m VET 4 1.19 1.76 0.040[4m2[24m 0.973 ## [90m123[39m VIB 4 3.69 3.92 [31mNA[39m 0.263 ## [90m124[39m BTG 4 0.032[4m9[24m 1.05 [31mNA[39m 0.016[4m1[24m ## [90m125[39m XNS 3 2.14 10.6 0.042[4m0[24m 0.015[4m8[24m ## [90m126[39m SRN 4 13.9 5.82 0.004[4m6[24m[4m5[24m 0.299 ## [90m127[39m XEM 4 1.18 1.30 0.000[4m2[24m[4m2[24m[4m3[24m 0.000[4m3[24m[4m2[24m[4m5[24m ## [90m128[39m HT 4 0.941 1.62 0.377 0.011[4m5[24m ## [90m129[39m EOS 4 0.081[4m3[24m 1.02 0.343 0.003[4m3[24m[4m8[24m ## [90m130[39m ETH 3 [31mNaN[39m 1.16 [31mNA[39m 0.003[4m8[24m[4m1[24m ## [90m131[39m XDN 4 4.11 3.10 0.256 0.056[4m3[24m ## [90m132[39m SUB 3 25.4 3.02 0.977 [31mNA[39m ## [90m133[39m BNT 3 3.38 1.81 [31mNA[39m 0.007[4m0[24m[4m3[24m ## [90m134[39m OAX 3 0.240 6.82 0.001[4m7[24m[4m4[24m 0.159 ## [90m135[39m ENJ 4 7.53 1.14 0.276 0.011[4m5[24m ## [90m136[39m CRO 3 [31mNaN[39m 1.03 [31mNA[39m 0.004[4m7[24m[4m6[24m ## [90m137[39m TRX 3 0.752 0.869 0.196 0.004[4m8[24m[4m8[24m ## [90m138[39m XMR 4 0.945 1.79 0.000[4m4[24m[4m5[24m[4m1[24m 0.002[4m0[24m[4m2[24m ## [90m139[39m EDG 4 30.4 8.44 0.133 0.086[4m6[24m ## [90m140[39m LTC 4 1.13 1.25 0.030[4m1[24m 0.015[4m9[24m ## [90m141[39m BTC 4 0.417 0.947 0.000[4m3[24m[4m8[24m[4m7[24m 0.016[4m3[24m ## [90m142[39m BAT 4 2.57 1.03 0.110 0.019[4m5[24m ## [90m143[39m DGB 4 2.42 1.35 0.000[4m2[24m[4m7[24m[4m6[24m 0.007[4m8[24m[4m9[24m ## [90m144[39m NEXO 4 2.38 1.87 0.059[4m8[24m 0.007[4m4[24m[4m9[24m ## [90m145[39m MANA 3 1.06 0.814 0.039[4m8[24m 0.002[4m6[24m[4m2[24m ## [90m146[39m ETP 3 0.781 0.769 0.067[4m4[24m 0.026[4m9[24m ## [90m147[39m BSV 3 0.736 1.15 0.291 0.010[4m8[24m ## [90m148[39m BTT 3 0.931 1.11 0.138 0.000[4m7[24m[4m5[24m[4m4[24m ## [90m149[39m DCR 4 1.58 1.73 0.002[4m4[24m[4m2[24m 0.010[4m9[24m ## [90m150[39m STORJ 4 1.70 1.49 0.031[4m0[24m 0.063[4m1[24m ## [90m151[39m SMART 3 1.75 0.819 0.001[4m6[24m[4m7[24m 0.039[4m7[24m ## [90m152[39m ZRX 3 1.60 1.13 0.080[4m5[24m 0.001[4m8[24m[4m9[24m ## [90m153[39m BCN 3 1.42 1.10 0.024[4m5[24m 0.679 ## [90m154[39m CBC 3 2.37 2.95 0.016[4m3[24m 0.081[4m4[24m ## [90m155[39m ZAP 2 4.32 14.0 0.051[4m0[24m 0.509 ## [90m156[39m KNC 2 1.02 1.29 0.025[4m2[24m 0.017[4m9[24m ## [90m157[39m IPX 2 [31mNaN[39m 0.170 [31mNA[39m 1 ## [90m158[39m ARDR 2 6.09 1.02 0.076[4m3[24m 0.003[4m7[24m[4m9[24m ## [90m159[39m KMD 2 0.583 1.58 0.238 0.000[4m6[24m[4m9[24m[4m3[24m ## [90m160[39m CUR 4 0.041[4m8[24m 0.018[4m8[24m 0.802 0.020[4m8[24m ## [90m161[39m CHZ 2 0.970 0.881 0.013[4m6[24m 0.007[4m4[24m[4m6[24m ## [90m162[39m ADA 2 1.47 1.26 0.003[4m2[24m[4m8[24m 0.010[4m4[24m ## [90m163[39m SOLO 2 6.43 2.46 0.116 0.004[4m5[24m[4m5[24m ## [90m164[39m XPR 2 0.679 2.31 0.075[4m9[24m 0.085[4m6[24m ## [90m165[39m ELF 3 0.769 0.836 0.228 0.024[4m4[24m ## [90m166[39m IHF 4 0.417 0.982 0.259 [31mNA[39m ## [90m167[39m NCT 4 0.268 1.36 [31mNA[39m [31mNA[39m ## [90m168[39m YFI 1 2.85 4.26 0.013[4m7[24m 0.257 ## [90m169[39m REP 4 0.980 1.70 0.003[4m0[24m[4m8[24m 0.058[4m7[24m ## [90m170[39m BRD 4 0.758 1.29 0.028[4m0[24m 0.015[4m8[24m ## [90m171[39m ZAP 3 2.16 14.0 0.749 0.509 ## [90m172[39m KNC 3 0.658 1.29 0.346 0.017[4m9[24m ## [90m173[39m XNS 4 4.77 10.6 0.000[4m5[24m[4m8[24m[4m0[24m 0.015[4m8[24m ## [90m174[39m YFI 2 2.66 4.26 0.608 0.257 ## [90m175[39m ARDR 3 10.5 1.02 0.088[4m8[24m 0.003[4m7[24m[4m9[24m ## [90m176[39m ADA 3 0.673 1.26 0.173 0.010[4m4[24m ## [90m177[39m CHZ 3 0.509 0.881 0.065[4m9[24m 0.007[4m4[24m[4m6[24m ## [90m178[39m SOLO 3 1.49 2.46 0.305 0.004[4m5[24m[4m5[24m ## [90m179[39m XPR 3 0.842 2.31 0.012[4m5[24m 0.085[4m6[24m ## [90m180[39m OAX 4 1.99 6.82 0.310 0.159 ## [90m181[39m TRX 4 0.746 0.869 0.180 0.004[4m8[24m[4m8[24m ## [90m182[39m KMD 3 1.99 1.58 0.257 0.000[4m6[24m[4m9[24m[4m3[24m ## [90m183[39m MANA 4 1.53 0.814 0.001[4m8[24m[4m9[24m 0.002[4m6[24m[4m2[24m ## [90m184[39m SMART 4 0.884 0.819 0.386 0.039[4m7[24m ## [90m185[39m ETP 4 1.15 0.769 0.794 0.026[4m9[24m ## [90m186[39m IPX 3 0.991 0.170 0.034[4m3[24m 1 ## [90m187[39m SUB 4 3.11 3.02 0.143 [31mNA[39m ## [90m188[39m BTT 4 0.778 1.11 0.002[4m8[24m[4m0[24m 0.000[4m7[24m[4m5[24m[4m4[24m ## [90m189[39m BCN 4 1.93 1.10 0.174 0.679 ## [90m190[39m ZRX 4 0.703 1.13 0.374 0.001[4m8[24m[4m9[24m ## [90m191[39m CBC 4 5.43 2.95 0.546 0.081[4m4[24m ## [90m192[39m BNT 4 1.23 1.81 0.017[4m1[24m 0.007[4m0[24m[4m3[24m ## [90m193[39m CRO 4 0.949 1.03 0.289 0.004[4m7[24m[4m6[24m ## [90m194[39m ETH 4 0.608 1.16 0.011[4m7[24m 0.003[4m8[24m[4m1[24m ## [90m195[39m BSV 4 1.19 1.15 0.000[4m7[24m[4m4[24m[4m0[24m 0.010[4m8[24m ## [90m196[39m VET 5 [31mNA[39m 1.76 [31mNA[39m 0.973 ## [90m197[39m CUR 5 [31mNA[39m 0.018[4m8[24m [31mNA[39m 0.020[4m8[24m ## [90m198[39m XVG 5 [31mNA[39m 0.924 [31mNA[39m 0.000[4m1[24m[4m4[24m[4m5[24m ## [90m199[39m NUT 5 [31mNA[39m 0.380 [31mNA[39m 0.136 ## [90m200[39m SRN 5 [31mNA[39m 5.82 [31mNA[39m 0.299 ## [90m201[39m ELF 4 0.869 0.836 0.012[4m4[24m 0.024[4m4[24m ## [90m202[39m IHF 5 [31mNA[39m 0.982 [31mNA[39m [31mNA[39m ## [90m203[39m YFI 3 2.44 4.26 0.015[4m8[24m 0.257 ## [90m204[39m VIB 5 [31mNA[39m 3.92 [31mNA[39m 0.263 ## [90m205[39m EOS 5 [31mNA[39m 1.02 [31mNA[39m 0.003[4m3[24m[4m8[24m ## [90m206[39m ENJ 5 [31mNA[39m 1.14 [31mNA[39m 0.011[4m5[24m ## [90m207[39m XDN 5 [31mNA[39m 3.10 [31mNA[39m 0.056[4m3[24m ## [90m208[39m EDG 5 [31mNA[39m 8.44 [31mNA[39m 0.086[4m6[24m ## [90m209[39m LTC 5 [31mNA[39m 1.25 [31mNA[39m 0.015[4m9[24m ## [90m210[39m NEXO 5 [31mNA[39m 1.87 [31mNA[39m 0.007[4m4[24m[4m9[24m ## [90m211[39m BAT 5 [31mNA[39m 1.03 [31mNA[39m 0.019[4m5[24m ## [90m212[39m XMR 5 [31mNA[39m 1.79 [31mNA[39m 0.002[4m0[24m[4m2[24m ## [90m213[39m HT 5 [31mNA[39m 1.62 [31mNA[39m 0.011[4m5[24m ## [90m214[39m BTC 5 [31mNA[39m 0.947 [31mNA[39m 0.016[4m3[24m ## [90m215[39m DGB 5 [31mNA[39m 1.35 [31mNA[39m 0.007[4m8[24m[4m9[24m ## [90m216[39m BTG 5 [31mNA[39m 1.05 [31mNA[39m 0.016[4m1[24m ## [90m217[39m DCR 5 [31mNA[39m 1.73 [31mNA[39m 0.010[4m9[24m ## [90m218[39m XEM 5 [31mNA[39m 1.30 [31mNA[39m 0.000[4m3[24m[4m2[24m[4m5[24m ## [90m219[39m ADA 4 0.849 1.26 0.002[4m5[24m[4m9[24m 0.010[4m4[24m ## [90m220[39m KNC 4 0.678 1.29 0.328 0.017[4m9[24m ## [90m221[39m SOLO 4 3.55 2.46 0.444 0.004[4m5[24m[4m5[24m ## [90m222[39m XPR 4 3.67 2.31 0.387 0.085[4m6[24m ## [90m223[39m CHZ 4 0.761 0.881 0.039[4m4[24m 0.007[4m4[24m[4m6[24m ## [90m224[39m ARDR 4 3.41 1.02 0.095[4m8[24m 0.003[4m7[24m[4m9[24m ## [90m225[39m ZAP 4 0.477 14.0 0.263 0.509 ## [90m226[39m KMD 4 2.45 1.58 0.016[4m4[24m 0.000[4m6[24m[4m9[24m[4m3[24m ## [90m227[39m NCT 5 [31mNA[39m 1.36 [31mNA[39m [31mNA[39m ## [90m228[39m ZEC 4 1.19 1.62 0.283 0.106 ## [90m229[39m IPX 4 1.36 0.170 0.107 1 ## [90m230[39m REP 5 [31mNA[39m 1.70 [31mNA[39m 0.058[4m7[24m ## [90m231[39m STORJ 5 [31mNA[39m 1.49 [31mNA[39m 0.063[4m1[24m ## [90m232[39m XNS 5 [31mNA[39m 10.6 [31mNA[39m 0.015[4m8[24m ## [90m233[39m BRD 5 [31mNA[39m 1.29 [31mNA[39m 0.015[4m8[24m ## [90m234[39m SUB 5 [31mNA[39m 3.02 [31mNA[39m [31mNA[39m ## [90m235[39m YFI 4 3.61 4.26 0.255 0.257 ## [90m236[39m BCN 5 [31mNA[39m 1.10 [31mNA[39m 0.679 ## [90m237[39m BTT 5 [31mNA[39m 1.11 [31mNA[39m 0.000[4m7[24m[4m5[24m[4m4[24m ## [90m238[39m ETP 5 [31mNA[39m 0.769 [31mNA[39m 0.026[4m9[24m ## [90m239[39m SMART 5 [31mNA[39m 0.819 [31mNA[39m 0.039[4m7[24m ## [90m240[39m CBC 5 [31mNA[39m 2.95 [31mNA[39m 0.081[4m4[24m ## [90m241[39m ZRX 5 [31mNA[39m 1.13 [31mNA[39m 0.001[4m8[24m[4m9[24m ## [90m242[39m BSV 5 [31mNA[39m 1.15 [31mNA[39m 0.010[4m8[24m ## [90m243[39m MANA 5 [31mNA[39m 0.814 [31mNA[39m 0.002[4m6[24m[4m2[24m ## [90m244[39m TRX 5 [31mNA[39m 0.869 [31mNA[39m 0.004[4m8[24m[4m8[24m ## [90m245[39m OAX 5 [31mNA[39m 6.82 [31mNA[39m 0.159 ## [90m246[39m CRO 5 [31mNA[39m 1.03 [31mNA[39m 0.004[4m7[24m[4m6[24m ## [90m247[39m BNT 5 [31mNA[39m 1.81 [31mNA[39m 0.007[4m0[24m[4m3[24m ## [90m248[39m ETH 5 [31mNA[39m 1.16 [31mNA[39m 0.003[4m8[24m[4m1[24m ## [90m249[39m ELF 5 [31mNA[39m 0.836 [31mNA[39m 0.024[4m4[24m ## [90m250[39m ZAP 5 [31mNA[39m 14.0 [31mNA[39m 0.509 ## [90m251[39m IPX 5 [31mNA[39m 0.170 [31mNA[39m 1 ## [90m252[39m ADA 5 [31mNA[39m 1.26 [31mNA[39m 0.010[4m4[24m ## [90m253[39m CHZ 5 [31mNA[39m 0.881 [31mNA[39m 0.007[4m4[24m[4m6[24m ## [90m254[39m XPR 5 [31mNA[39m 2.31 [31mNA[39m 0.085[4m6[24m ## [90m255[39m SOLO 5 [31mNA[39m 2.46 [31mNA[39m 0.004[4m5[24m[4m5[24m ## [90m256[39m ARDR 5 [31mNA[39m 1.02 [31mNA[39m 0.003[4m7[24m[4m9[24m ## [90m257[39m KNC 5 [31mNA[39m 1.29 [31mNA[39m 0.017[4m9[24m ## [90m258[39m KMD 5 [31mNA[39m 1.58 [31mNA[39m 0.000[4m6[24m[4m9[24m[4m3[24m ## [90m259[39m ZEC 5 [31mNA[39m 1.62 [31mNA[39m 0.106 ## [90m260[39m YFI 5 [31mNA[39m 4.26 [31mNA[39m 0.257 7.4 Adjust Prices - All Models 7.4.1 Add Last Price And now we can do the same for all the other models cryptodata_nested &lt;- mutate(cryptodata_nested, xgb_test_predictions = ifelse(split &lt; 5, map2(train_data, xgb_test_predictions, last_train_price), NA), xgbTree_test_predictions = ifelse(split &lt; 5, map2(train_data, xgbTree_test_predictions, last_train_price), NA), nnet_test_predictions = ifelse(split &lt; 5, map2(train_data, nnet_test_predictions, last_train_price), NA)) 7.4.1.0.1 Holdout (REMEMBER holdout step for all to get price from 5th train split first) cryptodata_nested_holdout &lt;- mutate(filter(cryptodata_nested, split == 5), xgb_holdout_predictions = map2(train_data, xgb_holdout_predictions, last_train_price), xgbTree_holdout_predictions = map2(train_data, xgbTree_holdout_predictions, last_train_price), nnet_holdout_predictions = map2(train_data, nnet_holdout_predictions, last_train_price)) Join the holdout data to all rows based on the cryptocurrency symbol alone: cryptodata_nested &lt;- left_join(cryptodata_nested, select(cryptodata_nested_holdout, symbol, xgb_holdout_predictions, xgbTree_holdout_predictions, nnet_holdout_predictions), by=&#39;symbol&#39;) # Remove unwanted columns cryptodata_nested &lt;- select(cryptodata_nested, -xgb_holdout_predictions.x, -xgbTree_holdout_predictions.x, -nnet_holdout_predictions.x, -split.y) # Rename the columns kept cryptodata_nested &lt;- rename(cryptodata_nested, xgb_holdout_predictions = &#39;xgb_holdout_predictions.y&#39;, xgbTree_holdout_predictions = &#39;xgbTree_holdout_predictions.y&#39;, nnet_holdout_predictions = &#39;nnet_holdout_predictions.y&#39;, split = &#39;split.x&#39;) # Reset the correct grouping structure cryptodata_nested &lt;- group_by(cryptodata_nested, symbol, split) 7.4.2 Convert to % Change Overwrite the old predictions with the new predictions adjusted as a percentage now: cryptodata_nested &lt;- mutate(cryptodata_nested, xgb_test_predictions = ifelse(split &lt; 5, map(xgb_test_predictions, standardize_perc_change), NA), # Holdout for all splits xgb_holdout_predictions = map(xgb_holdout_predictions, standardize_perc_change), # xgbTree xgbTree_test_predictions = ifelse(split &lt; 5, map(xgbTree_test_predictions, standardize_perc_change), NA), # Holdout for all splits xgbTree_holdout_predictions = map(xgbTree_holdout_predictions, standardize_perc_change), # nnet nnet_test_predictions = ifelse(split &lt; 5, map(nnet_test_predictions, standardize_perc_change), NA), # Holdout for all splits nnet_holdout_predictions = map(nnet_holdout_predictions, standardize_perc_change)) 7.4.3 Add Metrics cryptodata_nested &lt;- mutate(cryptodata_nested, # XGBoost - RMSE - Test xgb_rmse_test = unlist(ifelse(split &lt; 5, map2(xgb_test_predictions, actuals_test, evaluate_preds_rmse), NA)), # And holdout: xgb_rmse_holdout = unlist(map2(xgb_holdout_predictions, actuals_holdout ,evaluate_preds_rmse)), # XGBoost - R^2 - Test xgb_rsq_test = unlist(ifelse(split &lt; 5, map2(xgb_test_predictions, actuals_test, evaluate_preds_rsq), NA)), # And holdout: xgb_rsq_holdout = map2(xgb_holdout_predictions, actuals_holdout, evaluate_preds_rsq), # XGBoost Trees - RMSE - Test xgbTree_rmse_test = unlist(ifelse(split &lt; 5, map2(xgbTree_test_predictions, actuals_test, evaluate_preds_rmse), NA)), # And holdout: xgbTree_rmse_holdout = unlist(map2(xgbTree_holdout_predictions, actuals_holdout, evaluate_preds_rmse)), # XGBoost Trees - R^2 - Test xgbTree_rsq_test = unlist(ifelse(split &lt; 5, map2(xgbTree_test_predictions, actuals_test, evaluate_preds_rsq), NA)), # And holdout: xgbTree_rsq_holdout = unlist(map2(xgbTree_holdout_predictions, actuals_holdout, evaluate_preds_rsq)), # Neural Network - RMSE - Test nnet_rmse_test = unlist(ifelse(split &lt; 5, map2(nnet_test_predictions, actuals_test, evaluate_preds_rmse), NA)), # And holdout: nnet_rmse_holdout = unlist(map2(nnet_holdout_predictions, actuals_holdout, evaluate_preds_rmse)), # Neural Network - R^2 - Test nnet_rsq_test = unlist(ifelse(split &lt; 5, map2(nnet_test_predictions, actuals_test, evaluate_preds_rsq), NA)), # And holdout: nnet_rsq_holdout = unlist(map2(nnet_holdout_predictions, actuals_holdout, evaluate_preds_rsq))) 7.5 Eval RMSE New object without split column cryptodata_metrics: cryptodata_metrics &lt;- group_by(select(ungroup(cryptodata_nested),-split),symbol) 7.5.1 Test Now for each model we will create a new column giving the average RMSE for the 4 cross-validation test splits: rmse_test &lt;- mutate(cryptodata_metrics, lm = mean(lm_rmse_test, na.rm = T), xgb = mean(xgb_rmse_test, na.rm = T), xgbTree = mean(xgbTree_rmse_test, na.rm = T), nnet = mean(nnet_rmse_test, na.rm = T)) Now we can use the gather() function to summarize the columns as rows: rmse_test &lt;- unique(gather(select(rmse_test, lm:nnet), &#39;model&#39;, &#39;rmse&#39;, -symbol)) # Show results rmse_test ## [90m# A tibble: 208 x 3[39m ## [90m# Groups: symbol [52][39m ## symbol model rmse ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m BTG lm 0.847 ## [90m 2[39m VET lm 1.24 ## [90m 3[39m IHF lm 0.777 ## [90m 4[39m DGB lm 1.66 ## [90m 5[39m LTC lm 0.882 ## [90m 6[39m NUT lm 1.71 ## [90m 7[39m NEXO lm 1.79 ## [90m 8[39m XMR lm 0.977 ## [90m 9[39m VIB lm 4.26 ## [90m10[39m XDN lm 1.59 ## [90m# … with 198 more rows[39m Now tag the results as having been for the test set rmse_test$eval_set &lt;- &#39;test&#39; 7.5.2 Holdout Do the same for the holdout set rmse_holdout &lt;- mutate(cryptodata_metrics, lm = mean(lm_rmse_holdout, na.rm = T), xgb = mean(xgb_rmse_holdout, na.rm = T), xgbTree = mean(xgbTree_rmse_holdout, na.rm = T), nnet = mean(nnet_rmse_holdout, na.rm = T)) Now we can use the gather() function to summarize the columns as rows: rmse_holdout &lt;- unique(gather(select(rmse_holdout, lm:nnet), &#39;model&#39;, &#39;rmse&#39;, -symbol)) # Show results rmse_holdout ## [90m# A tibble: 208 x 3[39m ## [90m# Groups: symbol [52][39m ## symbol model rmse ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m BTG lm 1.05 ## [90m 2[39m VET lm 1.76 ## [90m 3[39m IHF lm 0.982 ## [90m 4[39m DGB lm 1.35 ## [90m 5[39m LTC lm 1.25 ## [90m 6[39m NUT lm 0.380 ## [90m 7[39m NEXO lm 1.87 ## [90m 8[39m XMR lm 1.79 ## [90m 9[39m VIB lm 3.92 ## [90m10[39m XDN lm 3.10 ## [90m# … with 198 more rows[39m Now tag the results as having been for the holdout set rmse_holdout$eval_set &lt;- &#39;holdout&#39; 7.5.3 Union Results rmse_scores &lt;- union(rmse_test, rmse_holdout) 7.6 Eval R^2 7.6.1 Test For each model we will create a new column giving the average R^2 for the 4 cross-validation test splits: rsq_test &lt;- mutate(cryptodata_metrics, lm = mean(lm_rsq_test, na.rm = T), xgb = mean(xgb_rsq_test, na.rm = T), xgbTree = mean(xgbTree_rsq_test, na.rm = T), nnet = mean(nnet_rsq_test, na.rm = T)) Now we can use the gather() function to summarize the columns as rows: rsq_test &lt;- unique(gather(select(rsq_test, lm:nnet), &#39;model&#39;, &#39;rsq&#39;, -symbol)) # Show results rsq_test ## [90m# A tibble: 208 x 3[39m ## [90m# Groups: symbol [52][39m ## symbol model rsq ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m BTG lm 0.081[4m5[24m ## [90m 2[39m VET lm 0.151 ## [90m 3[39m IHF lm 0.073[4m7[24m ## [90m 4[39m DGB lm 0.157 ## [90m 5[39m LTC lm 0.016[4m0[24m ## [90m 6[39m NUT lm 0.074[4m3[24m ## [90m 7[39m NEXO lm 0.171 ## [90m 8[39m XMR lm 0.088[4m0[24m ## [90m 9[39m VIB lm 0.084[4m2[24m ## [90m10[39m XDN lm 0.300 ## [90m# … with 198 more rows[39m Now tag the results as having been for the test set rsq_test$eval_set &lt;- &#39;test&#39; 7.6.2 Holdout Do the same for the holdout set rsq_holdout &lt;- mutate(cryptodata_metrics, lm = mean(lm_rsq_holdout, na.rm = T), xgb = mean(xgb_rsq_holdout, na.rm = T), xgbTree = mean(xgbTree_rsq_holdout, na.rm = T), nnet = mean(nnet_rsq_holdout, na.rm = T)) Now we can use the gather() function to summarize the columns as rows: rsq_holdout &lt;- unique(gather(select(rsq_holdout, lm:nnet), &#39;model&#39;, &#39;rsq&#39;, -symbol)) # Show results rsq_holdout ## [90m# A tibble: 208 x 3[39m ## [90m# Groups: symbol [52][39m ## symbol model rsq ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m BTG lm 0.016[4m1[24m ## [90m 2[39m VET lm 0.973 ## [90m 3[39m IHF lm [31mNaN[39m ## [90m 4[39m DGB lm 0.007[4m8[24m[4m9[24m ## [90m 5[39m LTC lm 0.015[4m9[24m ## [90m 6[39m NUT lm 0.136 ## [90m 7[39m NEXO lm 0.007[4m4[24m[4m9[24m ## [90m 8[39m XMR lm 0.002[4m0[24m[4m2[24m ## [90m 9[39m VIB lm 0.263 ## [90m10[39m XDN lm 0.056[4m3[24m ## [90m# … with 198 more rows[39m Now tag the results as having been for the holdout set rsq_holdout$eval_set &lt;- &#39;holdout&#39; 7.6.3 Union Results rsq_scores &lt;- union(rsq_test, rsq_holdout) 7.7 Visualize Results 7.7.1 RMSE Visualization Now we can take the same tools we learned in the Visualization section from earlier and visualize the results of the models. 7.7.2 Both 7.7.2.1 Join Datasets First join the two plot_scores &lt;- merge(rmse_scores, rsq_scores) 7.7.2.2 Plot Results ggplot(plot_scores, aes(x=rsq, y=rmse, color = model)) + geom_point() + ylim(c(0,50)) ADA only ggplot(filter(plot_scores, symbol == &#39;ADA&#39;), aes(x=rsq, y=rmse, color = model)) + geom_point() + facet_wrap(~eval_set) Now by the cryptocurrency ggplot(plot_scores, aes(x=rsq, y=rmse, color = model)) + geom_point() + geom_smooth() + ylim(c(0,50)) + facet_wrap(~symbol) 7.7.3 Results by the Cryptocurrency knitr::include_app(&#39;https://predictcrypto.shinyapps.io/tutorial_latest_model_summary/&#39;, height = &#39;600px&#39;) The app shown above also has a button to Show Code. If you were to copy and paste that code into an RStudio session on your computer into a file with the .Rmd file extension and you then Knit the file, the same exact app should show up on your computer, no logins or setup outside of the packages required for the code to run; RStudio should automatically prompt you to install packages that are not currently installed on your computer. "],["time-series.html", "Section - 8 Time Series", " Section - 8 Time Series Add here "],["predictions.html", "Section - 9 Predictions", " Section - 9 Predictions (look at past versions) "],["explain-predictions.html", "Section - 10 Explain Predictions", " Section - 10 Explain Predictions # remove after test! print(&#39;got here&#39;) ## [1] &quot;got here&quot; "],["considerations.html", "Section - 11 Considerations 11.1 Session Information", " Section - 11 Considerations What we have outlined here is a supervised machine learning problem and a practical possible approach to the problem, but the results contained in this document would not translate to real-world results. This tutorial is not meant to show anyone how to trade on the cryptocurrency markets, but rather encourage people to apply these tools to their own data problems, and that is the reason the tutorial stops here (also because we like not getting sued). We stop here before dealing with many difficult execution problems, including but not exclusive to: Finding a trading methodology that makes sense. There are lots of decisions to be made, market or limit orders? Coming up with a good trade execution plan that works consistently is not as easy. …Keep outlining the issues outlined by Chandler, especially relating to low market cap cryptocurrencies 11.1 Session Information Below is information relating to the specific R session that was run. If you are unable to reproduce these steps, find the correct version of the tools to install below: sessionInfo() ## R version 4.0.3 (2020-10-10) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] hydroGOF_0.4-0 zoo_1.8-8 deepnet_0.2 gbm_2.1.8 ## [5] xgboost_1.0.0.2 doParallel_1.0.15 iterators_1.0.12 foreach_1.5.0 ## [9] caret_6.0-86 lattice_0.20-38 transformr_0.1.3 gganimate_1.0.5 ## [13] ggforce_0.3.2 ggpubr_0.4.0 plotly_4.9.2.1 ggthemes_4.2.0 ## [17] magick_2.5.0 av_0.5.1 gifski_0.8.6 ggTimeSeries_1.0.1 ## [21] jsonlite_1.7.1 httr_1.4.2 anytime_0.3.7 tsibble_0.9.2 ## [25] forcats_0.5.0 stringr_1.4.0 dplyr_1.0.2 purrr_0.3.4 ## [29] readr_1.3.1 tidyr_1.1.1 tibble_3.0.4 ggplot2_3.3.2 ## [33] tidyverse_1.3.0 DT_0.15 skimr_2.1 pins_0.4.0 ## [37] pacman_0.5.1 knitr_1.30 bookdown_0.21 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 backports_1.1.6 plyr_1.8.6 ## [4] repr_1.1.0 lazyeval_0.2.2 sp_1.4-1 ## [7] splines_4.0.3 crosstalk_1.1.0.1 gstat_2.0-6 ## [10] digest_0.6.25 htmltools_0.5.0 fansi_0.4.1 ## [13] magrittr_1.5 openxlsx_4.2.2 recipes_0.1.14 ## [16] modelr_0.1.6 gower_0.2.1 xts_0.12.1 ## [19] lpSolve_5.6.15 prettyunits_1.1.1 colorspace_1.4-1 ## [22] rappdirs_0.3.1 rvest_0.3.5 haven_2.2.0 ## [25] xfun_0.18 crayon_1.3.4 survival_3.1-8 ## [28] glue_1.4.1 polyclip_1.10-0 gtable_0.3.0 ## [31] ipred_0.9-9 car_3.0-10 abind_1.4-5 ## [34] scales_1.1.1 emo_0.0.0.9000 DBI_1.1.0 ## [37] rstatix_0.6.0 Rcpp_1.0.4.6 viridisLite_0.3.0 ## [40] progress_1.2.2 units_0.6-7 foreign_0.8-80 ## [43] intervals_0.15.2 stats4_4.0.3 lava_1.6.7 ## [46] prodlim_2019.11.13 htmlwidgets_1.5.1 FNN_1.1.3 ## [49] ellipsis_0.3.1 reshape_0.8.8 pkgconfig_2.0.3 ## [52] farver_2.0.3 nnet_7.3-12 dbplyr_1.4.2 ## [55] utf8_1.1.4 labeling_0.3 tidyselect_1.1.0 ## [58] rlang_0.4.7 reshape2_1.4.3 munsell_0.5.0 ## [61] cellranger_1.1.0 tools_4.0.3 cli_2.0.2 ## [64] generics_0.0.2 broom_0.7.2 evaluate_0.14 ## [67] yaml_2.2.1 ModelMetrics_1.2.2.2 fs_1.4.1 ## [70] zip_2.0.4 hydroTSM_0.6-0 nlme_3.1-144 ## [73] xml2_1.3.1 compiler_4.0.3 rstudioapi_0.11 ## [76] filelock_1.0.2 curl_4.3 e1071_1.7-4 ## [79] ggsignif_0.6.0 reprex_0.3.0 spacetime_1.2-3 ## [82] tweenr_1.0.1 stringi_1.4.6 highr_0.8 ## [85] Matrix_1.2-18 classInt_0.4-3 vctrs_0.3.2 ## [88] pillar_1.4.4 lifecycle_0.2.0 maptools_1.0-2 ## [91] data.table_1.12.8 R6_2.4.1 KernSmooth_2.23-16 ## [94] rio_0.5.16 codetools_0.2-16 MASS_7.3-51.5 ## [97] assertthat_0.2.1 withr_2.3.0 mgcv_1.8-31 ## [100] hms_0.5.3 grid_4.0.3 rpart_4.1-15 ## [103] timeDate_3043.102 class_7.3-15 rmarkdown_2.5 ## [106] carData_3.0-4 automap_1.0-14 sf_0.9-6 ## [109] pROC_1.16.2 lubridate_1.7.8 base64enc_0.1-3 "],["archive.html", "Section - 12 Archive 12.1 May 2020", " Section - 12 Archive Below is an archive of this same document from different dates: 12.1 May 2020 May 23, 2020 - Morning May 22, 2020 - Morning May 21, 2020 - Morning May 20, 2020 - Morning May 19, 2020 - Morning May 18, 2020 - Morning "],["references.html", "Section - 13 References 13.1 Resources Used 13.2 Packages used and cited", " Section - 13 References The bookdown package (Xie 2020) was used to produce this document, which was built on top of R Markdown and knitr (???). knitr::write_bib(c(.packages()), &quot;packages.bib&quot;) 13.1 Resources Used 13.1.1 Visualization Section https://www.rayshader.com/ https://github.com/yutannihilation/gghighlight https://github.com/njtierney/rstudioconf20/blob/master/slides/index.Rmd … add rest here 13.1.2 Time Series Section https://stackoverflow.com/questions/42820696/using-prophet-package-to-predict-by-group-in-dataframe-in-r … add rest here 13.2 Packages used and cited Arnold, Jeffrey B. 2019. Ggthemes: Extra Themes, Scales and Geoms for Ggplot2. http://github.com/jrnold/ggthemes. Eddelbuettel, Dirk. 2020. Anytime: Anything to Posixct or Date Converter. http://dirk.eddelbuettel.com/code/anytime.html. Kassambara, Alboukadel. 2020. Ggpubr: Ggplot2 Based Publication Ready Plots. https://rpkgs.datanovia.com/ggpubr/. Kothari, Aditya. 2018. GgTimeSeries: Time Series Visualisations Using the Grammar of Graphics. https://github.com/Ather-Energy/ggTimeSeries. Luraschi, Javier. 2020. Pins: Pin, Discover and Share Resources. https://github.com/rstudio/pins. Morgan-Wall, Tyler. 2020. Rayshader: Create Maps and Visualize Data in 2D and 3D. https://github.com/tylermorganwall/rayshader. Pedersen, Thomas Lin. 2020. Ggforce: Accelerating Ggplot2. https://CRAN.R-project.org/package=ggforce. Pedersen, Thomas Lin, and David Robinson. 2020. Gganimate: A Grammar of Animated Graphics. https://CRAN.R-project.org/package=gganimate. Sievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik Ram, Marianne Corvellec, and Pedro Despouy. 2020. Plotly: Create Interactive Web Graphics via Plotly.js. https://CRAN.R-project.org/package=plotly. Wang, Earo, Di Cook, Rob Hyndman, and Mitchell O’Hara-Wild. 2020. Tsibble: Tidy Temporal Data Frames and Tools. https://tsibble.tidyverts.org. Waring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2020. Skimr: Compact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr. Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2020. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr. Xie, Yihui. 2020. Bookdown: Authoring Books and Technical Documents with R Markdown. https://github.com/rstudio/bookdown. References "]]
